I just got my ibook2 up and running on OSX again after a brief run with ppc linux. Vim looks like it will never run in gui mode, so I'm stuck in 16 color cterm hell. The airport card is beautiful though, no problems and almost no configuration needed with a netgear me102 wireless hub. Next up: XFree86 on Darwin.
---
title: Fink
---

Fink is absolutely solid, it's the only way to go for osx X11 ports.I have managed to get vim up, in full gui mode with an aqua wrapper. Ditto with xmms.
First, upgrade to 10.1. Then download the osx developer tools for 10.1. Get XDarwin.
Finally, get Fink 0.3.0.
---
title: Google Groups / My Usenet Posts
---

Check out google's newsgroup feature if you haven't already. Turns out my first post was in 1997...

[View all my usenet posts](http://groups.google.com/groups?q=%22Chase+Seibert%22&scoring=d)
---
title: Going Postal
---

From the Dean at Boston University:

>My secretary forward to me your message. Frankly, I am surprised by your response that you never check your mail.... if you continue your practice of not checking your mail, you may one day be surprised to learn that your credit is ruined (because you failed to pay a bill), you are unable to graduate (because you failed to settle your account with the University), or you are even subject to arrest (because you failed to respond to a summons). It would be a much wiser practice to check your mailbox on a regular basis...

>The fact of the matter is that polite society communicates by letter (as does most of government, businesses, and cultural institutions). I can only assume that you have no interest in becoming a member of polite society. This is a shame for your academic record suggests that you have the intelligence to contribute to it in significant ways. Your failure to play by its rules, however, will make it very difficult for you to be consequential in the world in which we live." - Professor C E Klafter, Assistant to the President

Response from me:

>I don't think I have a duty to check my mail every single day, or at all. I don't pick up my phone every time it rings either, is that ok with you? I don't remember ever signing a contract with the US postal system requiring me to even have an address never mind check mail on a regular basis. Frankly I would be surprised if the mail system even exists in 20 years aside from package delivery.
---
title: My Reaction to 911
---

The United States is over-reacting to terror.

I remember being openly ridiculed right after 911 when I said that as a country we should do nothing in retaliation. How many times do you let a bully hit you in the face before you retaliate? Good point, but the answer should not be once.

Personally, I'm much more worried about the government curtailing individual freedom in the name of security that I am about being hit by a hijacked airplane. Oh, you're being Orwellian. The US hasn't really drastically reduced anyone's freedom. Bullshit.

Let's indefinitely detain anyone we want without proving wrong doing.

Let's revert to 1950's style foreign and domestic propaganda. This is no time to be kowtowing to atheistic zealots. Sure, instead we should be fostering political subordination.

We tell our children not to solve their problems through violence. Sure, it's idealistic. But given that we are in the unique, secure position squarely on the apex of the world government hierarchy, don't we have a moral obligation to set an example? Yes, moral. Stick that in your religious crack pipe.

How much more of a show of strength would it be to act in a civil, detached manner, allowing the very terrorists we are after the same basic freedoms we say we want to give to the world? Why not just cut the military budget, pull US armed forces and support out of contested regions where there is not 100% agreement on whether we ought to be there, and use the money to, I don't know, rebuild our fucking economy. 
---
title: Procmail
---

I am now 100% spam free, thanks to procmail. For one reason or another, I finally started getting spam in my BU inbox. It took 3 full years, but those relentless bastards found me. Thanks go out to those motherfuckers at EveSecretGarden.com, which was the straw that broke the camel's back. What really pissed me off about this spam was that there way no way to be removed from the list and it had a fucking LEGAL disclaimer.

"Under Bill s.1618 Title III passed by the 105th U. S. Congress, mail cannot be considered spam as long as we include contact information and a remove link for removal from this mailing list. If this e-mail is unsolicited, please accept our apologies... Do Not Reply To This Message To Be Removed."

So, what the hell is procmail? It's a decade old email parsing Unix program, that happens to be installed on ACS. Anyone who has set up vacation email forwarding at BU has used it, indirectly.

The actual script is from Nic Wolff. It basically keeps a running list of people that you WILL accept email from. Anyone who I have not heard from before, including spammers, receives an email telling them how to add themselves to the list. Of course, everyone knows that most spammers would never reply to an email. They are not interested in assimilating individuals.

So, how does this affect you, my survey and chain-mail sending friends? It shouldn't. I have already dumped my entire existing contact list into the accept filter file, so most of you won't hear about this again. However, if I missed you, or if you get a new email, you will receive a nice personal message the first time you try to send me something. Just hit reply, and you're done!

Consider this part one of how to get clean email onto your cellphone, which will be coming up next!
---
title: VMWare
---

VMware kicks ass. In a nutshell, it lets you emulate one or many simultaneous "virtual machines", allowing you to host literally any OS on a windows or linux box.

For me, it was a question of Linux on Windows, or Windows on Linux. I already run WinXP, so using that as the base for Linux seemed natural. However, I'm fairly good with linux, and I would use it as my OS if I could get it to work with my specialized windows only hardware (Sigma, you bastards), and I could play games.

Ah, the games. Windows on Linux with vmware still doesn't do DirectX, and it doesn't look like it will anytime soon. For now, the best option appears to be transgaming.com.

So, it took about 40 minutes to do a full Redhat 7.3 ftp install, and it got up and running painlessly. No bootdisks; vmware can mount disk images directly. No CDs; ftp install is gangbusters over OC3.

VMware does network bridging to give linux and windows separate IPs. It has a special X11 server for accelerated 2D graphics under windows. In general, it's just pretty damn cool.
---
title: SSH
---

My Windows box is hosed again. Nothing serious, it just needs to run chkdsk off the Windows XP cd. Unfortunately, I seemed to have misplaced my 100% legitimate copy somewhere... Damn those piracy gremlins.

In the meantime, my laptop gets some much need attention. Running PPC Linux, I noticed that since I've gotten behind a router, X11 remote X-terminal sessions no longer work. The same goes for my VMware linux setup, so I assumed it was an actual router issue, and not some lost setting.

Right. It turns out that X-Win32 still worked with the "proxy" setting enabled, so there was my ticket. Previously I had been using telnet, but even with my router forwaring port 6000 (Xterm defualt) to the appropriate machine, still no dice.

Ssh was the ticket. I should really be using it anyway. In addition to some airport-critical RSA encryption, it also does proxy port forwarding, and has a specific X11 command line option (ssh -X)! Yippie!

The long and short of it is this worked:
router forwards port 6000 to intranet client IP.

`ssh -X csa.bu.edu`

You don't even need to set DISPLAY manually.
---
title: killn alias
---

I just took 20 minutes to scratch a 3 year old itch. Ever since I started programming in unix, I have wanted a kill command that can kill by process name, instead of just pid.

Behold, the one-liner:

```bash
kill -9 `ps|grep $1|awk '{print $1}'`
```

I tossed this into a fill named killn, did a `chmod +x killn` to make executable, and viola!

Now, you can run `killn project1` to kill all processes named project1, for example the 8 billion zombie process you left behind when you were fucking around with shared memory.

Toss another one-liner into your .cshrc file to include a new path in your run-time environment
``.cshrc <- setenv PATH {$PATH}:/home/ugrad/cseibert/sbin`.
---
title: Bayesian Spam Filtering
---

I officially declared phase one of my personal war on spam five months ago, when I started using procmail to do some server side email parsing. My original method was just a rip-off of a script I found online. The basic algorithm was as follows:

If you’re whitelisted, let the mail through.
If you’re blacklisted, delete the mail.
If I haven’t heard from you before, email you to ask if you should be whitelisted.

I have used this more or less without incident for a few months. Recently, I decided that it was not enough to just let all whitelisted emails through into my default inbox. Instead, I’ve rewritten the entire filter to sort whitelisted email into folders based on either sender or receives (for example, a mail list a belong to). I also stopped sending unsolicited emails to people to ask them if they wanted to be whitelisted. It just got to be too much damn work.

The result is what I’m calling Procmail Bladderwort, my own first line of spam defense. As of now, I’m manually adding new non-spam senders to my white-lists. This, however, is soon to change in phase two: Bayesian content filtering.

The basic premise of Bayesian filtering is to collect statistical word and word-pair information extracted from email you actually receive. Using Bayer’s (I) formula, it is possible to classify an unknown mail as either spam or non-spam with a high degree of accuracy (less than 5 in 1000 errors). There is a great article on it here.
---
title: Bayesian Spam Filtering 2
---

The spam battle rages on...

I have a beta version of my new bayesian spam filter as outlined in Paul Graham's
Plan for Spam, called SpamNet. It's basically a statistical token-based text analysis web service.

You can play with the development version here.

I am currently working on a nice .Net Framework GUI for building the SQL backend from saved IMAP data files, and other niceties. When the whole thing is done, look for a version in my code section.
---
title: Bullhorn
---

First off, I have a new job! It's a small ASP based software company in Boston called Bullhorn.

For the past couple of weeks, I've been learning their systems, gearing up for my role as Network and Systems Engineer. This will basically entail a lot of hardware upgrades/maintenance and programming.

My first coding job is to write some SMTP event sink handler routines in C# to try and keep us from occasionally losing emails. I'll post some demo code when it's done.

Microsoft provides event sinks for installing your own components in the SMTP delivery chain. Basically, you can write a series of COM objects in C#, C++ or whatever languages you want, and have incoming email passed through them in sequence. It's actually not that hard.

Here are some simple ideas of the kinds of things you could do: parse email into a database (ala Bullhorn), filter email for spam, log email statistics and debugging into to a file. More or less any custom email handling is done easiest as a SMTP event sink.

Start out by downloading a C# code template project for Visual Studio .Net. You can grab it here: SMTPEventSink.tar.gz

Note that the project has the option "Register as COM Interpo" set in the config. Before you can use it, you have to register each event sink handler with smtpreg.vbs, which you can grab off of msdn.

For an example, see the register/list.bat files provided. More to come later.
---
title: Xbox Moding
---

The xbox modding project is over and done, for now. A successful mod started with a few bumps and scratches, and was riddled with various unexpected stops and starts. What follows is my experience performing this mod, incomplete and circumstantial though it is.

First off, I would suggest that non-soldering individuals (read: mere-mortals), order a solder-less chip like the Xecuter 2.3 Lite+. After a brief bought trying to install a solder chip with duck tape and glue gun, I decided to order a solder-less chip and was much happier with it.

The solder-less chips are screw in, and this particular one had LEDs to let you know when the various pins were in contact correctly with the motherboard. After a little fiddling with the positioning, the chip allows you to bypass the on-board BIOS with an open-source BIOS that allows you to run non-sanctioned code.

Legally, mod chips cannot be sold with BIOS that allows you to run signed (encrypted) xbox code. So the first thing you have to do is flash the mod chip with an illegal BIOS. I used X2 rev 4977, which you can find on your favorite p2p network. The flashing procedure is very simple, you just run the BIOS file onto a CD with using a specific naming convention, along with a 50MB+ dummy file to make the CD more readable. The BIOS that ships with the mod chip will contain flashing software and it's all fairly automated.

The second thing you need to install is a new dashboard. This replaces the xbox boot loader and lets to run arbitrary programs. I suggest EvoX, which has given me no reason to seek out an alternative. After having done a few manual installs, I found a utility called Slayer's EvoX auto-installer, which I highly recommend. This is also an illegal piece of software, so p2p comes in handy again.

Getting EvoX up and running should be as simple as clicking your way through a few menus. As soon as it's up and running, the xbox will launch a ftp server that you can log into and use to transfer files back and forth. Backing up the C and E drives to another computer effectively gives you imunity from screwing up your unit.

At this point, I would suggest putting in another hard drive. I used a spare Maxtor 120GB, 5,4000 RPM (heat IS an issue). You don't have to format it or anything, Slayer can do that for you. I would also suggest installing the base apps off the slayer CD, which is a menu option buried some where. Now you are really ready to go...

The base application package from Slayers v.2.5 (the 250MB ISO image) includes: Px-HDD for ripping games from the DVDs to the hard drive and XBMP (not the newer XBMC) for playing DivX/Xvid/mp3/whatever files off of the network. Initial experience with the ripper was excellent, no issues with the 8 games I tried.

XBMP is absolutely awesome, and my real reason for the mod. It uses xine, the open-source linux video player, so compatibility is sure to be good. Configuration was a little rough, mostly because I wanted to use SMB to access the files remotely. SMB on the xbox seems a little flaky. It has a hard time with weird characters in file names ("[DviX]", notably), and it didn't seem to like my linux SMB default shares. I had to add the user "xbox:xbox" and give him rights to the files, as well as customize the connect string in the XBMP config file.

The actual movie quality is top-notch, due to excellent xbox A/V outputs. The interface leaves something to be desired, but it gets the job done. I have yet to play with XBMC, but I don't think it is supposed to be a great cosmetic improvement.

There you have it. All in all, a fun project and something I may end up doing again for a friend. I recommend the forums at http://www.xbox-scene.com/ if you intend on undertaking this yourself.
---
title: How to prepare for a software engineer interview
tags: interview workplace career
---

Interviews for software engineering positions are equal measures normal job
interview and extemporaneous logic bomb de-arming with an audience. Most developers
know the drill; you stand up in front of one of more senior developers and have
to write code on a whiteboard to solve what's normally a fairly academic toy
problem, the likes of which you have not thought about since college, what with
being busy writing real code for real problems.


# The Whiteboard

Having given countless interviews myself over the last 10 years, I have noticed
that a non-negligible percentage of candidates simply lock up when faced with the
prospect of whiteboard coding. Many don't seem prepared for this eventuality at all,
which is kind of surprising given that I personally can't imagine any decent software
companies NOT doing this during an interview.

So, you're going to have to do whiteboard coding. Accept it. The good news is that
**it's actually easier that coding on a computer**. I have personally tried to out-smart
the whiteboard by bringing a laptop to an interview, thinking that doing what I
do every day would be more successful than the unnatural exercise of coding
without a feedback loop. Big mistake.

As a candidate, the person interviewing you should be your feedback loop. That's what
they are really after anyway; an insight into how you think about coding. Plus, you only
have to write code that *looks* like it will work to the satisfaction of all parties
present. If you whip out a computer, the code will HAVE to be write, because you're
going to be running it.

The key to keeping your composure is to practice breaking test problems down into five
stages. *Restate* the problem as they have given it to you. Come up with *examples*,
including edge cases. Work out an *algorithm* by doing the problem by hand on the
whiteboard. Only then do you write some *code*. Finally, reason out loud about the
*time* your algorithm will take to run. This is known as **REACT**.


# The Other Stuff

Assuming you get past the technical coding portion of the interview, an additional chunk
of candidates flub what should be an easy "softer" portion of the interview. The absolute
key to this part is preparation. You're going to be asked questions, mostly of the kind
you could guess ahead of time.

- Tell me about yourself.
- Where do you want to be in 5 years?
- What are your greatest strengths and weaknesses?
- What are 5 annoying things about your favorite language and framework?
- Why are you looking for a job?
- How well do you work in a team?
- Tell me about a mistake you made at work.
- Have you ever not gotten along with a coworker?
- What makes you a great employee?

Just don't shoot yourself in the foot here. This isn't the time to show your sarcastic,
cynical side. You should write out the answers to these questions ahead of time, keeping
in mind the answers an employer is going to want to hear.

Then, it's time for you to ask question. In fact, don't even wait for the official offer
to ask your own questions, just launch right in whenever you have one that's appropriate,
or if there is any dead air time. **Your primary goal as an interviewer should be to make
the other person talk, ideally even more than you are**. People remember conversations more
favorably when they did more of the speaking. Plus, it leaves less time for them to grill
you.

Again, preparation is key here. You may be meeting up to 10 people at the company. Write
out at least 50 questions, ideally more. Think about all the processes, policies and technical
decisions you've seen in past jobs, and ask them about how they do those things.

- What technical debt are you going to be working on over the next year?
- How many hours did you work yesterday?
- Have you contributed to any open-source projects as a team?
- How often do you hang-out outside of work?
- Does your 401k provider suck as much as the rest of them?
- You glorious bastard, I read your book! Why did you say this on page 47?


# Caveats

Although live-coding on a laptop is risky, I still recommend bringing one. Occasionally
someone will ask you to solve a problem you have actually already solved in real life. In
that case, wiping out the code and taking a look at it together could both save you a
whiteboard session, and lead to a deeper conversation.

---
title: What to spend your last two weeks at a job doing
tags: career
---

So, you've given your notice at your job. Now what? First off, **stop coding**.
That's right. Finish whatever small task you're currently working on, but for
the love of god, stop writing more bugs. Of course, you want to get your manager's
sign-off on this, but in my experience managers generally accept that pumping out
just *one more feature* before you leave isn't the best use of your time.

So, what can you do to be useful?

# Write Documentation

Do you remember what documentation is? It's that thing that nobody ever produces.
You know, the stuff you wish you had when you started this job, and inherited a mess
of legacy code? (Every developer thinks that code they didn't write sucks.
The next guy is going to think you're code sucks. He's probably going to lobby to
re-write the whole damn thing.) Wouldn't it be nice if he had a road map to work with?

Never having written any documentation, some developers find themselves at a loss about
WHAT exactly they should be documenting. Maybe this will give you a starting point.

- Bookmarks
- Integrations with other systems
- Your new contact information
- How to create a new development environment from scratch
- How you deploy code
- Production redundancy configuration
- Schema migrations
- How to run any unit or automation tests
- How you typically debug the code, tools, etc
- Incident reports for past outages
- How the internationalization and localization happens
- Network diagrams
- A high level description of how all the pieces fit together
- Any reporting, especially random SQL scripts that are not in source control
- Any scheduled tasks that are running, expecially cron jobs not in source control
- How settings, entitlements and permissions work
- Anything you did that was "tricky"
---
title: The Post/Redirect/Get (PRG) Pattern
tags: django view prg
---

Anyone who has used a web browser has probably encountered the dreaded "form resubmission" dialog. This happens when the user tries to refresh or use the back button to navigate back to a HTTP POST.

![Internet Explorer resubmit form dialog](/blog/images/resubmit_ie.png)
![Chrome resubmit form dialog](/blog/images/resubmit_chrome.png)
![Firefox resubmit form dialog](/blog/images/resubmit_firefox.jpeg)

A typical case where you might see this is when checking out from a web store. Maybe you have one page that takes your shipping address, and a second page that takes your billing information. The first page submits your data with an HTTP POST, and then returns a 200 response with the payment details form HTML. If the user hits the back button in their browser, or tries to refresh the second page, they will see one of the above dialogs.

To the typical user, such a dialog is absolutely terrifying. Even if they know what it means, there is no clearly correct course of action. If they hit "Ok", they may be creating [duplicate](http://en.wikipedia.org/wiki/Post/Redirect/Get) shipping address records. Or maybe it's just updating the one they already submitted. Or maybe it will just throw an error saying they can't submit this form again. There is no way to be sure; they don't know how it's implemented on the server.

If they hit cancel, they will be greeted with an even more obscure blank browser page. Hitting back forward again will prompt them with yet another resubmit form dialog!

# The PRG Pattern

To avoid this usability issue, you want to try to keep POST events out of the browser history. Conveniently, there there is a mechanism for this that all the browsers respect. **If a HTTP POST returns a HTTP 302 redirect, only the location of the redirect will be stored in the browser history.** Hitting the back button will skip over the POST, and the user can bounce freely between the first and second forms.

# Bad code

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">view_record</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">record_id</span><span class="p">):</span>
    <span class="n">record</span> <span class="o">=</span> <span class="n">get_object_or_404</span><span class="p">(</span><span class="n">Record</span><span class="p">,</span> <span class="n">pk</span><span class="o">=</span><span class="n">record_id</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">request</span><span class="p">.</span><span class="n">method</span> <span class="o">==</span> <span class="s">"POST"</span><span class="p">:</span>
        <span class="n">record</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">POST</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"name"</span><span class="p">)</span>
        <span class="n">record</span><span class="p">.</span><span class="n">save</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">render_to_response</span><span class="p">(</span><span class="s">"page.html"</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span></code></pre></figure>

# Good code

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">view_record</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">record_id</span><span class="p">):</span>
    <span class="n">record</span> <span class="o">=</span> <span class="n">get_object_or_404</span><span class="p">(</span><span class="n">Record</span><span class="p">,</span> <span class="n">pk</span><span class="o">=</span><span class="n">record_id</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">request</span><span class="p">.</span><span class="n">method</span> <span class="o">==</span> <span class="s">"POST"</span><span class="p">:</span>
        <span class="n">record</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">POST</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"name"</span><span class="p">)</span>
        <span class="n">record</span><span class="p">.</span><span class="n">save</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">HttpResponseRedirect</span><span class="p">(</span><span class="n">reverse</span><span class="p">(</span><span class="s">"view_record"</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">record_id</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">render_to_response</span><span class="p">(</span><span class="s">"page.html"</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span></code></pre></figure>

# Caveats

* This only affects full page refreshes, Ajax calls are unaffected.
* In practice, this may sometimes require you to temporarily save state when otherwise you would not. For example, in multi-step forms. In those cases, you can often save the data safely in the [session scope](http://www.theserverside.com/news/1365146/Redirect-After-Post).


title: Logging Django shell and dbshell sessions
tags: django logging
---

The Django [shell management command](https://docs.djangoproject.com/en/dev/ref/django-admin/#shell) is a supremely useful tool for developers to explore and potentially modify data in their applications, both during development and in production. For jobs where that does not perform well enough, or you need lower level access to the data, the [dbshell command](https://docs.djangoproject.com/en/dev/ref/django-admin/#dbshell) gives you even more power.

# With great power, comes great responsibility

Every developer has been in a situation where they use their supremely powerful tools to make a supremely large mistake against production data. In those cases, although it may not save you, it can be critical to have a log of what you did, exactly. If you realize your mistake while you are in front of the console, they you have all the information you need. But what if you only realize minutes, hours or days later?

Or perhaps you just need plausible deniability. In either case, it's not too hard to hookup logging to Django's shell commands. In this post, I'll show you how to enable per-user logging into separate files, with date and time stamps.

# Shell Logging with iPython

[iPython](http://ipython.org/download.html) is a great replacement for the already pretty awesome interactive interpreter built into python. You should be running it anyway, but if you're looking for another reason, it has built-in logging capability. To get Django to use iPython for its `shell` command, you just need to have iPython installed with `pip install ipython`. Django will detect and use it.

To enable logging from Django's `shell` command, put the following in you project root directory in a file called `ipython_config.py`.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># lines of code to run at IPython startup.
</span><span class="n">c</span> <span class="o">=</span> <span class="n">get_config</span><span class="p">()</span>
<span class="n">c</span><span class="p">.</span><span class="n">InteractiveShellApp</span><span class="p">.</span><span class="n">exec_lines</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"%%logstart -t -o /var/log/django/shell-%s.log"</span> <span class="o">%</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"USER"</span><span class="p">,</span> <span class="s">"none"</span><span class="p">)]</span></code></pre></figure>

To get `shell` to recognize your config file, you need to write your own subclass. Place this in a file under your project directory called `management/commands/shell.py`, and Django will automatically pick it up. Make sure to create `__init__.py` files in the `management` and `commands` directories.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">django.core.management.commands.shell</span> <span class="kn">import</span> <span class="n">Command</span> <span class="k">as</span> <span class="n">DjangoShellCommand</span>
<span class="kn">from</span> <span class="nn">IPython.frontend.terminal.ipapp</span> <span class="kn">import</span> <span class="n">TerminalIPythonApp</span>

<span class="k">class</span> <span class="nc">Command</span><span class="p">(</span><span class="n">DjangoShellCommand</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">ipython</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">""" need to over-ride this to invoke the TerminalIPythonAdd, versus embed().
        The later does not take configuration options we require to enable logging.
        See: https://code.djangoproject.com/ticket/17078
        """</span>
        <span class="n">app</span> <span class="o">=</span> <span class="n">TerminalIPythonApp</span><span class="p">.</span><span class="n">instance</span><span class="p">()</span>
        <span class="n">app</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">argv</span><span class="o">=</span><span class="p">[])</span>
        <span class="n">app</span><span class="p">.</span><span class="n">start</span><span class="p">()</span></code></pre></figure>

You will also need to create the `/var/log/django` directory with write permissions.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">mkdir</span> /var/log/django
<span class="nb">chown </span>root:root /var/log/django
<span class="nb">chmod </span>775 /var/log/django</code></pre></figure>

# Database Shell Logging with MySQL

Assuming you're using MySQL as your database, you can enable logging by placing the following code in your `settings.py` file, bellow your `DATABASES` declaration.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">socket</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">options</span> <span class="o">=</span> <span class="n">DATABASES</span><span class="p">[</span><span class="s">'default'</span><span class="p">].</span><span class="n">get</span><span class="p">(</span><span class="s">'OPTIONS'</span><span class="p">,</span> <span class="p">{})</span>
<span class="n">hostname</span> <span class="o">=</span> <span class="n">socket</span><span class="p">.</span><span class="n">gethostname</span><span class="p">()</span>
<span class="k">if</span> <span class="s">'dbshell'</span> <span class="ow">in</span> <span class="n">sys</span><span class="p">.</span><span class="n">argv</span> <span class="ow">and</span> <span class="n">DATABASES</span><span class="p">[</span><span class="s">'default'</span><span class="p">].</span><span class="n">get</span><span class="p">(</span><span class="s">'ENGINE'</span><span class="p">).</span><span class="n">startswith</span><span class="p">(</span><span class="s">'mysql'</span><span class="p">):</span>
    <span class="c1"># have mysql executable log for us, using a per user config file
</span>    <span class="n">mycnf</span> <span class="o">=</span> <span class="s">'/tmp/%s.cnf'</span> <span class="o">%</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'USER'</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">mycnf</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
        <span class="n">contents</span> <span class="o">=</span> <span class="s">'''
[mysql]
prompt="[\D %(username)s@%(host)s]&gt; "
tee=/var/log/django/dbshell-%(username)s.log
'''</span> <span class="o">%</span> <span class="nb">dict</span><span class="p">(</span><span class="n">username</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'USER'</span><span class="p">),</span> <span class="n">host</span><span class="o">=</span><span class="n">hostname</span><span class="p">)</span>
        <span class="nb">file</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">contents</span><span class="p">)</span>
    <span class="n">options</span><span class="p">[</span><span class="s">'read_default_file'</span><span class="p">]</span> <span class="o">=</span> <span class="n">mycnf</span>
<span class="n">DATABASES</span><span class="p">[</span><span class="s">'default'</span><span class="p">][</span><span class="s">'OPTIONS'</span><span class="p">]</span> <span class="o">=</span> <span class="n">options</span></code></pre></figure>

This method also uses a logging facility built-in to an external tool, in this case the `mysql` command line interactive shell.
---
title: Using $EDITOR and a less paging from Python command line apps
tags: python git
---

Python's built-in [raw_input()](http://docs.python.org/2/library/functions.html#raw_input) function is a quick and dirty way to get text input from the user in your Python command line application. But it's really only optimal for a very small input string. Also, it can't provide a default value that the user can then edit. For more substantial input, many Linux tools use your $EDITOR environment variable to launch a visual editor, potentially with default text.

Some example are [git](http://git-scm.com/) commit messages, and `crontab -e`. A typical workflow is as follows:

1. User runs a command like `git commit`.
2. Git creates a temp file with a default commit template
2. Vim, emacs or nano opens the temp file
3. User edits the text
4. User saves and quits
5. Git reads in the new contents of the temp file, and deletes it

After [some research](http://stackoverflow.com/questions/13168083/python-raw-input-replacement-that-uses-a-configurable-text-editor), I came up with the following helper to do just that in Python:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">raw_input_editor</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">editor</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">''' like the built-in raw_input(), except that it uses a visual
    text editor for ease of editing. Unline raw_input() it can also
    take a default value. '''</span>
    <span class="k">with</span> <span class="n">tempfile</span><span class="p">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s">'r+'</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmpfile</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">default</span><span class="p">:</span>
            <span class="n">tmpfile</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">default</span><span class="p">)</span>
            <span class="n">tmpfile</span><span class="p">.</span><span class="n">flush</span><span class="p">()</span>
        <span class="n">subprocess</span><span class="p">.</span><span class="n">check_call</span><span class="p">([</span><span class="n">editor</span> <span class="ow">or</span> <span class="n">get_editor</span><span class="p">(),</span> <span class="n">tmpfile</span><span class="p">.</span><span class="n">name</span><span class="p">])</span>
        <span class="n">tmpfile</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tmpfile</span><span class="p">.</span><span class="n">read</span><span class="p">().</span><span class="n">strip</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">get_editor</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'VISUAL'</span><span class="p">)</span>
        <span class="ow">or</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'EDITOR'</span><span class="p">)</span>
        <span class="ow">or</span> <span class="s">'vi'</span><span class="p">)</span></code></pre></figure>

# Using less as a pager

With that turning out nicely, I decided to also try to copy git's pager. When you run a command like `git log` that can produce thousands of lines of text, it passes the content through `less`, which breaks it into scrollable pages. If you abstract your print statements into a logger or a custom function, you can easily enable/disable the pager.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="k">try</span><span class="p">:</span>
    <span class="c1"># args stolen fron git source, see `man less`
</span>    <span class="n">pager</span> <span class="o">=</span> <span class="n">subprocess</span><span class="p">.</span><span class="n">Popen</span><span class="p">([</span><span class="s">'less'</span><span class="p">,</span> <span class="s">'-F'</span><span class="p">,</span> <span class="s">'-R'</span><span class="p">,</span> <span class="s">'-S'</span><span class="p">,</span> <span class="s">'-X'</span><span class="p">,</span> <span class="s">'-K'</span><span class="p">],</span> <span class="n">stdin</span><span class="o">=</span><span class="n">subprocess</span><span class="p">.</span><span class="n">PIPE</span><span class="p">,</span> <span class="n">stdout</span><span class="o">=</span><span class="n">sys</span><span class="p">.</span><span class="n">stdout</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">pager</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">'This is output line %s</span><span class="se">\n</span><span class="s">'</span> <span class="o">%</span> <span class="n">num</span><span class="p">)</span>
    <span class="n">pager</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">pager</span><span class="p">.</span><span class="n">wait</span><span class="p">()</span>
<span class="k">except</span> <span class="nb">KeyboardInterrupt</span><span class="p">:</span>
    <span class="c1"># let less handle this, -K will exit cleanly</span></code></pre></figure>

# Terminal Colors

Finally, adding some color to your console text is easy with the [termcolor](http://pypi.python.org/pypi/termcolor/) library.

![Color terminal text example](/blog/images/termcolor.jpeg)
---
title: Keep Track of Vim Tabs Per Git Branch
tags: django view prg
---

[Mylyn](http://www.eclipse.org/mylyn/) is a "task lifecycle management framework" plugin for Eclipse. I'm not 100% sure what that means, but I know I really liked one particular feature. On teams where everything you worked on was a JIRA ticket, Mylyn let you associate source code files with a particular JIRA ticket. You would tell it that you were woring on ticket X, and it would keep track of which files you had open. If you started working on task X again at a later date, it could open all those same files again.

I've stopped using Eclipse and even JIRA since, but it seems like a workflow that's worth mapping over to my current editor and task groupings, namely vim and git branches. Vim has an excellent built-in "sessions" functionality, through the [mksession](http://vim.runpaint.org/editing/managing-sessions/) command. I wanted to be able to bind some keys to save the current session against the current git branch by name, and be able to restore a session for the current branch.

Here is a vimrc snippet that does just that.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">let </span>s:sessions_dir <span class="o">=</span> <span class="s2">"~/.vim/sessions/"</span>

<span class="k">function</span><span class="o">!</span> GetCurrentGitBranch<span class="o">()</span>
    <span class="k">return </span>system<span class="o">(</span><span class="s2">"git branch 2&gt; /dev/null | sed -e '/^[^*]/d' -e 's/* //'"</span><span class="o">)</span>
endfunction

<span class="k">function</span><span class="o">!</span> GetWorkingDirectory<span class="o">()</span>
    redir <span class="o">=&gt;</span> current_dir
    silent <span class="nb">pwd
    </span>redir END
    <span class="k">return </span>current_dir
endfunction

<span class="k">function</span><span class="o">!</span> GetSessionFile<span class="o">()</span>
    <span class="nb">let </span>branch <span class="o">=</span> GetCurrentGitBranch<span class="o">()</span>
    <span class="k">if </span>branch <span class="o">==</span> <span class="s2">""</span>
        <span class="nb">echo</span> <span class="s2">"No git repository at "</span> <span class="nb">.</span> GetWorkingDirectory<span class="o">()</span>
    <span class="k">else
        return </span>s:sessions_dir <span class="nb">.</span> GetCurrentGitBranch<span class="o">()</span>
    endif
    <span class="k">return</span> <span class="s2">""</span>
endfunction

<span class="k">function</span><span class="o">!</span> GitSessionSave<span class="o">()</span>
    <span class="nb">let </span>session_file <span class="o">=</span> GetSessionFile<span class="o">()</span>
    <span class="k">if </span>session_file <span class="o">!=</span> <span class="s2">""</span>
        execute <span class="s2">"mksession! "</span> <span class="nb">.</span> session_file
        <span class="nb">echo</span> <span class="s2">"Saved session to "</span> <span class="nb">.</span> session_file
    endif
endfunction

<span class="k">function</span><span class="o">!</span> GitSessionRestore<span class="o">()</span>
    <span class="nb">let </span>session_file <span class="o">=</span> GetSessionFile<span class="o">()</span>
    <span class="k">if </span>session_file <span class="o">!=</span> <span class="s2">""</span>
        execute <span class="s2">"tabo"</span>
        execute <span class="s2">"source "</span> <span class="nb">.</span> session_file
        <span class="nb">echo</span> <span class="s2">"Restored session "</span> <span class="nb">.</span> session_file
    endif
endfunction

<span class="nb">command</span><span class="o">!</span> Gss call GitSessionSave<span class="o">()</span>
<span class="nb">command</span><span class="o">!</span> Gsr call GitSessionRestore<span class="o">()</span></code></pre></figure>
---
title: Python Subprocess Asynchronous Read Stdout
tags: python subprocess threads
---

Python has a great standard library when it comes to invoking [external processes](http://docs.python.org/2/library/subprocess.html). But one weakness it does have is that it's not easy to communicate with a subprocess while it's running, i.e. streaming its stdout. If you look at the documentation for `popen`, you will repeatedly see caveats like the following from the Python docs for [Popen.communicate](http://docs.python.org/2/library/subprocess.html#subprocess.Popen.communicate):

> Interact with process: Send data to stdin. Read data from stdout and stderr, until end-of-file is reached. **Wait for process to terminate.** The optional input argument should be a string to be sent to the child process, or None, if no data should be sent to the child.

It's actually a well known problem. There is even an open enhancement proposal [PEP 3145](http://www.python.org/dev/peps/pep-3145/) to address it. But that is currently on track for Python 3.2. Python 2.x will never get that update.

Hunting around, I found a pretty decent partial solution on [Stackoverflow](http://stackoverflow.com/questions/375427/non-blocking-read-on-a-subprocess-pipe-in-python), but it took me quite a bit to tweaking to get it to work in my case.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">fcntl</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>

    <span class="n">mysql_process</span> <span class="o">=</span> <span class="n">subprocess</span><span class="p">.</span><span class="n">Popen</span><span class="p">(</span>
        <span class="p">[</span><span class="s">'mysql'</span><span class="p">,</span> <span class="s">'--user=%s'</span> <span class="o">%</span> <span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s">'--password=%s'</span> <span class="o">%</span> <span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s">'--batch'</span><span class="p">,</span> <span class="s">'--skip-tee'</span><span class="p">,</span> <span class="s">'--skip-pager'</span><span class="p">,</span> <span class="s">'--unbuffered'</span><span class="p">]</span>
        <span class="n">stdin</span><span class="o">=</span><span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">,</span>
        <span class="n">stdout</span><span class="o">=</span><span class="n">subprocess</span><span class="p">.</span><span class="n">PIPE</span><span class="p">,</span>
        <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="p">.</span><span class="n">PIPE</span><span class="p">)</span>

    <span class="n">thread</span> <span class="o">=</span> <span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">log_worker</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">mysql_process</span><span class="p">.</span><span class="n">stdout</span><span class="p">])</span>
    <span class="n">thread</span><span class="p">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">thread</span><span class="p">.</span><span class="n">start</span><span class="p">()</span>

    <span class="n">mysql_process</span><span class="p">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="n">thread</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">log_worker</span><span class="p">(</span><span class="n">stdout</span><span class="p">):</span>
    <span class="s">''' needs to be in a thread so we can read the stdout w/o blocking '''</span>
    <span class="n">username</span><span class="p">,</span> <span class="n">hostname</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'USER'</span><span class="p">),</span> <span class="n">socket</span><span class="p">.</span><span class="n">gethostname</span><span class="p">()</span>
    <span class="n">log_file</span> <span class="o">=</span> <span class="s">'/var/log/mysql-%s.log'</span> <span class="o">%</span> <span class="n">username</span>
    <span class="n">log</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">log_file</span><span class="p">,</span> <span class="s">'a'</span><span class="p">)</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">non_block_read</span><span class="p">(</span><span class="n">stdout</span><span class="p">).</span><span class="n">strip</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">output</span><span class="p">:</span>
            <span class="s">''' [Tue Oct 30 22:13:13 2012 cseibert@host1]&gt; '''</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="s">'[%(timestamp)s %(username)s@%(host)s]&gt; </span><span class="se">\n</span><span class="s">'</span> <span class="o">%</span> <span class="nb">dict</span><span class="p">(</span>
                    <span class="n">timestamp</span><span class="o">=</span><span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%a %b %d %H:%M:%S %Y'</span><span class="p">),</span>
                    <span class="n">username</span><span class="o">=</span><span class="n">username</span><span class="p">,</span>
                    <span class="n">host</span><span class="o">=</span><span class="n">hostname</span><span class="p">)</span>
            <span class="k">print</span> <span class="n">prompt</span> <span class="o">+</span> <span class="n">output</span>
            <span class="n">log</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">prompt</span> <span class="o">+</span> <span class="n">output</span> <span class="o">+</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="n">log</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">non_block_read</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="s">''' even in a thread, a normal read with block until the buffer is full '''</span>
    <span class="n">fd</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">fileno</span><span class="p">()</span>
    <span class="n">fl</span> <span class="o">=</span> <span class="n">fcntl</span><span class="p">.</span><span class="n">fcntl</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="n">fcntl</span><span class="p">.</span><span class="n">F_GETFL</span><span class="p">)</span>
    <span class="n">fcntl</span><span class="p">.</span><span class="n">fcntl</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="n">fcntl</span><span class="p">.</span><span class="n">F_SETFL</span><span class="p">,</span> <span class="n">fl</span> <span class="o">|</span> <span class="n">os</span><span class="p">.</span><span class="n">O_NONBLOCK</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">return</span> <span class="s">''</span></code></pre></figure>

As you can see, this code is invoking the `mysql` command-line client in batch mode. I'm piping a file into my python script, and then turning around and piping that into mysql. I start the mysql subprocess, but at the same time I'm spinning off a worker thread to read its output. Additionally, I'm re-opening stdout in non-blocking mode, so I don't have to wait for a buffer to fill up before I can read a chunk.

Then I'm reading the mysql output and writing it both to the console, and to a log file. For this application, it's critical that the mysql output be shown on the screen as it's running. What if there's an exception; the user will want to terminate before it runs any further.

Doesn't mysql provide logging by default? Yes, but only for interactive (i.e., non-batch) sessions. From the mysql command-line tool docs:

> By using the --tee option when you invoke mysql, you can log statements and their output. All the data displayed on the screen is appended into a given file. This can be very useful for debugging purposes also. mysql flushes results to the file after each statement, just before it prints its next prompt. **Tee functionality works only in interactive mode.**

One additional caveat; you need to make sure that the subprocess you are invoking is not doing its own buffering. It took me a bit to figure out that mysql does do that, which is what the `--unbuffered` flag is there to disable.
---
title: Introducing Django Pyfixtures
tags: django fixtures
---

[Django fixtures](https://docs.djangoproject.com/en/dev/howto/initial-data/) were initially touted as a great way to pre-populate your database, mainly for testing. Over time, various community leaders have suggested that fixtures are [slow](http://pyvideo.org/video/699/testing-and-django), [brittle](http://lincolnloop.com/blog/2012/may/3/fixtures-and-factories/), should be [bundled](http://nedbatchelder.com/blog/201206/tldw_speedily_practical_largescale_tests.html) instead of loaded from scratch for every unit test and should probably be replaced with class [factories](https://github.com/dnerdy/factory_boy).

If you're starting from scratch, that's great advice. But how do you get there if you already have a bunch of fixtures? Starting today, you can use [django-pyfixtures](https://github.com/chase-seibert/django-pyfixtures) to convert your json fixtures to python code.

> Using the regular Django dumpdata command, pyfixtures will generate a python file that contains all the code necessary to re-constitute that data in an empty database. You can take that code and refactor it into something you maintain going forward, or you can re-generate it from a target database when needed.

# How it works

Pyfixtures implements a new Django serializer that takes a model object stream and produces python constructors for that code. It ends up looking just as it would if you wrote the code by hand, complete with necessary imports, declaring models that other models depend on first, and using previously declared variables for foreign keys. It can also deal with circular references by letting the user decide which models to use primary keys for, instead of references.

Here is an example of what a fixture might look like.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">pytz</span>
<span class="kn">from</span> <span class="nn">models</span> <span class="kn">import</span> <span class="n">Organization</span>
<span class="kn">from</span> <span class="nn">models</span> <span class="kn">import</span> <span class="n">Group</span>
<span class="kn">from</span> <span class="nn">models</span> <span class="kn">import</span> <span class="n">Contact</span>


<span class="n">organization1</span> <span class="o">=</span> <span class="n">Organization</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">create_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2011</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tzinfo</span><span class="o">=</span><span class="n">pytz</span><span class="p">.</span><span class="n">timezone</span><span class="p">(</span><span class="s">'UTC'</span><span class="p">)),</span>
    <span class="n">name</span><span class="o">=</span><span class="sa">u</span><span class="s">'Marvel Comics'</span><span class="p">)</span>

<span class="n">group1</span> <span class="o">=</span> <span class="n">Group</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">create_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2011</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">59</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="n">tzinfo</span><span class="o">=</span><span class="n">pytz</span><span class="p">.</span><span class="n">timezone</span><span class="p">(</span><span class="s">'UTC'</span><span class="p">)),</span>
    <span class="n">name</span><span class="o">=</span><span class="sa">u</span><span class="s">'The Avengers'</span><span class="p">,</span>
    <span class="n">organization</span><span class="o">=</span><span class="n">organization1</span><span class="p">)</span>

<span class="n">contact1</span> <span class="o">=</span> <span class="n">Contact</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">group</span><span class="o">=</span><span class="n">group1</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="sa">u</span><span class="s">'Captain America'</span><span class="p">,</span>
    <span class="n">organization</span><span class="o">=</span><span class="n">organization1</span><span class="p">,</span>
    <span class="n">create_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2010</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">tzinfo</span><span class="o">=</span><span class="n">pytz</span><span class="p">.</span><span class="n">timezone</span><span class="p">(</span><span class="s">'UTC'</span><span class="p">)))</span>

<span class="n">contact2</span> <span class="o">=</span> <span class="n">Contact</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">group</span><span class="o">=</span><span class="n">group1</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="sa">u</span><span class="s">'Iron Man'</span><span class="p">,</span>
    <span class="n">organization</span><span class="o">=</span><span class="n">organization1</span><span class="p">,</span>
    <span class="n">create_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2010</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">tzinfo</span><span class="o">=</span><span class="n">pytz</span><span class="p">.</span><span class="n">timezone</span><span class="p">(</span><span class="s">'UTC'</span><span class="p">)))</span></code></pre></figure>

# Getting Started

- `pip install django-pyfixtures`
- Edit `settings.py`

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">INSTALLED_APPS</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">...</span>
    <span class="s">'pyfixtures'</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">...</span>
<span class="n">SERIALIZATION_MODULES</span> <span class="o">=</span> <span class="p">{</span><span class="s">'py'</span><span class="p">:</span> <span class="s">'pyfixtures.serializer'</span><span class="p">}</span></code></pre></figure>

- Convert your existing fixtures to python, and test them

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">./manage.py loaddata fixtures/initial_data.json
./manage.py dumpdata <span class="nt">--exclude</span> contenttypes <span class="nt">--format</span><span class="o">=</span>py <span class="o">&gt;</span> fixtures/initial_data.py
./manage.py loaddata fixtures/initial_data.py</code></pre></figure>
---
title: Django Nose Lint Quickstart - Enforce fast unit tests
tags: django testing
---

In the battle for blazing fast unit tests, you need all the tools you can get at your disposal. Enter [django-nose-lint](https://github.com/chase-seibert/django-nose-lint), a new [Nose](https://nose.readthedocs.org/en/latest/) plugin that lets you enforce certain runtime constraints on your test suite. You can just flat out fail tests that take over a configurable amount of time. You can also get more granular and fail tests that try to do certain slow stuff. For example, using the [Django test client](https://docs.djangoproject.com/en/dev/topics/testing/#module-django.test.client).

Here are the things it can currently check for:

- ESOK = Used a TCP socket
- ECLI = Used the Django Test Client
- ETEM = Tried to render a Django template
- ESLO = Test took over 1 second (takes --maxms argument)
- EALL = All of the above

## Quickstart for exsting Django projects already using django-nose

If you're already running `django-nose` on your Django app, all you need to do is install `django-nose-lint`, and run your tests with the `--lint` flag.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">pip <span class="nb">install </span>django-nose-lint
./manage.py <span class="nb">test</span> <span class="nt">--lint</span><span class="o">=</span>EALL</code></pre></figure>

## Starting from scratch on a new Django project

This is mostly an re-hash of the [django-nose setup docs](https://github.com/jbalogh/django-nose#readme). I assume you have [virtualenv](http://pypi.python.org/pypi/virtualenv) installed, and want to use it to isolate this skunkworks from your existing python stuff.

First, setup a new Django project inside `virtualenv`.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">mkdir </span>django-nose-lint-test
<span class="nb">cd </span>django-nose-lint-test/
virtualenv <span class="nt">--no-site-packages</span> <span class="nt">--distribute</span> virtualenv
<span class="nb">source </span>virtualenv/bin/activate
pip <span class="nb">install </span>django nose django-nose django-nose-lint
django-admin.py startproject mysite
<span class="nb">cd </span>mysite/</code></pre></figure>

Then, edit your Django `settings.py` file to do basic setup, and attach `django-nose`.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">DATABASES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'default'</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">'ENGINE'</span><span class="p">:</span> <span class="s">'django.db.backends.sqlite3'</span><span class="p">,</span>
        <span class="s">'NAME'</span><span class="p">:</span> <span class="s">''</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">INSTALLED_APPS</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">...</span>
    <span class="s">'django_nose'</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">TEST_RUNNER</span> <span class="o">=</span> <span class="s">'django_nose.NoseTestSuiteRunner'</span></code></pre></figure>

Create a basic test that we want to fail. Create this as `mysite/tests.py`. It should be along-side `urls.py` in the directory structure.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">unittest</span>
<span class="kn">import</span> <span class="nn">time</span>


<span class="k">class</span> <span class="nc">MyTest</span><span class="p">(</span><span class="n">unittest</span><span class="p">.</span><span class="n">TestCase</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">test_one</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">time</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">1.1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">assertTrue</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span></code></pre></figure>

Run your test suite.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">&gt;</span>python manage.py <span class="nb">test</span> <span class="nt">--lint</span><span class="o">=</span>EALL
Creating <span class="nb">test </span>database <span class="k">for </span><span class="nb">alias</span> <span class="s1">'default'</span>...
.E
<span class="o">======================================================================</span>
ERROR: test_one <span class="o">(</span>tests.MyTest<span class="o">)</span>
<span class="nt">----------------------------------------------------------------------</span>
Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
  File <span class="s2">"/Users/cseibert/projects/django-nose-lint-test/virtualenv/lib/python2.7/site-packages/nose/case.py"</span>, line 133, <span class="k">in </span>run
    self.runTest<span class="o">(</span>result<span class="o">)</span>
  File <span class="s2">"/Users/cseibert/projects/django-nose-lint-test/virtualenv/lib/python2.7/site-packages/nose/case.py"</span>, line 151, <span class="k">in </span>runTest
    <span class="nb">test</span><span class="o">(</span>result<span class="o">)</span>
  File <span class="s2">"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py"</span>, line 376, <span class="k">in </span>__call__
    <span class="k">return </span>self.run<span class="o">(</span><span class="k">*</span>args, <span class="k">**</span>kwds<span class="o">)</span>
  File <span class="s2">"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py"</span>, line 355, <span class="k">in </span>run
    result.stopTest<span class="o">(</span>self<span class="o">)</span>
  File <span class="s2">"/Users/cseibert/projects/django-nose-lint-test/virtualenv/lib/python2.7/site-packages/nose/proxy.py"</span>, line 180, <span class="k">in </span>stopTest
    self.plugins.stopTest<span class="o">(</span>self.test<span class="o">)</span>
  File <span class="s2">"/Users/cseibert/projects/django-nose-lint-test/virtualenv/lib/python2.7/site-packages/nose/plugins/manager.py"</span>, line 99, <span class="k">in </span>__call__
    <span class="k">return </span>self.call<span class="o">(</span><span class="k">*</span>arg, <span class="k">**</span>kw<span class="o">)</span>
  File <span class="s2">"/Users/cseibert/projects/django-nose-lint-test/virtualenv/lib/python2.7/site-packages/nose/plugins/manager.py"</span>, line 167, <span class="k">in </span>simple
    result <span class="o">=</span> meth<span class="o">(</span><span class="k">*</span>arg, <span class="k">**</span>kw<span class="o">)</span>
  File <span class="s2">"/Users/cseibert/projects/django-nose-lint-test/virtualenv/lib/python2.7/site-packages/noselint/__init__.py"</span>, line 76, <span class="k">in </span>stopTest
    raise DeprecationWarning<span class="o">(</span><span class="s1">'DjangoNoseLint Error: ESLO - test took %s ms'</span> % delta_ms<span class="o">)</span>
DeprecationWarning: DjangoNoseLint Error: ESLO - <span class="nb">test </span>took 1101 ms

<span class="nt">----------------------------------------------------------------------</span>
Ran 1 <span class="nb">test </span><span class="k">in </span>1.104s

FAILED <span class="o">(</span><span class="nv">errors</span><span class="o">=</span>1<span class="o">)</span></code></pre></figure>

If you run without the `--lint` flag, the same test should pass.

## Call for ideas

I'm looking for some more ideas of things to fail in unit tests. Please [submit a bug](https://github.com/chase-seibert/django-nose-lint/issues) to the tracker if you have any ideas.


---
title: Read only Django shell
tags: django shell
---

Say you have a bunch of developers that occasionally need Django shell access to production, but you want this to be an exceptional event. Here is a drop-in replacement for `./manage.py shell` that defaults to read-only mode, but lets the developer switch to writable mode on the fly, while notifying the team.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">optparse</span> <span class="kn">import</span> <span class="n">make_option</span>
<span class="kn">from</span> <span class="nn">django.core.management.commands.shell</span> <span class="kn">import</span> <span class="n">Command</span> <span class="k">as</span> <span class="n">DjangoShellCommand</span>
<span class="kn">from</span> <span class="nn">django.db</span> <span class="kn">import</span> <span class="n">router</span>
<span class="kn">from</span> <span class="nn">django.core.management.base</span> <span class="kn">import</span> <span class="n">NoArgsCommand</span>
<span class="kn">from</span> <span class="nn">myapp.utils</span> <span class="kn">import</span> <span class="n">hipchat</span>


<span class="n">_original_db_for_write</span> <span class="o">=</span> <span class="bp">None</span>


<span class="k">def</span> <span class="nf">confirm_writable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="s">''' user can over-ride the shell to be writable at any time, but it sends a message '''</span>

    <span class="c1"># for migrations, you might be in a non-interactive shell
</span>    <span class="c1"># so don't prompt, but still send out the notification
</span>    <span class="k">if</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">isatty</span><span class="p">():</span>
        <span class="n">cont</span> <span class="o">=</span> <span class="nb">raw_input</span><span class="p">(</span><span class="s">'Are you sure you want to connect to the production database in writable mode? [y/N] '</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">cont</span><span class="p">.</span><span class="n">lower</span><span class="p">().</span><span class="n">startswith</span><span class="p">(</span><span class="s">'y'</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nb">IOError</span><span class="p">(</span><span class="s">'Database in read-only mode.'</span><span class="p">)</span>

    <span class="n">router</span><span class="p">.</span><span class="n">db_for_write</span> <span class="o">=</span> <span class="n">_original_db_for_write</span>
    <span class="n">send_alert</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">send_alert</span><span class="p">():</span>
    <span class="n">hipchat</span><span class="p">.</span><span class="n">send_message</span><span class="p">(</span><span class="s">"I'm opening up a writable prod shell!"</span><span class="p">,</span>
        <span class="n">from_name</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'USER'</span><span class="p">),</span>
        <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Command</span><span class="p">(</span><span class="n">DjangoShellCommand</span><span class="p">):</span>

    <span class="n">option_list</span> <span class="o">=</span> <span class="n">DjangoShellCommand</span><span class="p">.</span><span class="n">option_list</span> <span class="o">+</span> <span class="p">(</span>
        <span class="n">make_option</span><span class="p">(</span><span class="s">'--write'</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s">'store_true'</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s">'writable'</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s">'Connect to the database in writable mode.'</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">handle_noargs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>

        <span class="c1"># only allow read-only shells in prod by default
</span>        <span class="k">if</span> <span class="n">options</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'writable'</span><span class="p">):</span>
            <span class="n">send_alert</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">global</span> <span class="n">_original_db_for_write</span>
            <span class="n">_original_db_for_write</span> <span class="o">=</span> <span class="n">router</span><span class="p">.</span><span class="n">db_for_write</span>
            <span class="n">router</span><span class="p">.</span><span class="n">db_for_write</span> <span class="o">=</span> <span class="n">confirm_writable</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Command</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">handle_noargs</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span></code></pre></figure>

The strategy here is to use Django's [database router](https://docs.djangoproject.com/en/dev/topics/db/multi-db/#using-routers) mechanism to throw an exception when trying to write to the database.

# Install

Drop this into your project as `myapp/management/commands/shell.py` and it will over-ride the default shell command.

# Hipchat

In my case, I'm notifying the team via [Hipchat](https://www.hipchat.com/). Of course, you can replace this function with a version that sends out an email, etc. If you're curious, here is the hipchat code:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">urllib</span>
<span class="kn">import</span> <span class="nn">urllib2</span>


<span class="k">def</span> <span class="nf">_make_hipchat_request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">auth_token</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">auth_token</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">django.conf</span> <span class="kn">import</span> <span class="n">settings</span>
        <span class="n">auth_token</span> <span class="o">=</span> <span class="n">settings</span><span class="p">.</span><span class="n">HIPCHAT_TOKEN</span>
    <span class="n">HIPCHAT_BASE_URL</span> <span class="o">=</span> <span class="s">"https://api.hipchat.com/v1"</span>
    <span class="n">final_url</span> <span class="o">=</span> <span class="s">"%s%s%sauth_token=%s"</span> <span class="o">%</span> <span class="p">(</span>
        <span class="n">HIPCHAT_BASE_URL</span><span class="p">,</span>
        <span class="n">url</span><span class="p">,</span>
        <span class="s">'&amp;'</span> <span class="k">if</span> <span class="s">'?'</span> <span class="ow">in</span> <span class="n">url</span> <span class="k">else</span> <span class="s">'?'</span><span class="p">,</span>
        <span class="n">auth_token</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">urllib2</span><span class="p">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">final_url</span><span class="p">).</span><span class="n">read</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">room_id</span><span class="o">=</span><span class="s">'Engineering'</span><span class="p">,</span> <span class="n">from_name</span><span class="o">=</span><span class="s">'Django'</span><span class="p">,</span> <span class="n">auth_token</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'yellow'</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="s">'/rooms/message?message=%s&amp;room_id=%s&amp;from=%s&amp;message_format=text&amp;color=%s'</span> <span class="o">%</span> <span class="p">(</span>
        <span class="n">urllib</span><span class="p">.</span><span class="n">quote</span><span class="p">(</span><span class="n">message</span><span class="p">),</span>
        <span class="n">urllib</span><span class="p">.</span><span class="n">quote</span><span class="p">(</span><span class="n">room_id</span><span class="p">),</span>
        <span class="n">urllib</span><span class="p">.</span><span class="n">quote</span><span class="p">(</span><span class="n">from_name</span><span class="p">),</span>
        <span class="n">urllib</span><span class="p">.</span><span class="n">quote</span><span class="p">(</span><span class="n">color</span><span class="p">))</span>
    <span class="n">_make_hipchat_request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">auth_token</span><span class="p">)</span></code></pre></figure>
---
title: New Year Python Meme 2012
tags: python pythonmeme
---

View all [#2012pythonmeme](https://twitter.com/search?q=%232012pythonmeme) posts by the community.

**1. What's the coolest Python application, framework, or library you have discovered in 2012?**

Definitely [celery](https://github.com/celery/celery). It challenges you to compose your solutions into ever smaller tasks. It's sometimes frustrating as hell to debug (mostly because it's naturally UI-less). But it has been an absolutely essential piece of several large projects in the last year.

**2. What new programming technique did you learn in 2012?**

[Monkey patching](http://stackoverflow.com/questions/5626193/what-is-monkey-patching). After a couple years of using Python as if it was Java, I finally started to grok the power of a dynamic language in 2012. I then proceeded to perhaps over-use it, by [over-writing python internals](https://github.com/chase-seibert/django-nose-lint/blob/master/noselint/__init__.py), reaching into [framework internals](http://chase-seibert.github.com/blog/2012/06/05/django-nosesqlite3-too-many-sql-variables-error.html), replacing [code in memory at runtime](http://chase-seibert.github.com/blog/2012/12/21/read-only-django-shell.html) and [fabricating look-alike objects](http://chase-seibert.github.com/blog/2012/07/27/faster-django-view-unit-tests-with-mocks.html).

**3. Which open source project did you contribute to the most in 2012? What did you do?**

I had a couple of [patches](https://github.com/celery/celery/issues/447) for celery and encouraged a co-worker to [contribute back](https://code.djangoproject.com/ticket/19385) to Django. But mostly I open-sourced a couple of new things I was working on, [django-pyfixtures](https://github.com/chase-seibert/django-pyfixtures) and [django-nose-lint](https://github.com/chase-seibert/django-nose-lint).

**4. Which Python blog or website did you read the most in 2012?**

In terms of volume, it was the Reddit's [r/python](http://www.reddit.com/r/Python/). But in terms of value, it would have to be [my twitter steam](https://twitter.com/chase_seibert/following).

**5. What are the top things you want to learn in 2013?**

I need to re-engage on client side technologies, which have made tremendous progress in the last couple of years while I've been focussed on server-side stuff. I'm particularly interesting in actually using stuff like [backbone](http://backbonejs.org/), [node.js](http://nodejs.org/) and [meteorjs](http://meteor.com/) in production.

**6. What is the top software, application, or library you wish someone would write in 2013?**

An open-source replacement for [Splunk](http://www.splunk.com/). I've rarely used a product so well designed and functional. Yet I can't deal with the high cost and licensing headaches anymore.
---
title: Ignore pyflakes warnings with bypass_pyflakes
tags: python pyflakes
---

[Pyflakes](https://github.com/kevinw/pyflakes) is a popular linter for python, even if it [isn't being maintained](https://github.com/kevinw/pyflakes/commits/master) anymore. One [long](http://stackoverflow.com/questions/5033727/how-do-i-get-pyflakes-to-ignore-a-statement) [standing](https://github.com/kevinw/pyflakes/pull/22) request is to allow ignoring of specific warnings with comments, like many other linters allow.

For example, it's common in python config files to use `import *`. But you definitely [don't want](http://pythonconquerstheuniverse.wordpress.com/2011/03/28/why-import-star-is-a-bad-idea/) to allow that in most places.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">form</span> <span class="n">settings</span><span class="p">.</span><span class="n">base</span> <span class="k">import</span> <span class="o">*</span>
<span class="n">MY_OVERIDE</span> <span class="o">=</span> <span class="s">'foobar'</span>  <span class="c1"># over-ride this setting from base</span></code></pre></figure>

This results in the warning `settings/local.py:1: 'from settings.base import *' used; unable to detect undefined names`.

It would be nice to be able to do the following.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">form</span> <span class="n">settings</span><span class="p">.</span><span class="n">base</span> <span class="k">import</span> <span class="o">*</span>  <span class="c1"># bypass_pyflakes
</span><span class="n">MY_OVERIDE</span> <span class="o">=</span> <span class="s">'foobar'</span>  <span class="c1"># over-ride this setting from base</span></code></pre></figure>

Here is a python script that monkey patches pyflakes to add this functionality.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#!/usr/bin/env python
</span>
<span class="kn">from</span> <span class="nn">pyflakes.scripts</span> <span class="kn">import</span> <span class="n">pyflakes</span>
<span class="kn">from</span> <span class="nn">pyflakes.checker</span> <span class="kn">import</span> <span class="n">Checker</span>


<span class="k">def</span> <span class="nf">report_with_bypass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messageClass</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">text_lineno</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">filename</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">code</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">code</span><span class="p">.</span><span class="n">readlines</span><span class="p">()[</span><span class="n">text_lineno</span><span class="p">].</span><span class="n">find</span><span class="p">(</span><span class="s">'bypass_pyflakes'</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">messageClass</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">filename</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>

<span class="c1"># monkey patch checker to support bypass
</span><span class="n">Checker</span><span class="p">.</span><span class="n">report</span> <span class="o">=</span> <span class="n">report_with_bypass</span>

<span class="n">pyflakes</span><span class="p">.</span><span class="n">main</span><span class="p">()</span></code></pre></figure>

If you save this as `pyflakes-bypass.py`, instead of running the `pyflakes <path>` command directly, you would now run `pyflakes-bypass.py <path>`.
---
title: Per-request Query Caching in Django
tags: python cache
---

The Django ORM is a wonderful thing. It makes it so easy to access the database, that sometimes you forget that it's even happening. That is, until you open [django-debug-toolbar](https://github.com/django-debug-toolbar/django-debug-toolbar) and see that you're suddenly running hundreds of queries! Not only that, but looking at the actual queries, many of them are duplicates! You think "Where did all these queries come from? Stupid coworkers, not writing efficient code!" Then you inevitably realize that half of the extra queries were ones you wrote yourself. How does this happen?

It's all too easy. Maybe you have a `User` object with a helper method on it that performs a join to get their recent activity. You're passing `user` instances around in many of your method calls. So as not to assume a wider contract than necessary with the caller, utility methods all over the place are calling this helper method. Your code is nice and tight; you're not repeating yourself anywhere, but some page requests are calling this function from various places in the stack half a dozen times!

Why is this a big deal? After the first query, the database will probably have a nice warm version in its cache. What you will likely see in the debug toolbar is that many of your duplicate queries will return in less than 2 milliseconds. However, any latency to the database server [can still kill you](http://chase-seibert.github.com/blog/2011/10/07/django-performance-latency-kills.html). Plus, even tiny queries are still causing some contention and load on the database.

There are [various](http://packages.python.org/johnny-cache/) [existing](https://github.com/dziegler/django-cachebot) [solutions](http://pypi.python.org/pypi/django-cache-machine) for query caching in Django. In general, they all require that you expire cache results manually if you have edge cases like writing data to your database. In other words, they are likely to introduce bugs.

What I have come up with is a monkey-patch for some Django internals to cache the results of individual SQL statements, but only inside the life cycle of a single request. This will take zero load off of your database if you have perfect code. For mere mortals, it could likely reduce your database calls significantly.

You start by adding a piece of middlware:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">myapp.utils</span> <span class="kn">import</span> <span class="n">query_cache</span>


<span class="k">class</span> <span class="nc">QueryCacheMiddleware</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">):</span>
        <span class="n">query_cache</span><span class="p">.</span><span class="n">patch</span><span class="p">()</span></code></pre></figure>

Then, you have to enable that middleware in `settings.py`:

<figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="n">MIDDLEWARE_CLASSES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">...</span>
    <span class="s">'myapp.middleware.QueryCacheMiddleware'</span><span class="p">,</span></code></pre></figure>

Finally, here is the `query_cache` patch itself.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="s">'''
Hack to cache SELECT statements inside a single Django request. The patch() method replaces
the Django internal execute_sql method with a stand-in called execute_sql_cache. That method
looks at the sql to be run, and if it's a select statement, it checks a thread-local cache first.
Only if it's not found in the cache does it proceed to execute the SQL. On any other type of
sql statement, it blows away the cache. There is some logic to not cache large result sets,
meaning anything over 100 records. This is to preserve Django's lazy query set evaluation.
'''</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">local</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">from</span> <span class="nn">django.db.models.sql.constants</span> <span class="kn">import</span> <span class="n">MULTI</span>
<span class="kn">from</span> <span class="nn">django.db.models.sql.compiler</span> <span class="kn">import</span> <span class="n">SQLCompiler</span>
<span class="kn">from</span> <span class="nn">django.db.models.sql.datastructures</span> <span class="kn">import</span> <span class="n">EmptyResultSet</span>
<span class="kn">from</span> <span class="nn">django.db.models.sql.constants</span> <span class="kn">import</span> <span class="n">GET_ITERATOR_CHUNK_SIZE</span>


<span class="n">_thread_locals</span> <span class="o">=</span> <span class="n">local</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">get_sql</span><span class="p">(</span><span class="n">compiler</span><span class="p">):</span>
    <span class="s">''' get a tuple of the SQL query and the arguments '''</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">compiler</span><span class="p">.</span><span class="n">as_sql</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">EmptyResultSet</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">return</span> <span class="p">(</span><span class="s">''</span><span class="p">,</span> <span class="p">[])</span>


<span class="k">def</span> <span class="nf">execute_sql_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result_type</span><span class="o">=</span><span class="n">MULTI</span><span class="p">):</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">_thread_locals</span><span class="p">,</span> <span class="s">'query_cache'</span><span class="p">):</span>

        <span class="n">sql</span> <span class="o">=</span> <span class="n">get_sql</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>  <span class="c1"># ('SELECT * FROM ...', (50)) &lt;= sql string, args tuple
</span>        <span class="k">if</span> <span class="n">sql</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">6</span><span class="p">].</span><span class="n">upper</span><span class="p">()</span> <span class="o">==</span> <span class="s">'SELECT'</span><span class="p">:</span>

            <span class="c1"># uses the tuple of sql + args as the cache key
</span>            <span class="k">if</span> <span class="n">sql</span> <span class="ow">in</span> <span class="n">_thread_locals</span><span class="p">.</span><span class="n">query_cache</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">_thread_locals</span><span class="p">.</span><span class="n">query_cache</span><span class="p">[</span><span class="n">sql</span><span class="p">]</span>

            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_execute_sql</span><span class="p">(</span><span class="n">result_type</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="s">'next'</span><span class="p">):</span>

                <span class="c1"># only cache if this is not a full first page of a chunked set
</span>                <span class="n">peek</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="nb">next</span><span class="p">()</span>
                <span class="n">result</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="p">.</span><span class="n">chain</span><span class="p">([</span><span class="n">peek</span><span class="p">],</span> <span class="n">result</span><span class="p">))</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">peek</span><span class="p">)</span> <span class="o">==</span> <span class="n">GET_ITERATOR_CHUNK_SIZE</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">result</span>

            <span class="n">_thread_locals</span><span class="p">.</span><span class="n">query_cache</span><span class="p">[</span><span class="n">sql</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span>

            <span class="k">return</span> <span class="n">result</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># the database has been updated; throw away the cache
</span>            <span class="n">_thread_locals</span><span class="p">.</span><span class="n">query_cache</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_execute_sql</span><span class="p">(</span><span class="n">result_type</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">patch</span><span class="p">():</span>
    <span class="s">''' patch the django query runner to use our own method to execute sql '''</span>
    <span class="n">_thread_locals</span><span class="p">.</span><span class="n">query_cache</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">SQLCompiler</span><span class="p">,</span> <span class="s">'_execute_sql'</span><span class="p">):</span>
        <span class="n">SQLCompiler</span><span class="p">.</span><span class="n">_execute_sql</span> <span class="o">=</span> <span class="n">SQLCompiler</span><span class="p">.</span><span class="n">execute_sql</span>
        <span class="n">SQLCompiler</span><span class="p">.</span><span class="n">execute_sql</span> <span class="o">=</span> <span class="n">execute_sql_cache</span></code></pre></figure>

What's going on here is that I'm replacing the Django internal `execute_sql` method with a wrapper that caches results in a thread local dictionary. It only caches small result sets. For any result more that 100 rows, Django will fire up a database cursor and a generator. Caching those without eagerly querying for the entire dataset would be [quite tricky](http://jeffelmore.org/2010/09/25/smarter-caching-of-django-querysets/), so I bail in that case. I have noticed that in my codebase, the majority of repeated calls are for a single record, or a small set of records.

So as not to have to deal with any tricky invalidation cases, I simply delete the cache if any UPDATE, INSERT or DELETE statement is run.

Of course, this will not work if you have long running page requests that purposely make the same request over and over, waiting for a particular result.
---
title: Mocking HTTP calls in Python tests
tags: python
---

There are at least a few decent libraries out there for mocking out HTTP calls in Python unit tests. The best solution looks like [HTTPretty](https://github.com/gabrielfalcao/HTTPretty). One feature that it does not have, however, is the ability to [specify url parameters](https://github.com/gabrielfalcao/HTTPretty/issues/25). For many applications, such as testing OAuth flows, a lot of the behavior you are trying to validate involves parameters being passed. At the same time, you don't want to be forced to specify _all_ the parameters. For example, the `oauth_timestamp` changes for every REST call; it's dynamic based on the system clock.

Here is a quick class that can mock `urllib2` requests, and lets you specify some parameters that you want to validate are being passed. Any parametes that you don't specify are allowed.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">urllib2</span>
<span class="kn">from</span> <span class="nn">StringIO</span> <span class="kn">import</span> <span class="n">StringIO</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">httplib</span>
<span class="kn">import</span> <span class="nn">urlparse</span>
<span class="kn">import</span> <span class="nn">urllib</span>


<span class="k">class</span> <span class="nc">MockHTTPHandler</span><span class="p">(</span><span class="n">urllib2</span><span class="p">.</span><span class="n">HTTPSHandler</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">requests</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">responses</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">calls_made</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">hash_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="s">''' takes an args dict and makes a hashable key, normalizing order '''</span>
        <span class="k">return</span> <span class="n">urllib</span><span class="p">.</span><span class="n">urlencode</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">mock</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="p">{},</span> <span class="n">status_code</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'data'</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">fixture</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'fixture'</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">fixture</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">fixture</span><span class="p">).</span><span class="n">read</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">json_data</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'json_data'</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">json_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">json_data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'must pass either data or fixture argument'</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">url</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">responses</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">responses</span><span class="p">[</span><span class="n">url</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">responses</span><span class="p">[</span><span class="n">url</span><span class="p">][</span><span class="n">MockHTTPHandler</span><span class="p">.</span><span class="n">hash_args</span><span class="p">(</span><span class="n">validate_args</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span><span class="n">status_code</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">https_open</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">req</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">http_open</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">http_open</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">req</span><span class="p">):</span>
        <span class="n">url_with_args</span> <span class="o">=</span> <span class="n">req</span><span class="p">.</span><span class="n">get_full_url</span><span class="p">()</span>
        <span class="n">parsed</span> <span class="o">=</span> <span class="n">urlparse</span><span class="p">.</span><span class="n">urlparse</span><span class="p">(</span><span class="n">url_with_args</span><span class="p">)</span>
        <span class="n">url</span> <span class="o">=</span> <span class="n">url_with_args</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'?'</span> <span class="o">+</span> <span class="n">parsed</span><span class="p">.</span><span class="n">query</span><span class="p">,</span> <span class="s">''</span><span class="p">)</span>
        <span class="n">actual_args</span> <span class="o">=</span> <span class="n">urlparse</span><span class="p">.</span><span class="n">parse_qs</span><span class="p">(</span><span class="n">parsed</span><span class="p">.</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">url</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">responses</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">args_qs</span><span class="p">,</span> <span class="p">(</span><span class="n">status_code</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">responses</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">).</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># check if this request matches all the args in a registered call
</span>                <span class="n">expected_args</span> <span class="o">=</span> <span class="n">urlparse</span><span class="p">.</span><span class="n">parse_qs</span><span class="p">(</span><span class="n">args_qs</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">item</span> <span class="ow">in</span> <span class="n">actual_args</span><span class="p">.</span><span class="n">items</span><span class="p">()</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">expected_args</span><span class="p">.</span><span class="n">items</span><span class="p">()):</span>
                    <span class="k">continue</span>
                <span class="n">resp</span> <span class="o">=</span> <span class="n">urllib2</span><span class="p">.</span><span class="n">addinfourl</span><span class="p">(</span><span class="n">StringIO</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="p">{},</span> <span class="n">req</span><span class="p">.</span><span class="n">get_full_url</span><span class="p">())</span>
                <span class="n">resp</span><span class="p">.</span><span class="n">code</span> <span class="o">=</span> <span class="n">status_code</span>
                <span class="n">resp</span><span class="p">.</span><span class="n">msg</span> <span class="o">=</span> <span class="n">httplib</span><span class="p">.</span><span class="n">responses</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">status_code</span><span class="p">,</span> <span class="s">'OK'</span><span class="p">)</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">calls_made</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">url</span> <span class="o">+</span> <span class="s">'?'</span> <span class="o">+</span> <span class="n">args_qs</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">resp</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">(</span><span class="s">'need to mock url %s'</span> <span class="o">%</span> <span class="n">req</span><span class="p">.</span><span class="n">get_full_url</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">assert_all_called</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test</span><span class="p">):</span>
        <span class="n">calls_expected</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">responses</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">args_qs</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">responses</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
                <span class="n">calls_expected</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">url</span> <span class="o">+</span> <span class="s">'?'</span> <span class="o">+</span> <span class="n">args_qs</span><span class="p">)</span>
        <span class="n">test</span><span class="p">.</span><span class="n">assertEquals</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">calls_expected</span><span class="p">),</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">calls_made</span><span class="p">))</span>


    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">patch</span><span class="p">():</span>
        <span class="n">opener</span> <span class="o">=</span> <span class="n">urllib2</span><span class="p">.</span><span class="n">build_opener</span><span class="p">(</span><span class="n">MockHTTPHandler</span><span class="p">)</span>
        <span class="n">urllib2</span><span class="p">.</span><span class="n">install_opener</span><span class="p">(</span><span class="n">opener</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">h</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">opener</span><span class="p">.</span><span class="n">handlers</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">MockHTTPHandler</span><span class="p">)][</span><span class="mi">0</span><span class="p">]</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">unpatch</span><span class="p">():</span>
        <span class="n">urllib2</span><span class="p">.</span><span class="n">_opener</span> <span class="o">=</span> <span class="bp">None</span></code></pre></figure>

You can enable this mock in your unit tests as follows. Note the call to `unpatch()` to remove the mock. Without this, other tests in your test suite may fail if they try to make a HTTP call.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">unittest</span>


<span class="k">class</span> <span class="nc">TwitterOAuthCallsTest</span><span class="p">(</span><span class="n">unittest</span><span class="p">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">setUp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">requests</span> <span class="o">=</span> <span class="n">MockHTTPHandler</span><span class="p">.</span><span class="n">patch</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">tearDown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">MockHTTPHandler</span><span class="p">.</span><span class="n">unpatch</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">test_http_request</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="c1"># you can specify json results directly
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">requests</span><span class="p">.</span><span class="n">mock</span><span class="p">(</span><span class="s">'https://api.twitter.com/1/friends/ids.json'</span><span class="p">,</span> <span class="p">{</span>
            <span class="s">'screen_name'</span><span class="p">:</span> <span class="s">'foobar'</span><span class="p">,</span>
            <span class="s">'oauth_token'</span><span class="p">:</span> <span class="s">'BAR'</span><span class="p">,</span>
            <span class="s">'oauth_consumer_key'</span><span class="p">:</span> <span class="s">'dsafsdfdsfsdf'</span><span class="p">},</span>
            <span class="n">json_data</span><span class="o">=</span><span class="p">{</span>
                <span class="s">"ids"</span><span class="p">:</span> <span class="p">[</span>
                    <span class="mi">38596298</span><span class="p">,</span>
                    <span class="mi">30516966</span><span class="p">,</span>
                    <span class="mi">14399709</span><span class="p">,</span>
                <span class="p">],</span>
                <span class="s">"next_cursor"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s">"next_cursor_str"</span><span class="p">:</span> <span class="s">"0"</span><span class="p">,</span>
                <span class="s">"previous_cursor"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s">"previous_cursor_str"</span><span class="p">:</span> <span class="s">"0"</span>
            <span class="p">})</span>

        <span class="c1"># you can also specify json via an external file
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">requests</span><span class="p">.</span><span class="n">mock</span><span class="p">(</span><span class="s">'https://api.twitter.com/1/users/lookup.json'</span><span class="p">,</span> <span class="p">{</span>
            <span class="s">'user_id'</span><span class="p">:</span> <span class="s">'38596298,30516966,14399709'</span><span class="p">},</span>
            <span class="n">fixture</span><span class="o">=</span><span class="s">'twitter/fixtures/lookup.json'</span><span class="p">)</span>

        <span class="c1"># INVOKE THE TWITTER CODE HERE
</span>
        <span class="c1"># test that all the urls you registered were called
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">requests</span><span class="p">.</span><span class="n">assert_all_called</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code></pre></figure>
---
title: HBase/Pig/Python Quickstart on OSX
tags: hbase pig hadoop python
---

Having spent a good chunk of the last two weeks getting a prototype analytics system running, I thought I would write up my findings. I was pleased to find that installing all the pieces was smooth via [Homebrew](http://mxcl.github.com/homebrew/), but getting them all to play together was less smooth.

# The Playing Field

* [Hadoop](http://hadoop.apache.org/) is an framework for distributed computing. It's also used interchangibly to reference an entire ecosystem of technologies.
- [HDFS](http://en.wikipedia.org/wiki/Apache_Hadoop#Hadoop_Distributed_File_System) is the underlying distributed file ssytem that makes Hadoop possible.
* [HBase](http://hbase.apache.org/) is non-relational data store built on top of Hadoop. It provides concepts like rows, columns and keys. The similarity to relational databases stops there.
* [Zookeeper](http://zookeeper.apache.org/) provides configuration management for Hadoop cluster machines
* [Pig](http://pig.apache.org/) is high level language for map/reduce queries
* [Hive](http://hive.apache.org/) is a SQL-like high level language for map/reduce queries
* [Thrift](http://wiki.apache.org/hadoop/Hbase/ThriftApi) is a REST API for HBase
* [HappyBase](https://github.com/wbolster/happybase) is a python client for Thrift

# Installing

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">brew <span class="nb">install </span>hadoop hbase pig hive</code></pre></figure>

# Getting Started with HBase

First, you start the server with `start-hbase.sh`, then you can enter an interactive shell with `hbase shell` and create some test tables. HBase schema is a whole separate discussion. For now, we're going to create a table with a column family of "stats". Our primary keys are going to be in the format `md5(customer id)[:5] + customer id + date`.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">create <span class="s1">'test_table'</span>, <span class="s1">'stats'</span>
put <span class="s1">'test_table'</span>, <span class="s1">'c41e4:1:20130201'</span>, <span class="s1">'stats:count'</span>, 100
put <span class="s1">'test_table'</span>, <span class="s1">'c41e4:1:20130202'</span>, <span class="s1">'stats:count'</span>, 102
put <span class="s1">'test_table'</span>, <span class="s1">'ed516:2:20130201'</span>, <span class="s1">'stats:count'</span>, 80
scan <span class="s1">'test_table'</span>, <span class="o">{</span><span class="nv">LIMIT</span><span class="o">=&gt;</span>25<span class="o">}</span>
    ROW                                      COLUMN
     c41e4:1:20130201                        <span class="nv">column</span><span class="o">=</span>stats:count, <span class="nv">timestamp</span><span class="o">=</span>1359402726388, <span class="nv">value</span><span class="o">=</span>100
     c41e4:1:20130202                        <span class="nv">column</span><span class="o">=</span>stats:count, <span class="nv">timestamp</span><span class="o">=</span>1359402786126, <span class="nv">value</span><span class="o">=</span>102
     ed516:2:20130201                        <span class="nv">column</span><span class="o">=</span>stats:count, <span class="nv">timestamp</span><span class="o">=</span>1359402786180, <span class="nv">value</span><span class="o">=</span>80
    3 row<span class="o">(</span>s<span class="o">)</span> <span class="k">in </span>0.0150 seconds</code></pre></figure>

Take a look at the full list of [HBase shell commands](http://wiki.apache.org/hadoop/Hbase/Shell).


# Getting Started with Pig

Getting Pig to connect to HBase is a little tricky. It involves some monkeying around with `CLASSPATH` variables. You can run these `export` commands in bash to set everything up properly. Note, this is for a very specific combination of versions, but you can substibute newer versions easily.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span><span class="si">$(</span>/usr/libexec/java_home<span class="si">)</span>
<span class="nb">export </span><span class="nv">HBASE_HOME</span><span class="o">=</span>/usr/local/Cellar/hbase/0.94.2/
<span class="nb">export </span><span class="nv">PIG_CLASSPATH</span><span class="o">=</span><span class="s2">"/usr/local/Cellar/hbase/0.94.2/libexec/conf/hbase-site.xml:/usr/local/Cellar/hbase/0.94.2/libexec/hbase-0.94.2.jar:/usr/local/Cellar/hbase/0.94.2/libexec/lib/zookeeper-3.4.3.jar:/usr/local/Cellar/hbase/0.94.2/libexec/lib/guava-11.0.2.jar:/usr/local/Cellar/hbase/0.94.2/libexec/lib/protobuf-java-2.4.0a.jar"</span>
<span class="nb">export </span><span class="nv">HBASE_CONF_DIR</span><span class="o">=</span>/usr/local/Cellar/hbase/0.94.2/libexec/conf</code></pre></figure>

## Exporting Data

You can enter the pig shell (aka grunt), by simply running `pig`. You may find it useful if you run into problems to examine pig's classpath with `pip -secretDebugCmd`, and run pig in verbose mode with `pig -debug DEBUG -versbose`.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">data <span class="o">=</span> load <span class="s1">'hbase://test_table'</span> using org.apache.pig.backend.hadoop.hbase.HBaseStorage<span class="o">(</span><span class="s1">'stats:count'</span>, <span class="s1">'-loadKey true'</span><span class="o">)</span> AS <span class="o">(</span><span class="nb">id</span>:chararray, count:chararray<span class="o">)</span><span class="p">;</span>
store data into <span class="s1">'/tmp/test_table.csv'</span> using PigStorage<span class="o">(</span><span class="s1">','</span><span class="o">)</span><span class="p">;</span>
<span class="nb">cat</span> /tmp/test_table.csv
grunt&gt;
c41e4:1:20130201,100
c41e4:1:20130202,102
ed516:2:20130201,80</code></pre></figure>

Note: the last `cat` command is Pig's version of cat. Outside pig, the data is actually stored in a _directory_ called `/tmp/test_table.csv/`, in separate parts files. But they are just regular text files.

## Importing Data

For this example, let's create a larger data set. Here is a simple python script to create a CSV file in the correct format.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">random</span>
<span class="k">for</span> <span class="n">md5</span><span class="p">,</span> <span class="n">customer_id</span> <span class="ow">in</span> <span class="p">((</span><span class="s">'c41e4'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s">'ed516'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s">'a86f4'</span><span class="p">,</span> <span class="mi">3</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">month</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">day</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">):</span>
            <span class="k">print</span> <span class="s">'%s:%s:2013%02d%02d,%s'</span> <span class="o">%</span> <span class="p">(</span><span class="n">md5</span><span class="p">,</span> <span class="n">customer_id</span><span class="p">,</span> <span class="n">month</span><span class="p">,</span> <span class="n">day</span><span class="p">,</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">150</span><span class="p">))</span></code></pre></figure>

You can grab a pre-rendered version and save it locally with `curl -L http://chase-seibert.github.com/blog/files/import.csv > /tmp/import.csv`. Then, you can import it in `pig` like so. One confusing note here is that you don't include the ID field in the store command; that's automatic.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">data</span> <span class="o">=</span> <span class="n">load</span> <span class="s">'/tmp/import.csv'</span> <span class="n">using</span> <span class="n">PigStorage</span><span class="p">(</span><span class="s">','</span><span class="p">)</span> <span class="n">AS</span> <span class="p">(</span><span class="nb">id</span><span class="p">:</span><span class="n">chararray</span><span class="p">,</span><span class="n">count</span><span class="p">:</span><span class="n">chararray</span><span class="p">);</span>
<span class="n">store</span> <span class="n">data</span> <span class="n">into</span> <span class="s">'hbase://test_table'</span> <span class="n">using</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">pig</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="n">hadoop</span><span class="p">.</span><span class="n">hbase</span><span class="p">.</span><span class="n">HBaseStorage</span><span class="p">(</span><span class="s">'stats:count'</span><span class="p">);</span></code></pre></figure>

If you switch back to hbase shell, you should be able to scan and see those records.

## Aggregating with Map/Reduce

There is a [lot](http://pig.apache.org/docs/r0.7.0/piglatin_ref1.html#Using+Comments+in+Scripts) you can do with the built-in [pig latin language](http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html#Overview
). Here is one example, where we are going to get an average count by day for all customers. Because my day is only represented as an encoded portion of my row key, I will break that up as part of the aggregation.


<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">A</span> <span class="o">=</span> <span class="n">load</span> <span class="s">'hbase://test_table'</span> <span class="n">using</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">pig</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="n">hadoop</span><span class="p">.</span><span class="n">hbase</span><span class="p">.</span><span class="n">HBaseStorage</span><span class="p">(</span><span class="s">'stats:count'</span><span class="p">,</span> <span class="s">'-loadKey true'</span><span class="p">)</span> <span class="n">AS</span> <span class="p">(</span><span class="nb">id</span><span class="p">:</span><span class="n">chararray</span><span class="p">,</span> <span class="n">count</span><span class="p">:</span><span class="nb">int</span><span class="p">);</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">foreach</span> <span class="n">A</span> <span class="n">generate</span> <span class="nb">id</span><span class="p">,</span> <span class="n">FLATTEN</span><span class="p">(</span><span class="n">STRSPLIT</span><span class="p">(</span><span class="nb">id</span><span class="p">,</span> <span class="s">':'</span><span class="p">))</span> <span class="k">as</span> <span class="p">(</span><span class="n">md5</span><span class="p">:</span><span class="n">chararray</span><span class="p">,</span> <span class="n">customer_id</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">date</span><span class="p">:</span><span class="n">chararray</span><span class="p">),</span> <span class="n">count</span><span class="p">;</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">group</span> <span class="n">B</span> <span class="n">by</span> <span class="n">date</span><span class="p">;</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">foreach</span> <span class="n">C</span> <span class="n">generate</span> <span class="n">group</span><span class="p">,</span> <span class="n">AVG</span><span class="p">(</span><span class="n">B</span><span class="p">.</span><span class="n">count</span><span class="p">);</span>
<span class="n">dump</span> <span class="n">D</span><span class="p">;</span>
<span class="p">...</span>
<span class="mi">20</span> <span class="n">seconds</span> <span class="n">later</span>
<span class="p">...</span>
<span class="p">(</span><span class="mi">20130101</span><span class="p">,</span><span class="mf">63.333333333333336</span><span class="p">)</span>
<span class="p">(</span><span class="mi">20130102</span><span class="p">,</span><span class="mf">97.33333333333333</span><span class="p">)</span>
<span class="p">(</span><span class="mi">20130103</span><span class="p">,</span><span class="mf">96.66666666666667</span><span class="p">)</span>
<span class="p">(</span><span class="mi">20130104</span><span class="p">,</span><span class="mf">94.0</span><span class="p">)</span>
<span class="p">(</span><span class="mi">20130105</span><span class="p">,</span><span class="mf">74.0</span><span class="p">)</span>
<span class="p">(</span><span class="mi">20130106</span><span class="p">,</span><span class="mf">135.33333333333334</span><span class="p">)</span>
<span class="p">...</span></code></pre></figure>

You could save this dataset back to HBase using `store D into 'hbase://test_table2' using org.apache.pig.backend.hadoop.hbase.HBaseStorage('stats:date stats:count');`. Remember that you need to create the table first.

# Trouble-shooting

If you have not setup your `CLASSPATH` propery (ie, the `export` statements), you may get any one of the following errors:

* `ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2998: Unhandled internal error. org/apache/hadoop/hbase/filter/WritableByteArrayComparable`
---
title: Flatten entire HBase column families with Pig and Python UDFs
tags: hbase pig
---

Most [Pig](http://pig.apache.org/) tutorials you will find assume that you are working with data where you know all the column names ahead of time, and that the column names themselves are just labels, versus being composites of labels and data. For example, when working with [HBase](http://hbase.apache.org/), it's actually not uncommon for both of those assumptions to be false. Being a columnar database, it's very common to be working to rows that have [thousands of columns](https://issues.apache.org/jira/browse/HBASE-867). Under that circumstance, it's also common for the column names themselves to encode to dimensions, such as [date and counter type](https://www.facebook.com/video/video.php?v=707216889765).

How do you solve this mismatch? If you're in the early stages of designing a schema, you could reconsider a more row based approach. If you have to work with an existing schema, however, you can with the help of [Pig UDFs](http://ofps.oreilly.com/titles/9781449302641/udf_lists.html).

Say we have the following table:

<table>
    <tr>
        <th>rowkey</th>
        <th>cf1:20130101post</th>
        <th>cf1:20130101comment</th>
        <th>cf1:20130101like</th>
        <th>cf1:20130102post</th>
        <th>...</th>
    </tr>
    <tr>
        <td>ce410-00005031-00089182</td>
        <td>147</td>
        <td>5</td>
        <td>41</td>
        <td>153</td>
    </tr>
    <tr>
        <td>ce410-00005031-00021915</td>
        <td>1894</td>
        <td>33</td>
        <td>86</td>
        <td>1945</td>
    </tr>
    <tr>
        <td>5faa4-00009521-00019828</td>
        <td>30</td>
        <td>2</td>
        <td>8</td>
        <td>31</td>
    </tr>
    <tr>
        <td>...</td>
    </tr>
</table>

Here there is a composite row key, but also composite column keys. Because the date is part of the column keys, there are potentially many, many columns. Enumerating them all in your Pig scripts in impractical. Notice that they are also in the same column family. To load them all, you can do the following in Pig:

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">data</span> <span class="o">=</span> <span class="k">load</span> <span class="s1">'hbase://table_name'</span> <span class="k">using</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">pig</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="n">hadoop</span><span class="p">.</span><span class="n">hbase</span><span class="p">.</span><span class="n">HBaseStorage</span><span class="p">(</span><span class="s1">'cf1:*'</span><span class="p">,</span> <span class="s1">'-loadKey true'</span><span class="p">)</span> <span class="k">AS</span> <span class="p">(</span><span class="n">id</span><span class="p">:</span><span class="n">chararray</span><span class="p">,</span> <span class="n">stats</span><span class="p">:</span><span class="k">map</span><span class="p">[</span><span class="nb">int</span><span class="p">]);</span>
<span class="n">illustrate</span> <span class="k">data</span><span class="p">;</span></code></pre></figure>

This will result in all columns being loaded into a [Pig map](http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html#Data+Types), which is just a collection of tuples:

<pre>
-----------------------------------------------------------------------------------------------------
| data         | id:chararray            | stats:map(:int)                                          |
-----------------------------------------------------------------------------------------------------
|              | ce410-00005031-00089182 | {20130101post=147,20130101comment=5,20130101like=41,...} |
-----------------------------------------------------------------------------------------------------
</pre>

So, now you have loaded all the data, but how to you parse the column names into their respective parts, so you can apply logic to the values? Here is a very simply Python implementation of a UDF that will turn that map into a bag:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">@</span><span class="n">outputSchema</span><span class="p">(</span><span class="s">"values:bag{t:tuple(key, value)}"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">bag_of_tuples</span><span class="p">(</span><span class="n">map_dict</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">map_dict</span><span class="p">.</span><span class="n">items</span><span class="p">()</span></code></pre></figure>

You can include this UDF (place the above in a file called udfs.py in the current working directory for pig), and invoke it like this:

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="n">register</span> <span class="s1">'udfs.py'</span> <span class="k">using</span> <span class="n">jython</span> <span class="k">as</span> <span class="n">py</span>
<span class="k">data</span> <span class="o">=</span> <span class="k">load</span> <span class="s1">'hbase://table_name'</span> <span class="k">using</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">pig</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="n">hadoop</span><span class="p">.</span><span class="n">hbase</span><span class="p">.</span><span class="n">HBaseStorage</span><span class="p">(</span><span class="s1">'cf1:*'</span><span class="p">,</span> <span class="s1">'-loadKey true'</span><span class="p">)</span> <span class="k">AS</span> <span class="p">(</span><span class="n">id</span><span class="p">:</span><span class="n">chararray</span><span class="p">,</span> <span class="n">stats</span><span class="p">:</span><span class="k">map</span><span class="p">[</span><span class="nb">int</span><span class="p">]);</span>
<span class="n">databag</span> <span class="o">=</span> <span class="n">foreach</span> <span class="k">data</span> <span class="n">generate</span> <span class="n">id</span><span class="p">,</span> <span class="n">FLATTEN</span><span class="p">(</span><span class="n">py</span><span class="p">.</span><span class="n">bag_of_tuples</span><span class="p">(</span><span class="n">stats</span><span class="p">));</span>
<span class="n">illustrate</span> <span class="n">databag</span><span class="p">;</span></code></pre></figure>

This is taking advantage of the built-in [FLATTEN](http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html#Flatten+Operator) operator, which takes a bag and does a cross product with bag's row to produce N new rows.

<pre>
------------------------------------------------------------------------------
| databag      | id:chararray            | key:bytearray  | value:bytearray  |
------------------------------------------------------------------------------
|              | ce410-00005031-00089182 | 20130101post    | 147             |
------------------------------------------------------------------------------
|              | ce410-00005031-00089182 | 20130101comment | 5               |
------------------------------------------------------------------------------
|              | ce410-00005031-00089182 | 20130101like    | 41              |
------------------------------------------------------------------------------
</pre>

You can then process your data as normal. You can then write your data bag to HBase in the same format by using the built-in UDFs [TOMAP](http://pig.apache.org/docs/r0.9.1/api/org/apache/pig/builtin/TOMAP.html) and and same * syntax. Assuming you have produced new column names and values in your script, you can do:

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="p">...</span>
<span class="n">mapped</span> <span class="o">=</span> <span class="n">foreach</span> <span class="n">processed_data</span> <span class="n">generate</span> <span class="n">TOMAP</span><span class="p">(</span><span class="n">columnname</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="k">as</span> <span class="n">stats</span><span class="p">;</span>
<span class="n">store</span> <span class="n">mapped</span> <span class="k">into</span> <span class="s1">'hbase://table_name'</span> <span class="k">using</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">pig</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="n">hadoop</span><span class="p">.</span><span class="n">hbase</span><span class="p">.</span><span class="n">HBaseStorage</span><span class="p">(</span><span class="s1">'stats:*'</span><span class="p">);</span></code></pre></figure>
---
title: Why Database Integrity Matters
tags: database
---

Given the rise in popularity of [NoSQL](http://en.wikipedia.org/wiki/NoSQL) solutions, typically set apart by their explicit forfeiture of traditional [RDBMS](http://en.wikipedia.org/wiki/Relational_database_management_system) features, you might think that age old concepts like ACID transactions and foreign key constraints are simply antiquated. In the face of the new shiny hotness, why did we ever value [data integrity](ftp://public.dhe.ibm.com/software/solutions/soa/pdfs/Whydataintegritymatters-trifold.pdf) so highly, anyway?

Personally, I think it's critical to realize that NoSQL solutions were born primarily out of necessity. At some extremely high volumes of data, you simply don't have the luxury of data integrity. But that's a problem for the Facebooks and Googles of the world. The other 99% of us are flattering ourselves if we think we have the same data scaling problems. Of course, given NoSQL tools, more data will be accumulated by smaller companies, simply because it is now practical to work with.

But for traditional RDBMS systems, it's important to re-emphasize the value of data consistency. In short:

> Anything that can happen in a schema, will happen.

# Soft Deletion

Soft deleting a record refers to marking it as deleting, typically with a 0/1 value in a column like `is_deleted`. Some NoSQL solutions like HBase can do this natively, by keeping multiple versions of a data cell around. But it's actually a long-standing pattern from RDBMS. Typical reasons include enabling undo features, providing audit trails and simple fear of deleting data permanently.

Soft deletes are totally reasonable, but there are some [trade offs](http://weblogs.asp.net/fbouma/archive/2009/02/19/soft-deletes-are-bad-m-kay.aspx) to be aware of.

- Natural primary keys become more difficult. Do you allow two user records with the same username if one is deleted?
- Prevents you from using the cascading delete feature of you database, leading to orphaned records.
- Invites logically [invalid foreign key references](http://ayende.com/blog/4157/avoid-soft-deletes)- ie references from a soft-deleted record to a live record.
- Easiest to enforce through an ORM, but you won't always be accessing the data through the ORM.

Most importantly, they can slow down performance. If you have a significant amount of data, you will end up adding more indexes that you would otherwise need, simply to be sure you are always looking at non-deleted records. That will [slow down writes](http://richarddingwall.name/2009/11/20/the-trouble-with-soft-delete/) to some extent. You could regularly delete soft-deleted records, but in practice I have not seen people actually doing that regularly.

It's easy to get into a situation where you have multiple soft-deleted versions of every record. You may find that you have more deleted data than live data! One alternative to soft-deletes is to put these records into a separate archive. It could even be another RDBMS. If you really ever do need the data, it will be there. But don't be surprised if you never use it.

# Foreign Key Constraints

There seems to be a misunderstanding that "Real Internet Applications Don't Use Foreign Keys" ([source]( http://www.thisblog.runsfreesoftware.com/?q=2009/03/20/real-internet-applications-dont-use-foreign-keys)). Let me assure you, foreign key constraints are an absolutely fundamental building block of most of the web apps that you use. It's what the "relational" in RDBMS stands for!

Foreign keys are the primary method for enforcing data integrity. Without them, you are inviting [orphaned rows and giving up cascading deletes](http://stackoverflow.com/questions/83147/whats-wrong-with-foreign-keys). These are similar trade-offs to use soft deletes. In fact, soft deletes can [exacerbate the issue](http://stackoverflow.com/questions/3492485/mysql-with-soft-deletion-unique-key-and-foreign-key-constraints).

In the end, not using foreign keys is similar to not [validating your input](http://www.oreillynet.com/onlamp/blog/2006/05/misunderstanding_foreign_keys.html
). You're assuming that your infallible application layer will never insert bad data into the system. First of all, that's just wishful thinking. In practice, I have observed that if the schema allows bad data, you will eventually see that exact form of bad data in the system. Better to put as much data validation as you can right next to the data itself, where it is much harder to circumvent.

# Transactions

[ACID](http://en.wikipedia.org/wiki/ACID) compliance is a big deal. Databases competed on this playing field for decades. Yes, transactions absolutely do slow down performance. Would you rather have wrong answers quickly, or correct answers a few milliseconds later?

Yes, transactions [can throw errors](http://jasonswett.net/blog/why-you-should-use-database-transactions/
) when you are inserting data. You can't insert bad data; that's the point! When you say you want to disable transactions because "I don't ever want to loose data", I hear "I would rather have bad data in the system than no data". (Application layer isn't looking so infallible now, is it?) In general, transactions reduce the scope of data problems from "I just inserted a bunch of invalid data, let's fix the 'bugs' in the application layer to deal with it" to "one user got an error trying to insert bad data". IMHO, that's a good thing.

# JSON Data

Like all of these anti-patterns, putting JSON data into your relational database is fine in small doses. I actually think it's advisable when dealing will small bits of schema-less data. But it should be the exception, not the rule. It's all too easy to avoid the small up-front cost of parsing the data and defining a schema, only to find that you're fighting bugs in your application when you're trying to do just that on the fly per-request.

- You're relying on your application layer to validate the format; the database can't even check that it IS JSON. (Unless you use [Postgres](http://www.postgresql.org/docs/devel/static/datatype-json.html))
- You can't index inside this field.
- You are probably storing these as TEXT to account for their in determinant length; MySQL in particular can't do [standard optimizations](http://dev.mysql.com/doc/refman/5.1/en/internal-temporary-tables.html
) on tables with TEXT blobs.

Note: the later is also [not a problem](http://stackoverflow.com/questions/348416/in-postgresql-is-it-faster-to-include-text-columns-in-the-same-table-rather-th
) on Postgres.

# Conclusion

In the final analysis, poor data integrity is really all about _SPEED_. Not the speed of your application, but the speed of your engineers. Soft deletion, disabling foreign keys, eschewing transactions and not normalizing data properly all have their valid use cases. But over-used, you're slowing down every engineer that has to write new code on top of the database by inundating them with edge cases they frankly shouldn't have to worry about.

They have a reference to an active user, but the required profile record is soft-deleted. Add more code to deal with that. I tried to show the user a link from an invoice to their internal CRM with a reference_id, but that key was missing from the json column. Add more code to deal with that. Not to mention volume of bugs that can be traced back to data integrity issues.
---
title: Facebook Page Post Insights Empty Results
tags: facebook
---

Spent a couple of hours tearing my hair out over the Facebook insights API this week. Say you have a Facebook page with an ID of **173304932707127**, and a post on that page with ID **497058920331725**. The [Facebook Insights API docs](https://developers.facebook.com/docs/reference/fql/insights/) would lead you to believe that you can query for post level details with just `/497058920331725/insights` as the URL. Instead, any query at that base URL just returns an empty data property:

![Facebook Insights Post Empty Data](/blog/images/insights_empty_data.png)

The fact that just `/497058920331725` correctly returns the meta data about the page would further strengthen your belief that there just is no insights data for this post.

![Facebook Insights Post Metadata](/blog/images/insights_post.png)

However, though it is mentioned nowhere in the docs, you in fact need to hit `/173304932707127_497058920331725/insights` to retreive insights data for that post.

![Facebook Insights Post Success](/blog/images/insights_with_data.png)
---
title: Introducing Django Migratron
tags: python django
---

[Django Migratron](https://github.com/chase-seibert/django-migratron) is a schema migration tool for Django. Why write another schema migration tool? Here is how we stack up against other tools.

### Similarities

- Migrations are defined as scripts in a configurable directory
- There are commands to list, run and fake one or more migrations
- Migrations side effects can include schema changes, data updates or anything you can do from python script
- Migration history is stored in the same database that the migrations are run against

### Differences

- Migrations can be either python or sql files
- Migrations can be placed in a sub directory by type, and you can run/list a single type at a time
- Migrations are not ordered
- Schema migrations cannot be automatically generated
- Schema updates are only as database-independent as you make them
- Migration history includes who ran the migration, when it was run, who authored the script, and any notes either person has entered.

# Install

Just run `pip install pytz PyYAML django-yamlfield termcolor django-migratron`.

### Add to INSTALLED_APPS

In your `settings.py`, add `migratron` to the INSTALLED_APPS setting:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">INSTALLED_APPS</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">...</span>
    <span class="s">'migratron'</span><span class="p">,</span>
    <span class="p">)</span></code></pre></figure>

### Create database tables

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">./manage.py syncdb</code></pre></figure>

### Settings

You can configure the following settings in your `settings.py`. All of these are optional, if you don't specify them, they will use defaults.

- `MIGRATIONS_DIR` - The directory where migrations will be stored either at the top level, or in top-level directories corresponding to migration types.

Different migration types might be things like "pre", "post", "delayed", etc.

You can use an absolute path, or build one dynamically like so:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="n">MIGRATIONS_DIR</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">__file__</span><span class="p">),</span> <span class="s">'migrations'</span><span class="p">)</span></code></pre></figure>

- `MIGRATIONS_ALLOWED_TYPES` - A tuple of allowed migration types. If not specified, any type is allowed, including no type.
To explicitly allow no type (ie, migrations in the root directory), add None to the tuple.

Example:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">MIGRATIONS_ALLOWED_TYPES</span> <span class="o">=</span> <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="s">'pre'</span><span class="p">,</span> <span class="s">'post'</span><span class="p">)</span></code></pre></figure>

- `MIGRATIONS_SCRIPT_AUTHOR_LINK_FUNCTION` - The function to run on a file path to get the script commit link (ie, github, bitbucket, etc).

Example:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">migratron.authors</span> <span class="kn">import</span> <span class="n">get_git_script_author_link</span>
<span class="n">MIGRATIONS_SCRIPT_AUTHOR_LINK_FUNCTION</span> <span class="o">=</span> <span class="n">get_git_script_author_link</span></code></pre></figure>

- `MIGRATIONS_TIMEZONE` - The timezone used to calculate the display value of the script run datetimes in --list.

Example:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">MIGRATIONS_TIMEZONE</span> <span class="o">=</span> <span class="s">'US/Pacific'</span></code></pre></figure>

- `MIGRATIONS_DBSHELL_CMD` - The command to run to exclude `dbshell`. Typically `manage.py dbshell`, but you may use an
alternate shell, or an alternate python intepreter.

# Usage

### Creating Migrations

Migrations are created based on python/sql templates. The body of the script must be implemented manually. The templates have standard meta-data slots in them, in the form of a leading comment block.

- `./manage.py createmigration "Add date_added and user columns to the Foobar model` - Create a new python migration
- `./manage.py createmigration --type pre "Add date_added and user columns to the Foobar model` - Create a new python migration of type "pre"
- `./manage.py createmigration --template sql "Add date_added and user columns to the Foobar model` - Create a new sql migration

### Listing and Running Migrations

- `./manage.py migrate --list` - List the migrations under the root `MIGRATIONS_DIR`.


<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Migrations</span><span class="p">:</span>
<span class="mi">2012</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">24</span> <span class="mi">18</span><span class="p">:</span><span class="mi">06</span> <span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="n">test</span><span class="p">.</span><span class="n">sql</span>
<span class="mi">2012</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">25</span> <span class="mi">11</span><span class="p">:</span><span class="mi">31</span> <span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="n">dedupe_sso</span><span class="p">.</span><span class="n">py</span>
<span class="mi">2012</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">25</span> <span class="mi">12</span><span class="p">:</span><span class="mi">47</span> <span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="n">run_jsunit</span><span class="p">.</span><span class="n">py</span>
<span class="mi">2012</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">25</span> <span class="mi">13</span><span class="p">:</span><span class="mi">43</span> <span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="n">suggestions_add_org_backfill_cleanup</span><span class="p">.</span><span class="n">sql</span>
<span class="mi">2012</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">25</span> <span class="mi">18</span><span class="p">:</span><span class="mi">41</span> <span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="n">mail_message_allow_null</span><span class="p">.</span><span class="n">sql</span>
<span class="mi">2012</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">25</span> <span class="mi">19</span><span class="p">:</span><span class="mi">02</span> <span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="n">test</span><span class="p">.</span><span class="n">py</span>
                 <span class="p">(</span> <span class="p">)</span> <span class="n">activities_slave_index</span><span class="p">.</span><span class="n">sql</span>
                 <span class="p">(</span> <span class="p">)</span> <span class="n">add_group_is_corporate_column</span><span class="p">.</span><span class="n">sql</span>
                 <span class="p">(</span> <span class="p">)</span> <span class="n">add_org_sharing_preferences_column</span><span class="p">.</span><span class="n">sql</span>
                 <span class="p">(</span> <span class="p">)</span> <span class="n">add_org_to_subscription</span><span class="p">.</span><span class="n">sql</span></code></pre></figure>

- `./manage.py migrate --type pre --list` - List the migrations under the `MIGRATIONS_DIR/pre`.
- `./manage.py migrate foobar.py` - Run the migration `MIGRATIONS_DIR/foobar.py`.
- `./manage.py migrate --type pre foobar.py` - Run the migration `MIGRATIONS_DIR/pre/foobar.py`.
- `./manage.py migrate --all` - Run ALL migrations in `MIGRATIONS_DIR`.
- `./manage.py migrate foobar.py --log-only` - Don't really run the migration, but add it to the migration history as successfully run
- `./manage.py migrate foobar.py --delete-log` - Delete the migration history for this file
- `./manage.py migrate foobar.py --pending` - Exit with status code 1 if there are pending migrations
- `./manage.py migrate foobar.py --info` - Print all meta-data, migration history and notes for a migration.
- `./manage.py migrate foobar.py --notes` - Create or edit the migration runner's note for the latest migration using $EDITOR.
- `./manage.py migrate --list --verbose` - List migrations with extra meta-data, like runner's notes.
- `./manage.py migrate test.py --flag "Need to run this again after the next deploy"` - Flag a migration as needing further attention, with an optional note.
- `./manage.py migrate test.py --flag` - Toggle the flag on an existing migration.
- `./manage.py migrate test.py --clear` - Delete all migration history from the database.


### Migration History

If you need to play back sql migrations run on one database against another one, you may find it useful to list migrations in the order they were actually run, optionally with runner comments. For history commands, types do not matter; all run migrations are output.

- `./manage.py migrate --history` - List just the file names in the order they were run.
- `./manage.py migrate --history --verbose` - List file names and runner comments.

## Confirmation Inside Migrations

If you want to require manual confirmation for a particular migration, just make sure you exit
with an error code, or raise an exception, if the script does not run. That way, the script will
not be marked as having run successfully.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">char</span> <span class="o">=</span> <span class="nb">raw_input</span><span class="p">(</span><span class="s">'Nuke all the things? (Y/N): '</span><span class="p">)</span>
<span class="k">if</span> <span class="n">char</span><span class="p">.</span><span class="n">upper</span><span class="p">()</span> <span class="o">==</span> <span class="s">'Y'</span><span class="p">:</span>
    <span class="k">print</span> <span class="s">"Nuking!"</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span></code></pre></figure>

---
title: Mocking HappyBase for unit testing HBase code
tags: python hbase
---

[HappyBase](https://github.com/wbolster/happybase) is a friendly interface to interact with [HBase](http://hbase.apache.org/) from Python. It lets you perform basic HBase operations like `get`, `put` and `scan`. But say you have a bunch of puts littered around your code. How do you unit test that? One method would be to mock out the happy base calls themselves, and just assert that they are called with specific parameters. But what if you want to test the final state of the HBase tables after a series of operations?

For that, you can replace the HappyBase `Table` class with a version that keeps the data in memory.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">unittest</span>
<span class="kn">import</span> <span class="nn">happybase</span>


<span class="k">class</span> <span class="nc">MockTable</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_name</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">table_name</span> <span class="o">=</span> <span class="n">table_name</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">put</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">row</span><span class="p">].</span><span class="n">update</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">row</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">scan</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
        <span class="s">''' does not respect any options like start/stop row '''</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">items</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">MockConnection</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">''' singleton object, so that multiple HBaseTables can collaborate '''</span>

    <span class="n">_instance</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">tables</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__new__</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">cls</span><span class="p">.</span><span class="n">_instance</span><span class="p">:</span>
            <span class="n">cls</span><span class="p">.</span><span class="n">_instance</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">MockConnection</span><span class="p">,</span> <span class="n">cls</span><span class="p">).</span><span class="n">__new__</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">.</span><span class="n">_instance</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">is_table_enabled</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">True</span>

    <span class="k">def</span> <span class="nf">table</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="n">table</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tables</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">table</span><span class="p">:</span>
            <span class="n">table</span> <span class="o">=</span> <span class="n">MockTable</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">tables</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">table</span>
        <span class="k">return</span> <span class="n">table</span>

    <span class="k">def</span> <span class="nf">flush</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tables</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">HBaseTestCase</span><span class="p">(</span><span class="n">unittest</span><span class="p">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="s">''' mock out calls to hbase
    if you over-ride setUp(), make sure to call super '''</span>

    <span class="k">def</span> <span class="nf">setUp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">happybase</span><span class="p">.</span><span class="n">_Connection</span> <span class="o">=</span> <span class="n">happybase</span><span class="p">.</span><span class="n">Connection</span>
        <span class="n">happybase</span><span class="p">.</span><span class="n">Connection</span> <span class="o">=</span> <span class="n">MockConnection</span>
        <span class="n">MockConnection</span><span class="p">().</span><span class="n">flush</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">tearDown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">happybase</span><span class="p">.</span><span class="n">Connection</span> <span class="o">=</span> <span class="n">happybase</span><span class="p">.</span><span class="n">_Connection</span></code></pre></figure>

To use it, just have your unit test class extend `HBaseTestCase`.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">happybase</span>


<span class="k">class</span> <span class="nc">HappybaseMockTests</span><span class="p">(</span><span class="n">HBaseTestCase</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">setUp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">connection</span> <span class="o">=</span> <span class="n">happybase</span><span class="p">.</span><span class="n">Connection</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">table</span> <span class="o">=</span> <span class="n">connection</span><span class="p">.</span><span class="n">table</span><span class="p">(</span><span class="s">'table-name'</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HappybaseMockTests</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">setUp</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">test_put</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">table</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">'row1'</span><span class="p">,</span> <span class="p">{</span><span class="s">'cf1:col1'</span><span class="p">:</span> <span class="s">'1'</span><span class="p">,</span> <span class="s">'cf1:col2'</span><span class="p">:</span> <span class="s">'2'</span><span class="p">,</span> <span class="s">'cf2:col1'</span><span class="p">:</span> <span class="s">'3'</span><span class="p">})</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">assertEquals</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">table</span><span class="p">.</span><span class="n">row</span><span class="p">(</span><span class="s">'row1'</span><span class="p">),</span> <span class="p">{</span><span class="s">'cf1:col1'</span><span class="p">:</span> <span class="s">'1'</span><span class="p">,</span> <span class="s">'cf1:col2'</span><span class="p">:</span> <span class="s">'2'</span><span class="p">,</span> <span class="s">'cf2:col1'</span><span class="p">:</span> <span class="s">'3'</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">test_scan</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">table</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">'row1'</span><span class="p">,</span> <span class="p">{</span><span class="s">'cf1:col1'</span><span class="p">:</span> <span class="s">'1'</span><span class="p">})</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">table</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">'row1'</span><span class="p">,</span> <span class="p">{</span><span class="s">'cf1:col2'</span><span class="p">:</span> <span class="s">'2'</span><span class="p">})</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">table</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">'row2'</span><span class="p">,</span> <span class="p">{</span><span class="s">'cf2:col1'</span><span class="p">:</span> <span class="s">'3'</span><span class="p">})</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">assertEquals</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">table</span><span class="p">.</span><span class="n">scan</span><span class="p">(),</span> <span class="p">[(</span><span class="s">'row1'</span><span class="p">,</span> <span class="p">{</span><span class="s">'cf1:col1'</span><span class="p">:</span> <span class="s">'1'</span><span class="p">,</span> <span class="s">'cf1:col2'</span><span class="p">:</span> <span class="s">'2'</span><span class="p">}),</span> <span class="p">(</span><span class="s">'row2'</span><span class="p">,</span> <span class="p">{</span><span class="s">'cf2:col1'</span><span class="p">:</span> <span class="s">'3'</span><span class="p">})]</span></code></pre></figure>

*Note:* this version does not force you to use valid column families, it just created families on the fly as you put columns in. It also does not support any of the options on `scan`, such as filtering by a range of rowkeys. Some of those options could be added easily. Others would be very difficult, such as java based column filters. Hopefully this is enough you get you started testing your HBase code.
---
title: Using HBase/Thrift through the Rackspace Load Balancer
tags: hbase
---

Using the binary [Thrift](http://thrift.apache.org/) protocol through a load balancer can be a little tricky. While it works out of the box in [HAProxy](http://blog.milford.io/2011/07/productionizing-the-hive-thrift-server/), you may run into some slight trouble running it through other load balancers. By default, when trying to access Thrift through the [Rackspace Cloud Load Balancers](http://www.rackspace.com/cloud/load-balancing/), you may get the following exception:

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:

File <span class="s2">"/usr/local/lib/python2.6/dist-packages/Django-1.2-py2.6.egg/django/core/handlers/base.py"</span>,
line 100, <span class="k">in </span>get_response
response <span class="o">=</span> callback<span class="o">(</span>request, <span class="k">*</span>callback_args, <span class="k">**</span>callback_kwargs<span class="o">)</span>
...
File <span class="s2">"/usr/local/lib/python2.6/dist-packages/Thrift-0.2.0-py2.6-linux-x86_64.egg/thrift/protocol/TBinaryProtocol.py"</span>,
line 126, <span class="k">in </span>readMessageBegin
sz <span class="o">=</span> self.readI32<span class="o">()</span>

File <span class="s2">"/usr/local/lib/python2.6/dist-packages/Thrift-0.2.0-py2.6-linux-x86_64.egg/thrift/protocol/TBinaryProtocol.py"</span>,
line 203, <span class="k">in </span>readI32
buff <span class="o">=</span> self.trans.readAll<span class="o">(</span>4<span class="o">)</span>

File <span class="s2">"/usr/local/lib/python2.6/dist-packages/Thrift-0.2.0-py2.6-linux-x86_64.egg/thrift/transport/TTransport.py"</span>,
line 58, <span class="k">in </span>readAll
chunk <span class="o">=</span> self.read<span class="o">(</span>sz-have<span class="o">)</span>

File <span class="s2">"/usr/local/lib/python2.6/dist-packages/Thrift-0.2.0-py2.6-linux-x86_64.egg/thrift/transport/TTransport.py"</span>,
line 155, <span class="k">in </span><span class="nb">read
</span>self.__rbuf <span class="o">=</span> StringIO<span class="o">(</span>self.__trans.read<span class="o">(</span>max<span class="o">(</span>sz, self.DEFAULT_BUFFER<span class="o">)))</span>

File <span class="s2">"/usr/local/lib/python2.6/dist-packages/Thrift-0.2.0-py2.6-linux-x86_64.egg/thrift/transport/TSocket.py"</span>,
line 94, <span class="k">in </span><span class="nb">read
</span>raise TTransportException<span class="o">(</span><span class="nb">type</span><span class="o">=</span>TTransportException.END_OF_FILE,
<span class="nv">message</span><span class="o">=</span><span class="s1">'TSocket read 0 bytes'</span><span class="o">)</span>

thrift.transport.TTransport.TTransportException: TSocket <span class="nb">read </span>0 bytes</code></pre></figure>

After playing around with the [HappyBase](https://github.com/wbolster/happybase) connection `transport` [settings](http://happybase.readthedocs.org/en/latest/api.html#connection), I was able to rule out [framed versus binary transport](https://github.com/wbolster/happybase/issues/6) as the issue. I was also able to verify that a connection directly to the machine worked. 

The issue turned out to be a setting on the Rackspace load balancer.

![Rackspace Load Balancer TCP Client First](/blog/images/rackspace_lb.png)

The default TCP protocol, called simply "TCP", did not work. By changing the setting to "TCP (Client First)", HappyBase was able to connect via Thrift.

What does this setting do? The [Rackspace documentation](http://docs.rackspace.com/loadbalancers/api/v1.0/clb-devguide/content/List_Load_Balancing_Protocols-d1e4269.html) simply says "This protocol is similiar to TCP, but is more efficient when a client is expected to write the data first". This would seem to hint that the regular TCP option attemps to read from the server socket when a client connects, where-as as an RPC protocol, Thrift [expects the client](http://en.wikipedia.org/wiki/Apache_Thrift) to send the first data packet.
---
title: Introducing Advocoders
tags: opensource blog
---

*Update: this app was decommissioned in 2022*

Hiring engineers is tough, especially right now with a general shortage of qualified candidates and record demand. Good companies realize they need involve their engineering team to help recruit. That could mean hosting drink ups, sending the team to conferences and old fashioned professional networking. But what about your engineering blog?

Many medium sized companies have some kind of blog for the engineering team. Larger companies often have a site run by corporate marketing disguised as a grassroots blog. Lots of companies have nothing! But I bet even those companies have engineers generating _some_ interesting content. Under their own personal [Stackoverflow](http://stackoverflow.com/) and [Github](http://github.com) accounts, they are likely generating at least a slow trickle of content. Some of them may even have their own blogs. But none of that stuff is directly tied back to the team, or aiding the recruiting effort.

That's why I'm introducing [Advocoders](http://advocoders.herokuapp.com/), a place where you can pull in the organic content your team is generating under a corporate brand. Right now it's only for Google Apps businesses, which includes most start ups.

Any engineer on your team can go sign up with their corporate Google Apps account.

![Advododers Signup 1](/blog/images/advocoders1.png)

They can fill out some basic information about themselves.

![Advododers Signup 2](/blog/images/advocoders2.png)

Then they link in their blog's RSS feed, their Stackoverflow account and their Github account.

![Advododers Signup 3](/blog/images/advocoders3.png)

They can also put in a blurb about your company.

![Advododers Signup 4](/blog/images/advocoders4.png)

Advocoders generates a combined news feed for all the engineers in the company, complete with syntax highlighting for code snippets. The engineers continue to get credit for their content, but right beside the content is a call out to contact the company for job opportunities.

![Advododers News Feed](/blog/images/advocoders.png)

Right now, it's hosted on [Heroku](https://www.heroku.com/). But it's just a Django app that you can [fork the code](https://github.com/chase-seibert/advocoders) and run yourself.
---
title: Dynamic Attributes in Python
tags: python tutorial
---

One of the strengths of a dynamic language is that it allows you to more easily work introspection and light weight meta-programming into your every day code. In Python, one of the primary ways of taking advantage of the dynamic nature of the language is through attribute access.

__Note__: this is part one in a series of posts about basic Python functionality.

In most cases, to get an attribute of an object, you just want to use `obj.field`. But if you don't know the name of the field until runtime, you can use `getattr(obj, 'field')`.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">print_field</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">field</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">print</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">field</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">AttributeError</span><span class="p">:</span>
        <span class="k">print</span> <span class="s">'No %s field'</span> <span class="o">%</span> <span class="n">field</span></code></pre></figure>

This is a fairly common pattern, so you can avoid the extra try/catch and use the third `default` parameter:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">print_field</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">field</span><span class="p">):</span>
    <span class="k">print</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">field</span><span class="p">,</span> <span class="s">'No %s field'</span> <span class="o">%</span> <span class="n">field</span><span class="p">)</span></code></pre></figure>

Both attribute access methods are [virtually identical](http://stackoverflow.com/questions/2909423/is-it-bad-practice-to-use-pythons-getattr-extensively#answer-2909734) in terms of performance. The regular method produces slightly cleaner code, so normally you would use that. Besides, when do you __NOT__ know the names of the fields you want to access ahead of time?

If you're dealing with data, you don't always know. For example, say you're mapping URLs to view methods. If the user hits `/user/123/settings`, you could route that to a view function as follows:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">ViewClass</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">route</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">.</span><span class="n">url</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s">'not_found_404'</span><span class="p">)(</span><span class="n">request</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">settings</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">HttpResponse</span><span class="p">(</span><span class="s">'Here is your settings page!'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">not_found_404</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">HttpResponse</span><span class="p">(</span><span class="s">'404 Page Not Found'</span><span class="p">,</span> <span class="n">code</span><span class="o">=</span><span class="mi">404</span><span class="p">)</span></code></pre></figure>

Of course, you could always do this with a pre-defined set of URLs, but the point is that you have a built-in way to avoid that code duplication. In general, this is known as keeping your code [DRY](http://en.wikipedia.org/wiki/Don't_repeat_yourself). For example, notice the duplication of tokens in code like the following:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">obj</span><span class="p">.</span><span class="n">first_name</span><span class="p">.</span><span class="n">first_child</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="s">'Chase'</span>
<span class="n">obj</span><span class="p">.</span><span class="n">last_name</span><span class="p">.</span><span class="n">first_child</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="s">'Seibert'</span>
<span class="n">obj</span><span class="p">.</span><span class="n">phone</span><span class="p">.</span><span class="n">first_child</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="s">'(555) 123-4567'</span></code></pre></figure>

Instead, you could do something like the following:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="ow">in</span> <span class="p">((</span><span class="s">'first_name'</span><span class="p">,</span> <span class="s">'Chase'</span><span class="p">),</span> <span class="p">(</span><span class="s">'last_name'</span><span class="p">,</span> <span class="s">'Seibert'</span><span class="p">),</span> <span class="p">(</span><span class="s">'phone'</span><span class="p">,</span> <span class="s">'(555) 123-4567'</span><span class="p">)):</span>
    <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">field</span><span class="p">).</span><span class="n">first_child</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span></code></pre></figure>

While this is certainly more code, for a larger number of lines, there will be a code savings. It's also easy to refactor all the obj.field lines at once, if for example you need to change it to `obj.field.set(value)`.

You can also make use of dynamic attributes on the class side by over-riding `__getattr__`.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Counter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="s">''' will only get called for undefined attributes '''</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
<span class="n">counter</span><span class="p">.</span><span class="n">foo</span> <span class="o">=</span> <span class="n">counter</span><span class="p">.</span><span class="n">foo</span> <span class="o">+</span> <span class="mi">100</span>
<span class="k">print</span> <span class="n">counter</span><span class="p">.</span><span class="n">foo</span>  <span class="c1"># prints '100'</span></code></pre></figure>

There is an alternate [magic method](http://www.rafekettler.com/magicmethods.html) called `__getattribute__` that fires even for attributes that are already declared. But be careful, it's easy to get into an infinite recursion loop here.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Logger</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="k">print</span> <span class="s">'Accessed attribute %s'</span> <span class="o">%</span> <span class="n">name</span>
        <span class="k">return</span> <span class="nb">object</span><span class="p">.</span><span class="n">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">Logger</span><span class="p">()</span>
<span class="n">logger</span><span class="p">.</span><span class="n">foobar</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># prints 'Accessed attribute foobar'</span></code></pre></figure>

This is a trivial example, better implemented with [decorators](http://wiki.python.org/moin/PythonDecorators). But that is a subject for another post!

Finally, there is a sister to `getattr` called `setattr`. As you would expect, this will set attributes by name. Here is a quick example:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="o">@</span><span class="nb">classmethod</span>
    <span class="k">def</span> <span class="nf">from_kwargs</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">cls</span><span class="p">()</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">field</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">obj</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">.</span><span class="n">from_kwargs</span><span class="p">(</span><span class="n">foo</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bar</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span> <span class="n">model</span><span class="p">.</span><span class="n">foo</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">bar</span>  <span class="c1"># prints '1, 2'</span></code></pre></figure>
---
title: Python pickle AttributeError 'module' object has no attribute 'foobar'
tags: python pickle
---

Ran into an interesting edge case with [pickle](http://docs.python.org/2/library/pickle.html) this week. I had a producer task that was querying objects from a database, and pickling them plus a reference to a callback function to pass to worker tasks. Everything was working fine, but I was getting sick of logging into a [Django shell](https://docs.djangoproject.com/en/dev/intro/tutorial01/#playing-with-the-api) to invoke the workers with test data. So I wrote a quick `__main__` function in my task code to do the same thing.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">django.contrib.auth</span> <span class="kn">import</span> <span class="n">user</span>
<span class="kn">from</span> <span class="nn">myproject.utils</span> <span class="kn">import</span> <span class="n">my_task_func</span><span class="p">,</span> <span class="n">worker</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s">'Invoke the workers manually.'</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'user_id'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">'User ID'</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">user</span> <span class="o">=</span> <span class="n">User</span><span class="p">.</span><span class="n">objects</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">pk</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">user_id</span><span class="p">)</span>
    <span class="n">pickled_callback</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">my_task_func</span><span class="p">)</span>
    <span class="n">pickled_cb_args</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">dumps</span><span class="p">([</span><span class="n">user</span><span class="p">])</span>
    <span class="n">worker</span><span class="p">.</span><span class="n">delay</span><span class="p">(</span><span class="n">pickled_callback</span><span class="p">,</span> <span class="n">pickled_cb_args</span><span class="p">)</span></code></pre></figure>

To my surprise, this exact same code I had been typing into a shell was now throwing an exception when invoked from inside a `__main__` function.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">):</span>
  <span class="p">...</span>
  <span class="n">File</span> <span class="s">"/usr/lib/python2.4/pickle.py"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">872</span><span class="p">,</span> <span class="ow">in</span> <span class="n">load</span>
    <span class="n">dispatch</span><span class="p">[</span><span class="n">key</span><span class="p">](</span><span class="bp">self</span><span class="p">)</span>
  <span class="n">File</span> <span class="s">"/usr/lib/python2.4/pickle.py"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">1083</span><span class="p">,</span> <span class="ow">in</span> <span class="n">load_inst</span>
    <span class="n">klass</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">find_class</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="n">File</span> <span class="s">"/usr/lib/python2.4/pickle.py"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">1140</span><span class="p">,</span> <span class="ow">in</span> <span class="n">find_class</span>
    <span class="n">klass</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
<span class="nb">AttributeError</span><span class="p">:</span> <span class="s">'module'</span> <span class="nb">object</span> <span class="n">has</span> <span class="n">no</span> <span class="n">attribute</span> <span class="s">'my_task_func'</span></code></pre></figure>

It turns out that when you pickle a function from inside a `__main__` block, the module reference that it will be pickled with is `__main__`, not the actual module namespace. This is actually a [known issue](http://bugs.python.org/issue5509). In my case, it was easy to work-around; I simply put this code into a [Django custom management command](https://docs.djangoproject.com/en/dev/howto/custom-management-commands/).

More discussion of this issue [here](http://stefaanlippens.net/pickleproblem).
---
title: HBase Schema Introduction for Programmers
tags: hbase
---

Schema design in NoSQL is very different from schema design in a RDBMS. Once you get something like HBase [up and running]( http://chase-seibert.github.io/blog/2013/02/01/getting-starting-with-hbase-and-pig.html), you may find yourself staring blankly at a shell, lost in the possibilities of creating your first table.

You're probably used to thinking of tables like this:

<table>
    <thead>
        <th>rowkey</th>
        <th>title</th>
        <th>url</th>
        <th>clicks</th>
        <th>clicks_twitter</th>
        <th>clicks_facebook</th>
    </thead>
    <tr>
        <td>fcb75-bit.ly/Z0pngZ</td>
        <td>Some Page</td>
        <td>http://www.example.com</td>
        <td>16</td>
        <td>13</td>
        <td>3</td>
    </tr>
    <tr>
        <td>fb499-bit.ly/15C2TLF</td>
        <td>null</td>
        <td>null</td>
        <td>1</td>
        <td>null</td>
        <td>null</td>
    </tr>
</table>

In HBase, this is actually modelled like this:

![HBase Table Shema](/blog/images/hbase_tables.png)

Notice that each row is basically a linked list, ordered by column family and then column name. This is how it's laid down on disk, as well. Missing columns are free, because there is no space on disk pre-allocated to a null column. Given that, it's reasonable to design a schema where rows have hundreds or thousands of columns.

Just as columns are laid down on disk like a linked list, so too are rows. They are put on disk in order by row key. Because row keys can by any collection of bytes, ordering of row keys is [lexicographical](http://en.wikipedia.org/wiki/Lexicographical_order), aka alphabetical. This is in contrast to most RDBMS, where rowkeys are integers and ordered as such.

Consider the following row key order: `1 < 256 < 43 < 7`. The row key `265` is actually before `43`, because `2` comes before `4`. This is why it's common in HBase to make at least parts of your row key fixed width, ex: `00000001 < 00000007 < 00000043 < 00000256`. However, now we have another problem known as hot spotting.

If all your row keys start with the same value, then they will all be going to the same region, and thus the same server. This could easily happen for monotonically increasing row keys, such as traditional RDBMS auto-incrementing PKs, or for timestamps. This can cause all the load for a big write job to block waiting for a single region server, versus spreading out the writes to the whole cluster. A common way to avoid this is to prefix row keys, for example by the md5 hash of the customer ID.

Rows can most efficiently be read back by scanning for consecutive blocks. Say you have a table with a rowkey of `customer-date-user`. You can easily read back all the data for a given customer and date range using the prefix `customer-first-part-of-date`, but you can't easily read back dates ranges for all users at once without scanning all the rows. If you reverse the row key and use `customer-user-date`, you have the reverse problem. So you want to think about what your primary read pattern is going to be when designing your keys. 

Say your primary read patten is going to be reading off the most recent rows. Depending on the format of the dates in your row keys, you may end up with the more recent data at the end of the table. For example: `20130101 > 20130102 > 20130303`. Instead, a common pattern is to invert your dates, such as `79869898 > 79869897 > 798698986`. This may not apply if you will know at run time the range of values that will be the most recent, i.e. the last 30 days.

For more about HBase schema design, I recommend the online [HBase Reference Book](http://hbase.apache.org/book.html), as well as the excellent [HBase: The Definitive Guide](http://www.amazon.com/HBase-Definitive-Guide-Lars-George/dp/1449396100).
---
title: Happybase Connection Pooling
tags: python hbase
---

Wrote a simple connection pool for [Happybase](https://github.com/wbolster/happybase) using [socketpool](https://pypi.python.org/pypi/socketpool).

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">happybase</span>
<span class="kn">from</span> <span class="nn">socketpool</span> <span class="kn">import</span> <span class="n">ConnectionPool</span>
<span class="kn">from</span> <span class="nn">socketpool.conn</span> <span class="kn">import</span> <span class="n">TcpConnector</span>


<span class="k">class</span> <span class="nc">HappybaseConnectionPool</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">''' singleton to share a connection pool per process '''</span>

    <span class="n">_instance</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">__new__</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">cls</span><span class="p">.</span><span class="n">_instance</span><span class="p">:</span>
            <span class="n">cls</span><span class="p">.</span><span class="n">_instance</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">HappybaseConnectionPool</span><span class="p">,</span> <span class="n">cls</span><span class="p">).</span><span class="n">__new__</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">.</span><span class="n">_instance</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">host</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
        <span class="n">options</span><span class="p">[</span><span class="s">'host'</span><span class="p">]</span> <span class="o">=</span> <span class="n">host</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">'pool'</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">ConnectionPool</span><span class="p">(</span>
                <span class="n">factory</span><span class="o">=</span><span class="n">HappybaseConnector</span><span class="p">,</span>
                <span class="n">max_size</span><span class="o">=</span><span class="n">options</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'max_size'</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
                <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">connection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool</span><span class="p">.</span><span class="n">connection</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span>

    <span class="o">@</span><span class="n">contextlib</span><span class="p">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">table</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_name</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool</span><span class="p">.</span><span class="n">connection</span><span class="p">()</span> <span class="k">as</span> <span class="n">connector</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">connector</span><span class="p">.</span><span class="n">table</span><span class="p">(</span><span class="n">table_name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">HappybaseConnector</span><span class="p">(</span><span class="n">TcpConnector</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">host</span> <span class="o">=</span> <span class="n">host</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">port</span> <span class="o">=</span> <span class="n">port</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">connection</span> <span class="o">=</span> <span class="n">happybase</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">host</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">port</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_connected</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="c1"># use a 'jiggle' value to make sure there is some
</span>        <span class="c1"># randomization to expiry, to avoid many conns expiring very
</span>        <span class="c1"># closely together.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">_life</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_pool</span> <span class="o">=</span> <span class="n">pool</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">logging</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'logging'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">is_connected</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">_connected</span> <span class="ow">and</span> <span class="bp">self</span><span class="p">.</span><span class="n">connection</span><span class="p">.</span><span class="n">transport</span><span class="p">.</span><span class="n">isOpen</span><span class="p">():</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># isOpen is unreliable, actually try to do something
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">connection</span><span class="p">.</span><span class="n">tables</span><span class="p">()</span>
                <span class="k">return</span> <span class="bp">True</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>
        <span class="k">return</span> <span class="bp">False</span>

    <span class="k">def</span> <span class="nf">handle_exception</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exception</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">logging</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">logging</span><span class="p">.</span><span class="n">error</span><span class="p">(</span><span class="n">exception</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span> <span class="n">exception</span>

    <span class="k">def</span> <span class="nf">invalidate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">connection</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_connected</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_life</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">def</span> <span class="nf">open</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">release</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'table'</span><span class="p">,</span> <span class="s">'tables'</span><span class="p">,</span> <span class="s">'create_table'</span><span class="p">,</span> <span class="s">'delete_table'</span><span class="p">,</span>
                <span class="s">'enable_table'</span><span class="p">,</span> <span class="s">'disable_table'</span><span class="p">,</span> <span class="s">'is_table_enabled'</span><span class="p">,</span> <span class="s">'compact_table'</span><span class="p">]:</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">connection</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">AttributeError</span><span class="p">(</span><span class="n">name</span><span class="p">)</span></code></pre></figure>

You can get a single pool object from anywhere in your stack with the following:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">pool</span> <span class="o">=</span> <span class="n">HappybaseConnectionPool</span><span class="p">(</span><span class="s">'localhost'</span><span class="p">,</span> <span class="s">'9090'</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pool</span><span class="p">.</span><span class="n">connection</span><span class="p">()</span> <span class="k">as</span> <span class="n">connection</span><span class="p">:</span>
     <span class="n">connection</span><span class="p">.</span><span class="n">create_table</span><span class="p">(</span><span class="s">'foobar'</span><span class="p">)</span></code></pre></figure>

Note: happybase may be adding their [own connection pool](https://github.com/wbolster/happybase/issues/21) shortly.
---
title: Hive with HBase Quickstart
tags: hbase hive
---

Though there is some [decent documentation](https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration), I found that setting up Hive with a HBase back-end to be somewhat fiddly. Hopefully this guide will help you get started quicker. This article presumes that you already have HBase set up. If not, see my [HBase quickstart](http://chase-seibert.github.io/blog/2013/02/01/getting-starting-with-hbase-and-pig.html).

Note: these directions are for development. They don't use HDFS, for example. For a full guide on production deployment, see the excellent [CDH4 directions](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/cdh4ig_topic_18.html).

## Linux

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">sudo </span>apt-get <span class="nb">install </span>hive

<span class="c"># create directory that Hive stores data in by default</span>
<span class="nb">sudo mkdir</span> <span class="nt">-p</span> /user/hive/warehouse
<span class="nb">sudo chown</span> <span class="nt">-R</span> myusername:myusername /user/hive/warehouse/

<span class="c"># copy HBase JARs into the Hive lib</span>
<span class="nb">sudo cp</span> /usr/share/hbase/hbase-0.92.1.jar /usr/lib/hive/lib
<span class="nb">sudo cp</span> /usr/share/hadoop-zookeeper/zookeeper-3.4.3.jar /usr/lib/hive/lib</code></pre></figure>

## OSX

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">brew <span class="nb">install </span>hive</code></pre></figure>

## Connect to HBase

Now, you can fire up hive with the `hive` command and create a table that's backed by HBase. For this example, my HBase table is called `test`, and has a column family of integer values called `values`. Note that the dropping/creating of tables is just effecting Hive meta-data; no actual changes are made in HBase.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">DROP TABLE IF EXISTS <span class="nb">test</span><span class="p">;</span>

CREATE EXTERNAL TABLE
    <span class="nb">test</span><span class="o">(</span>key string, values map&lt;string, int&gt;<span class="o">)</span>
STORED BY
    <span class="s1">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span>
WITH SERDEPROPERTIES <span class="o">(</span>
    <span class="s2">"hbase.columns.mapping"</span> <span class="o">=</span> <span class="s2">":key,values:"</span>
    <span class="o">)</span>
TBLPROPERTIES <span class="o">(</span>
    <span class="s2">"hbase.table.name"</span> <span class="o">=</span> <span class="s2">"test"</span>
    <span class="o">)</span><span class="p">;</span>

SELECT <span class="k">*</span> FROM <span class="nb">test</span><span class="p">;</span>

<span class="o">&gt;</span>c4ca4-0000001-79879483-000000000124-000000000000000000000000000025607621 <span class="o">{</span><span class="s1">'comments'</span>:0, <span class="s1">'likes'</span>:0<span class="o">}</span>
<span class="o">&gt;</span>c4ca4-0000001-79879483-000000000124-000000000000000000000000000025607622 <span class="o">{</span><span class="s1">'comments'</span>:0, <span class="s1">'likes'</span>:0<span class="o">}</span></code></pre></figure>

## Simple Map Reduce Example

Give the above raw data in the table, here is example GROUP/SUM map reduce where you sum up the various HBase columns in the values column family. This example creates a view to handle the blowing apart of the HBase rowkey. You can use an `INSERT OVERWRITE` statement at the end to write the results back into Hbase.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">CREATE VIEW
    test_view AS
SELECT
    substr<span class="o">(</span>key, 0, 36<span class="o">)</span> as org_date_asset_prefix,
    <span class="nb">split</span><span class="o">(</span>key, <span class="s1">'-'</span><span class="o">)[</span>2] as inverse_date_str,
    stats[<span class="s1">'comments'</span><span class="o">]</span> as comments,
    stats[<span class="s1">'likes'</span><span class="o">]</span> as likes
FROM
    <span class="nb">test</span><span class="p">;</span>

SELECT
    org_date_asset_prefix,
    map<span class="o">(</span>
      <span class="s1">'comments'</span>, SUM<span class="o">(</span>comments<span class="o">)</span>,
      <span class="s1">'likes'</span>, SUM<span class="o">(</span>likes<span class="o">)</span>
    <span class="o">)</span> as stats
FROM
    test_view
GROUP BY
    org_date_asset_prefix<span class="p">;</span></code></pre></figure>

## Thrift REST API

If you want to connect to Hive via thrift, you can start the thrift service with `hive --service hiveserver`. [Hiver](https://github.com/tebeka/hiver) is a nice little Python API wrapper.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">hiver</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">hiver</span><span class="p">.</span><span class="n">connect</span><span class="p">(</span><span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">)</span>
<span class="n">client</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">'SHOW TABLES'</span><span class="p">)</span>
<span class="n">rows</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">fetchAll</span><span class="p">()</span></code></pre></figure>
---
title: Write CSV data into Hive and Python
tags: python hive
---

[Apache Hive](http://hive.apache.org/) is a high level SQL-like interface to Hadoop. It lets you execute mostly unadulterated SQL, like this:

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_table</span><span class="p">(</span><span class="k">key</span> <span class="n">string</span><span class="p">,</span> <span class="n">stats</span> <span class="k">map</span><span class="o">&lt;</span><span class="n">string</span><span class="p">,</span> <span class="nb">int</span><span class="o">&gt;</span><span class="p">);</span></code></pre></figure>

The `map` column type is the only thing that doesn't look like vanilla SQL here. Hive can actually use different backends for a given table. Map is used to interface with column oriented backends like HBase. Essentially, because we won't know ahead of time all the column names that could be in the HBase table, Hive will just return them all as a key/value dictionary. There are then helpers to access individual columns by key, or even pivot the map into one key per logical row.

As part of the Hadoop family, Hive is focused on bulk loading and processing. So it's not a surprise that Hive does not support inserting raw values like the following SQL:

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">suppliers</span> <span class="p">(</span><span class="n">supplier_id</span><span class="p">,</span> <span class="n">supplier_name</span><span class="p">)</span> <span class="k">VALUES</span> <span class="p">(</span><span class="mi">24553</span><span class="p">,</span> <span class="s1">'IBM'</span><span class="p">);</span></code></pre></figure>

However, for unit testing Hive scripts, it would be nice to be able to insert a few records manually. Then you could run your map reduce HQL, and validate the output. Luckily, Hive can load CSV files, so it's relatively easy to insert a handful or records that way.

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">foobar</span><span class="p">(</span><span class="k">key</span> <span class="n">string</span><span class="p">,</span> <span class="n">stats</span> <span class="k">map</span><span class="o">&lt;</span><span class="n">string</span><span class="p">,</span> <span class="nb">bigint</span><span class="o">&gt;</span><span class="p">)</span>
<span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">DELIMITED</span>
<span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">','</span>
<span class="n">COLLECTION</span> <span class="n">ITEMS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">'|'</span>
<span class="k">MAP</span> <span class="n">KEYS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">':'</span> <span class="p">;</span>

<span class="k">LOAD</span> <span class="k">DATA</span> <span class="k">LOCAL</span> <span class="n">INPATH</span> <span class="s1">'/tmp/foobar.csv'</span> <span class="k">INTO</span> <span class="k">TABLE</span> <span class="n">foobar</span><span class="p">;</span></code></pre></figure>

This will load a CSV file with the following data, where `c4ca4-0000001-79879483-000000000124` is the key, and `comments` and `likes` are columns in a map.

<figure class="highlight"><pre><code class="language-text" data-lang="text">c4ca4-0000001-79879483-000000000124,comments:0|likes:0
c4ca4-0000001-79879483-000000000124,comments:0|likes:0</code></pre></figure>

Because I've been doing this quite a bit in my unit tests, I wrote a quick Python helper to dump a list of key/map tuples to a temporary CSV file, and then load it into Hive. This uses [hiver](https://github.com/tebeka/hiver) to talk to Hive over thrift.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">hiver</span>
<span class="kn">from</span> <span class="nn">django.core.files.temp</span> <span class="kn">import</span> <span class="n">NamedTemporaryFile</span>


<span class="k">def</span> <span class="nf">_hql</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hql</span><span class="p">):</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">hiver</span><span class="p">.</span><span class="n">connect</span><span class="p">(</span><span class="n">settings</span><span class="p">.</span><span class="n">HIVE_HOST</span><span class="p">,</span> <span class="n">settings</span><span class="p">.</span><span class="n">HIVE_PORT</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">client</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="n">hql</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">client</span><span class="p">.</span><span class="n">shutdown</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">insert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">rows</span><span class="p">):</span>
    <span class="s">''' cannot insert single rows via hive, need to save to a temp file and bulk load that '''</span>
    <span class="n">csv_file</span> <span class="o">=</span> <span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">delete</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span>
        <span class="n">map_repr</span> <span class="o">=</span> <span class="s">'|'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="s">'%s:%s'</span> <span class="o">%</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">items</span><span class="p">())</span>
        <span class="n">csv_file</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s">","</span> <span class="o">+</span> <span class="n">map_repr</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
    <span class="n">csv_file</span><span class="p">.</span><span class="n">flush</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">_hql</span><span class="p">(</span><span class="s">'DROP TABLE IF EXISTS %s'</span> <span class="o">%</span> <span class="n">table_name</span><span class="p">)</span>
        <span class="n">_hql</span><span class="p">(</span><span class="s">"""
            CREATE TABLE
                %s (
                    key string,
                    map&lt;string, int&gt;
                )
            ROW FORMAT DELIMITED
            FIELDS TERMINATED BY ','
            COLLECTION ITEMS TERMINATED BY '|'
            MAP KEYS TERMINATED BY ':'
        """</span> <span class="o">%</span> <span class="p">(</span><span class="n">table_name</span><span class="p">))</span>
        <span class="n">_hql</span><span class="p">(</span><span class="s">"""
            LOAD DATA LOCAL INPATH '%s' INTO TABLE %s
        """</span> <span class="o">%</span> <span class="p">(</span><span class="n">csv_file</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">table_name</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">csv_file</span><span class="p">.</span><span class="n">close</span><span class="p">()</span></code></pre></figure>

You can call it like this:

<figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="n">insert</span><span class="p">(</span><span class="s">'test_table'</span><span class="p">,</span> <span class="p">[</span>
        <span class="p">(</span><span class="s">'c4ca4-0000001-79879483-000000000124'</span><span class="p">,</span> <span class="p">{</span><span class="s">'comments'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'likes'</span><span class="p">:</span> <span class="mi">2</span><span class="p">}),</span>
        <span class="p">(</span><span class="s">'c4ca4-0000001-79879483-000000000124'</span><span class="p">,</span> <span class="p">{</span><span class="s">'comments'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'likes'</span><span class="p">:</span> <span class="mi">2</span><span class="p">}),</span>
        <span class="p">(</span><span class="s">'c4ca4-0000001-79879496-000000000124'</span><span class="p">,</span> <span class="p">{</span><span class="s">'comments'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'likes'</span><span class="p">:</span> <span class="mi">2</span><span class="p">}),</span>
        <span class="p">(</span><span class="s">'b4aed-0000002-79879783-000000000768'</span><span class="p">,</span> <span class="p">{</span><span class="s">'comments'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'likes'</span><span class="p">:</span> <span class="mi">2</span><span class="p">}),</span>
        <span class="p">(</span><span class="s">'b4aed-0000002-79879783-000000000768'</span><span class="p">,</span> <span class="p">{</span><span class="s">'comments'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'likes'</span><span class="p">:</span> <span class="mi">2</span><span class="p">}),</span>
    <span class="p">])</span></code></pre></figure>

---
title: High availability via client libraries versus dedicated services
tags:
---

Some newer storage technologies allow you to connect to one of a set of servers right from their client library. For example, [MongoDB](http://api.mongodb.org/python/current/examples/high_availability.html) lets you specify one host in a replica set. If that replica becomes unavailable, the client will try to connect to another replica in the same set automatically. Likewise, all [memcached](https://github.com/linsomniac/python-memcached/blob/master/memcache.py) clients allow you to specify a whole list of connect strings. If one host is down, the cache lookups for that host will automatically start going to another server. Both of these are examples of high availability by client library.

For MongoDB and memcached in particular, there are features of the platform that make specific load balancing logic desirable. For MongoDB, you want the queries to be sharded consistently. For memcached, you want cache keys to consistently hash to the same server, at least while that server is up. For MySQL and Apache Thift, typically any server in the high availability set can receive a query. Why don't more client libraries implement high availability? For example, I could not find a MySQL or Apache Thrift client that did. For those services, it's commonly recommended to run an internal load balancer in between the client and the servers. 

Given a datastore where no high availability client library exists, should you write your own, or use a dedicated load balancer? At first blush, it would seem that just randomly choosing a server from a list would get you basic load balancing. For high availability, you would additionally need to intelligently remove servers from the list when they are down, and add them in again when they come back up. This is known as health checking, and it's built-in to any decent load balancer. This should be your first hint that you're embarking on re-inventing the wheel.

Let's look at [HAProxy](http://haproxy.1wt.eu/) in particular, an excellent open-source load balancer. Here is a list of the kinds of features that I have found useful over the years. I certainly would not want to have to write all these myself!

* Header inspection
* Header re-writing
* Buffering
* Least-connection routing
* ACLs
* Logging
* SSL Proxy
* Compression
* Timeouts
* [Configurable](http://chase-seibert.github.io/blog/2011/02/26/haproxy-quickstart-w-full-example-config-file.html) from a simple text file
* Metrics!

![haproxy dashboard](https://tech.shareaholic.com/post-assets/2012/10/26/haproxy-dashboard.png)

There is also a fundamental advantage to having a centralized load balancer versus distributed load balancing logic. It allows you to make routing decisions based on the activity of the aggregated cluster, not just your individual node. For example, you can enforce that a given node never receives more than X concurrent connections across all clients.

In addition, like any good open-source project, HAProxy is well documented. It's also re-usable, as you built knowledge about how to deploy it for various solutions, you're leveraging what you're already learned about it to provide even better solutions.

The downside is that while client library high availability removes single points of failure from the system, with a dedicated load balancer you have to implement high availability of the load balancer itself. Typically, this is done via shared IPs and [heartbeat failover](http://www.howtoforge.com/high-availability-load-balancer-haproxy-heartbeat-fedora8).
---
title: Sqoop/HBase Quickstart on Linux
tags: sqoop hadoop hbase
---

[Sqoop](http://sqoop.apache.org/) is a tool for bulk copying data between a relational database like MySQL and HDFS or another Hadoop based data store like Hive or HBase. It can either export a table or set of tables, or you can specify a custom SQL query to pull the data out. It's the best solution out there for moving massive data sets; it can even fan out sqoop workers to a configurable number of Hadoop data nodes, which will all run partitioned versions of the main SQL query in parallel.

# Install

To get started, you will need to install Sqoop. The easiest method on Linux is to use the [Cloudera](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/CDH4-Installation-Guide.html) repository. You will also need the JDBC MySQL driver, and the JDK (Sqoop compiles a JAR on the fly and sends it out as a MapReduce job).

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt;&gt; /etc/apt/sources.list.d/cloudera.list
deb [arch=amd64] http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh precise-cdh4 contrib
deb-src http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh precise-cdh4 contrib
</span><span class="no">EOF
</span>curl <span class="nt">-s</span> http://archive.cloudera.com/cdh4/ubuntu/lucid/amd64/cdh/archive.key| <span class="nb">sudo </span>apt-key add -
<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nb">install </span>sqoop libmysql-java openjdk-7-jdk</code></pre></figure>

# Run it!

Say you have a MySQL table `user` and a HBase table with the same name. If you want to do a straight copy of the data and use the `id` column as the HBase rowkey and store all the columns in a HBase column family named `data`, all you need to do is:

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sqoop-import <span class="nt">--connect</span> jdbc:mysql://<span class="nv">$MYSQL_SERVER</span>/<span class="nv">$DATABASE</span> <span class="nt">--driver</span> com.mysql.jdbc.Driver <span class="nt">--username</span> <span class="nv">$USER</span> <span class="nt">--password</span> <span class="nv">$PASSWORD</span> <span class="nt">--table</span> user <span class="nt">--hbase-table</span> user <span class="nt">--hbase-row-key</span> <span class="nb">id</span> <span class="nt">--column-family</span> data</code></pre></figure>

At least for HBase, you are almost always want to compose some more sophisticated rowkey, to avoid [region hotspotting](http://my.safaribooksonline.com/book/databases/database-design/9781449314682/optimizing-splits-and-compactions/id3163684). If you can express that rowkey as a SQL statement, you're good to go. Instead of `--table`, you specify a `--query`, and change the column referenced in `--hbase-row-key`.

Say we want the rowkey to be the first five characters of the md5 hash of the `company_id` field in the `user` table, plus the `date_added` field formatted as an eight character string, plus the `id` field. Example: `a8f5c-20130101-47`.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sqoop-import <span class="nt">--connect</span> jdbc:mysql://<span class="nv">$MYSQL_SERVER</span>/<span class="nv">$DATABASE</span> <span class="nt">--driver</span> com.mysql.jdbc.Driver <span class="nt">--username</span> <span class="nv">$USER</span> <span class="nt">--password</span> <span class="nv">$PASSWORD</span> <span class="nt">--hbase-table</span> user <span class="nt">--hbase-row-key</span> <span class="nb">id</span> <span class="nt">--column-family</span> data <span class="nt">--query</span> <span class="s2">"SELECT  CONCAT_WS('-', SUBSTR(MD5(a.company_id), 1, 5), DATE_FORMAT(a.date_added, '%Y%m%d'), a.id) as rowkey, a.* FROM user a WHERE </span><span class="se">\$</span><span class="s2">CONDITIONS"</span></code></pre></figure>

_Note: the `$CONDITIONS` is a placeholder for the dynamic partitioning of data across server. You may also need to specify a `--split-by` column._

# Tuning

For a large number of rows, you may find that Sqoop is using a lot of memory to copy the data over. You may even run into a `java.lang.OutOfMemoryError: Java heap space` or a `java.lang.OutOfMemoryError: GC overhead limit exceeded`. If you do, it's likely because the MySQL database driver is fetching all of the rows of the table, and keeping them in memory. You can tell it to chunk up the query into pages and use a cursor by changing the connect string to: `mysql://$MYSQL_SERVER/$DATABASE?dontTrackOpenResources=true\&defaultFetchSize=1000\&useCursorFetch=true`. See the [documentation](http://dev.mysql.com/doc/refman/5.1/en/connector-j-reference-configuration-properties.html).

# Trouble-shooting

If you get an error saying that sqoop cannot load the MySQL driver, you may need to do a manual `sudo cp /usr/share/java/mysql.jar /usr/lib/sqoop/lib` to copy it to the right place.

If you get the error `0000-00-00 00:00:00' can not be represented as java.sql.Timestamp`, you should modify your connect string to add the `zeroDateTimeBehavior` flag, ie `mysql://$MYSQL_SERVER/$DATABASE?zeroDateTimeBehavior=convertToNull`
---
title: Hadoop from a Python Perspective
tags: python hbase hadoop
---

I'm just coming off a project where we decided to use Hadoop for the first time. We're a Python shop developing an analytics feature. We have about 150m records we need to analyze daily, or approx 20GB of data. Even in our initial discussions, we knew that we could do this with our existing stack of Python and MySQL. But we wanted to "get our feet wet" with Hadoop, and gain some experience with it to see if we could make use of it more broadly.

> Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it... - [Dan Ariely](https://www.facebook.com/dan.ariely/posts/904383595868?imm_mid=0a9701&cmp=em-strata-newsletters-strata-olc-20130529-elist)

We actually did get everything working with a stack of HBase/Sqoop/Hive. But we were so unhappy with a number of aspects of the solution that we're currently going back and re-implementing in Python/MySQL. So what did we learn?

## Hadoop/HBase are not general purpose

As should be obvious from even a cursory reading of the documentation, Hadoop HDFS itself is not a general purpose random access system. That's was HBase is for. But even then, you're gaining scalability of bulk writes and reads, but losing flexibility in the form of fast deletes and updates, as well as real-time sorting. You also end up having to denormalize your data once for every read pattern you will have. Having worked for so long with relational databases, you tend to take for granted all of the flexibility they give you to change your requirements on the fly. SQL is down-right magical for data sets up to a fairly large size like 1TB.

Don't give up one of the most valuable tools in your toolbox until you absolutely have to. I would go as far as to say if you have a big data problem, you'll know it because it's crushing your servers and you have already spent months optimizing. Until then, avoid Hadoop.

> It's likely that you simply have "large data," which practically every company has. Large data - and even most big data right now - can be analyzed and visualized in real time using BI software like arcplan without the need to invest in in-memory appliances like SAP HANA, massive data warehouses like Teradata, NoSQL databases like Cassandra, and distributed processing like Hadoop. - [Tiemo Winterkamp](http://biblog.arcplan.com/2012/12/why-im-not-impressed-with-your-big-data/)

## You can't avoid Java

I've worked with Java a lot in the past, and I made an explicit decision to re-orient my career more towards Python. I could list a dozen reasons why, but it all boils down to this: I personally find working with Python to be a joy, while working with Java always felt like work. With that in mind, you shouldn't delude yourself into thinking that you can treat Hadoop like a black box.

In theory you can just talk to HBase and kick off Hive jobs via their respective thrift clients. In practice, you end up having to deploy the services themselves, which is fairly complicated even with good distributions like Cloudera. You will also end up tuning JVM parameters and hunting through I-shit-you-not hundred line `CLASSPATH` declarations. You will end up Googling for that one JAR that you just can't seem to find, and then decompressing it to make sure it really is the exact version you need.

You will watch everything crash and burn because of a .01 version discrepancy between two dependencies you're only notionally aware of. And you will find yourself staring at a 1,000 line stacktrace that tells you absolutely nothing about what the actual problem is. Don't say I didn't warn you.

## You can't avoid MapReduce, either

Don't think you can get away with exclusively using Hive or Pig for your map reduce jobs. Firstly, even if you could, the best case scenario is that you end up with either pig of hql scripts. Those are fairly ugly looking pieces of code, especially the pig scripts. They provide a decent abstraction layer, but not enough that you could put an actual Python abstraction layer on top of them without losing all their functionality. They are also not testable outside a JVM, so be prepared to implement a new stack of continuous integrations tests.

Pig does have some Python interoperability. You can write custom UDFs in Python, and they will run via Jython in your map reduce jobs. But that only gives you very basic extensions. There are whole classes of UDFs that can only be written in Java.

Hive, while pretty cool, is also missing a lot of basic operations. For example, there is currently no way to compute a running total without [a custom UDF](https://issues.apache.org/jira/browse/HIVE-2361). Implementing one is non-trivial; we're talking thousands of lines of Java code, during which you have to fully comprehend the way map reduce works. Suddenly your map reduce abstraction isn't so abstract.

## Hadoop is still maturing

Hadoop itself is fairly tricky to deploy. It's also still a work in progress. Even minor version changes can still break things in spectacular fashion. Definitely use Cloudera, but don't expect a turn-key deployment. You should expect to invest in at least a dozen physical machines or VMs to get started. In fact, if you're not willing to eventually run tens or hundreds of machines in a cluster, you probably don't really need Hadoop.

Be prepared to spend at least some time on reliability issues. Like anything, new infrastructure can be flaky and will go down regularly until you work out all the kinks. For that reason, I would not recommend having anything user facing directly hitting HBase.

## Trust your gut

> You don't have a Big Data problem. - [Brent Ozar](http://www.brentozar.com/archive/2013/03/you-dont-have-a-big-data-problem/)

Double and triple check that you really need a big data solution. Until then, you're  better off with a traditional stack. If you think you can deliver a solution using Python + MySQL, do that. Leverage the "magic" of a relational database. Don't under-value the loss of data integrity, flexibility, tooling, testability and maintainability of your existing stack.

If you do end up with a big data infrastructure project, at least take a look at some of the newer Python native solutions. Also, think about doing a pure research/prototyping spike before you try to deliver a new feature AND a new infrastructure together.

> A recent Microsoft research investigation facetiously titled 'No one ever got fired for using Hadoop on a cluster' found misguided Hadoop installations both at their company and at Yahoo!,  processing less than 14G of data.  The paper concludes by advising analysts to not go through the Hadoop hoops until your data size passes standard hard drive limits (currently around 1 Terabyte) or at least reasonable memory limits (512 GB). - [Dave Fowler](http://www.wired.com/insights/2012/11/lets-talk-your-data-not-big-data/)
---
title: Fixing styling incompatibility with Google Charts and Twitter Bootstrap
tags: bootstrap gcharts
---

Ran into a vexing issue this week when we upgraded to using [Twitter Bootstrap](http://twitter.github.io/bootstrap/). Some of our analytics graphs using [Google Charts](https://developers.google.com/chart/interactive/docs/index) were suddenly broken in IE 7/8/9. Instead of the expected 100px wide bar graphs, they were just a few pixels wide.

![google charts bad](/blog/images/google_chart_bars_bad.png)

Normally Google Charts produces the following DOM for their bar graphs.

<figure class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;span</span> <span class="na">style=</span><span class="s">"padding: 0; float: left; white-space: nowrap;"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;img</span> <span class="na">style=</span><span class="s">"padding: 0px;"</span> <span class="na">src=</span><span class="s">"/images/bar_s.png"</span> <span class="na">height=</span><span class="s">"12"</span> <span class="na">width=</span><span class="s">"1"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;img</span> <span class="na">style=</span><span class="s">"padding: 0px;"</span> <span class="na">src=</span><span class="s">"/images/bar_b.png"</span> <span class="na">height=</span><span class="s">"12"</span> <span class="na">width=</span><span class="s">"16"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;img</span> <span class="na">style=</span><span class="s">"padding: 0px;"</span> <span class="na">src=</span><span class="s">"/images/bar_w.png"</span> <span class="na">height=</span><span class="s">"12"</span> <span class="na">width=</span><span class="s">"34"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;img</span> <span class="na">style=</span><span class="s">"padding: 0px;"</span> <span class="na">src=</span><span class="s">"/images/bar_s.png"</span> <span class="na">height=</span><span class="s">"12"</span> <span class="na">width=</span><span class="s">"1"</span><span class="nt">&gt;</span>
    <span class="ni">&amp;nbsp;</span>1.6
<span class="nt">&lt;/span&gt;</span></code></pre></figure>

It turns out that the issue is with Bootstrap's CSS for responsive images, and in particular the `img { width: auto; }` declaration. It causes older versions if IE to ignore `height` and `width` attributes that are set directly on the DOM element. This issue has [come](https://github.com/twitter/bootstrap/issues/1552) up [before](https://github.com/twitter/bootstrap/pull/7369) in [other](http://stackoverflow.com/questions/13694837/internet-explorer-img-width-not-taking-effect) contexts. Removing the offending CSS solves the issue, but I wanted to stick with vanilla Bootstrap so we could cleanly upgrade to new vesions. Likewise, you probably don't want to edit the Google Charts javascript.

Instead, you can layer in some additional javascript, fired on the Google Charts `onReady` event, to add inline `width` declarations into the `style` attribute for the images. While IE 7/8/9 ignore width attributes when combined with `img { width: auto; }`, it _does_ respect `style` attributes.

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">function</span> <span class="nx">fixGoogleChartsBarsBootstrap</span><span class="p">()</span> <span class="p">{</span>
    <span class="cm">/* Google charts uses &lt;img width="12px"&gt;, which is incompatible with Twitter
     * bootstrap in responsive mode, which inserts a css rule for: img { width: auto; }.
     * On IE7/8/9, that causes the browser to ignore the width attribute, and
     * the bars are collapsed to 1px each.
     *
     * The fix is to use inline style width attributes, ie &lt;img style="width: 12px;"&gt;.
     * BUT we can't change the way Google Charts renders its bars. Nor can we change
     * the Twitter bootstrap CSS and remain future proof.
     *
     * Instead, this function can be called after a Google charts render to "fix" the
     * issue by setting the style attributes dynamically.
     */</span>
    <span class="nx">$</span><span class="p">(</span><span class="dl">"</span><span class="s2">img[width]</span><span class="dl">"</span><span class="p">).</span><span class="nx">each</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">index</span><span class="p">,</span> <span class="nx">img</span><span class="p">)</span> <span class="p">{</span>
        <span class="nx">img</span><span class="p">.</span><span class="nx">css</span><span class="p">(</span><span class="dl">"</span><span class="s2">width</span><span class="dl">"</span><span class="p">,</span> <span class="nx">img</span><span class="p">.</span><span class="nx">attr</span><span class="p">(</span><span class="dl">"</span><span class="s2">width</span><span class="dl">"</span><span class="p">));</span>
    <span class="p">});</span>
<span class="p">};</span></code></pre></figure>

![google charts bad](/blog/images/google_chart_bars_good.png)
---
title: Agile Motivations and Objections
tags: agile scrum
---

Often when I first start talking to someone new about Agile and/or Scrum, they have questions and occasionally objections. More often they just don't understand the motivation for trying a new development process. It's rare that these biases against Scrum persist, which is probably a testament to how common-sense it all is.

Still, in these situations I find it personally useful to have my thoughts collected and written down somewhere. So here is a quick run-down of common motivations and objections for Scrum.

## Motivation - For Engineers

TL;DR - Scrum is purpose built to make developer's lives easier.

### Fewer interruptions

In many development environments, it's common to be constantly or at least persistently distracted from your primary work by secondary requests coming in from outside the development team. Maybe it's a bug fix for a customer, or a site outage. Maybe it's an important feature to close a sale.

Most of the time, those tasks make sense and need to happen. Sometimes they really do need to happen **right now**. But too often, they are mis-prioritized and improperly allocated to a particular person instead of to the whole team. Scrum at least makes the product owner aware that these are coming in and reducing team velocity. In the best case, a good Scrum Master will deflect or service the majority of these themselves.

### Sustainable pace, aka the anti-crunch

No one likes crunch time. Many organizations institutionalize their lack of planning, and impose it as a recurring crisis on the engineering team. It doesn't have to be that way. Scrum empowers developers to own their commitments, but makes sure those estimates are solely reality driven. No PM can sign you up for an unrealistic deadline. If your estimates are off, they can only effect your life for on average half the sprint duration (usually one week).

### No requirements changes during the sprint

Tired of doing some work, showing it to a PM and having them say "actually, I meant THIS". Tough, that will always be the case! But at least with Scrum you can have a structured discussion about whether this scope change is large enough to constitute a new story for **next** sprint. If it's small, you can always just do it.

### Have a say in what gets built

Good teams always do this. With Scrum, it's a formally acknowledged part of the process. Engineers have an explicit venue (the grooming session) to talk about the technical details and design trade-offs of an upcoming story **before** it gets committed to.

### Someone to support YOU for a change

Sometimes engineers feel like everyone in the company has license to wander up and ask them to do some very important totally random task. Sweet, you must be super indispensable. With Scrum, now you have a lackey to hand these tasks off too; someone **you** can ask to "just take care of something". Meet your friendly neighborhood Scrum Master. They are like a task rabbit that can code.

## Motivation - For Product Managers

Product peeps have a (another) trade-off to make. With Scrum, you are giving up the illusion that you can predict what exact features can be delivered 6 or 12 months from now. But, your team will maximize customer value and deliver what they say they will deliver on time, consistently.

### Predictability

Maybe you don't know where you will end up in 6 months, but you will always be making progress on the next most valuable feature. Bonus: you will have a high degree of confidence that the current work will be done in two weeks or less. Plus, now that really means "Done-Done", i.e. no stuff that's 90% done for weeks on end.

### Definition of done

Speaking of which, you get to formally define a definition of what it means for a story to be "Done-Done". If you want that to include manually checking 16 different language packs, you can do that (if you can get the team to agree to it).

### Fast feedback

Every two weeks, you can also show the new hotness to actual users. Done means deployable, which means it's all integrated and ready to show to a customer. You can get real feedback on a real, working version of the feature in as little as two weeks.

### Bounds planning errors to two weeks

What happens if you royally, totally mess up scoping/grooming/executing an entire sprints worth of work? You lose exactly two weeks. That's it.

### Can change mind between sprints

True, you cannot change your mind about what you want to build during a sprint. But between sprints you can make any super radical pivot you want. It's less than two weeks away!

## Common Objections

### More meetings

Yeah, you may spend up to 10% of your time on stand-ups, grooming meetings, planning meetings, demos and retrospectives. You might be talking about one day of collaborating for a two week sprint. You will quickly get better at it, and get it down to maybe four hours.

In my experience, you are probably currently wasting way more time than that by not always working on the right thing. If you re-allocate the hours from that once a quarter, three day "oh my god we're so behind, what do we do?" Uber-meeting, it's a wash.

### But we **NEED** a 6 month plan

Just make one up at random. It's probably as accurate as the one you thought you had with a Waterfall design process.

### Can't change priorities

What are you talking about? You're now in **sole** control of the priority list. True, you need to wait until the end of the sprint, which is at most two weeks away.

### Story Points don't mean anything

By design, story points do not equate to hours. That's fine; they are only used for comparing stories _relative_ to each other.

## Pitfalls

### Velocity is not an employee evaluation metric

If you tie compensation to velocity, I assure you that you will get exactly what you asked for. Your velocity will quickly approach infinity. You can breathlessly tweet that your mobile team just completed 8 billion story points in two weeks. It will also cease to have any correlation to actual work getting done.

### Velocity should not be ever-increasing

See above. You can expect velocity to start out rocky with a new Scrum team. Things should stabilize in a just a few sprints, as people get the hang of it. After that, you can expect it to plateau, except for unrelated efficiency gains in automation/tooling/etc. Adding people to the team will temporarily drop velocity, but it should plateau again at a higher level (at least up to 7 or 9 total members).

### Scrum masters should not manage team members

This one is tricky for smaller organizations. Ideally, a Scrum team should be self-organizing. It can be difficult for all team members to feel fully self-directed and free to speak their mind if the person who gives them their performance review is also on the team. Scrum Master is especially at risk as they are explicitly not supposed to tell team members what to do, or how to do it.

### Retrospectives are forever

The process is malleable. But don't stop holding a retrospective regularly. Don't even cancel just one, not even once. As soon as you stop your commitment to self-improvement, you begin a slow atrophy to dysfunction. Morale is unsurprisingly correlated to how empowered team members feel.

## Trade-offs

Everything is a trade-off. Scrum is no different.

### No gold plating time

Quality should be built in. Refactoring is a daily routine that falls under the same bucket as writing new code in your estimates. The other 80% of "polishing" time is probably not adding as much value as the engineers think it is. YAGNI. If it is really valuable, you should be able to sell it to the product owner as worthwhile.

### Pure refactoring negotiable

Everyone has larger pieces of pure technical debt that need to be addressed. The barrier to entry for actually getting time allocated to those **is** higher with Scrum, which is exclusively focused on delivering user facing value. As an engineer, you need to learn to articulate those projects in terms of value. Also, try to tie them to related feature work.

### Forces grooming up front

Some seasoned teams can execute on features with little or no planning effectively. Even if you're in that camp, you will benefit from talking through the work in a low level of detail before you begin coding. Your design will improve, if nothing else. For most teams, it's simply critical to the cause of getting it right the first time.
---
title: Scrum in a Nutshell
tags: agile scrum reading-list
---

Scrum is a fundamentally adaptable process. One of the core pieces is the retrospective, where the team can collaboratively change the process itself. Team members learn from their mistakes and from each other about how to get better at delivering software. But before the iterative improvement can begin, there is a necessary bootstrapping. When introducing Scrum to a team for the first time, you can try to explain all of the nuances and edge cases up front. Or, like teaching people a new card game, sometimes it's better to just jump in and play a few hands.

If you're jumping in to Scrum, what are the bare basics you need to know? At its core, Scrum is about roles (Product Owner, Team Members and a Scrum Master), regular collaboration meetings (Sprint Planning, Daily Stand Up, Demo and Retrospective) and the organizational artifacts (the Backlog, User Stories and the Sprint itself).


## Scrum Artifacts

What you are delivering is the same thing that it always has been; working software. These artifacts are just ideas and tools that the team uses for their own aid.

- __The Backlog__ is an *absolutely ordered* list of all the features that the team has ever considered delivering.
- __User Stories__ are individual features that could be delivered, defined at variable levels of clarity and granularity, from small already broken down and well specified stories at the top of the list (the next to be worked on), to large amorphous stories at the bottom that won't get worked on for a long time, if ever.
- __The Sprint__ is a set interval (typically two weeks) during which the team commits to *completely finish* a small set of the user stories.

## Scrum Roles

Likewise, the Scrum roles are just more well defined descriptions of people that are probably already on your team.

- The __Product Owner__ sets the order of the backlog according to the business value of each story. This person is the designated representative of both the company at large and the customer.
- __Team Members__ are your existing engineers, designers, QA, etc. Usually more than 3 and less than 10 people comprise the team. Ideal scrum teams are composed of generalists who are also technically deep in disparate specialties.
- The __Scrum Master__ is just a team member that the team itself elects to off-load any of the process drudgery onto. That team member needs to schedule the Scrum team meetings, do any prep needed for the meetings, and keep the team on track and following their own rules during the meetings. Critically, they also are tasked with removing obstacles to success, which most often means getting the team resources they need, and personally owning ad hoc work that would otherwise interrupt a team member (mostly bugs).

## Scrum Meetings

No one likes meetings, which is why Scrum promotes the bare minimum of meetings needed to consistently and reliably produce working software on schedule, get feedback from the business and continually improve themselves. A scrum team can plan on spending up to 10% of their time in one of these meetings.

- __Sprint Planning__ is where the team breaks down the highest priority user stories into tasks and estimates how much work they are going to be. This is also where the team commits to which stories they are confident they can finish in the next Sprint. Ideally, features are defined in detail and mocked up going into this meeting.
- The __Daily Stand Up__ occurs at a set time every day, and consists of each team member briefly answering the questions: What did you do yesterday, what are you planning to do today, and are there any obstacles in your way? This should take less than 15 minutes, and usually occurs standing up to promote brevity.
- The __Demo__ is where the team demonstrates the functionality they have produced at the end of the sprint. The audience should be the product owner, plus any interested stakeholders. Ideally everyone on the team gets a chance to regularly demo what they have done.
- The __Retrospective__ is the team's chance to make better mistakes tomorrow. Team members talk about what went well, what didn't go well, and what they can do differently going forward. The team commits to an small number of achievable improvements and assigns owners to them.

## Learning More

That's all the context you need to get started, assuming at least one person on the team has done Scrum before. In any case, you may want to do some more learning on your own. To that end, I would recommend [Essential Scrum](http://www.amazon.com/Essential-Scrum-Practical-Addison-Wesley-Signature/dp/0137043295) or the classic [Agile Software Development with Scrum](http://www.amazon.com/books/dp/0130676349).

For visual learners, I would recommend the three part video introduction by Collabnet. Parts: [1](http://www.youtube.com/watch?v=D8vT7G0WATM), [2](http://www.youtube.com/watch?v=b_WeHcZcx1w) and [3](http://www.youtube.com/watch?v=wPvG9NZNUa4).
---
title: Diagnosing Memory "Leaks" in Python
tags: python
---

## The Problem

We wrote some new code in the form of [celery](https://github.com/celery/celery) tasks that we expected to run for up to five minutes, and use a few hundred megabytes of memory. Rinse and repeat for a thousand different data sets. We ran through a few data sets successfully, but once we started running though ALL of them, we noticed that the memory of the celery process was continuing to grow.

In celery, each task runs in one of a fixed number of processes that persist between tasks. We assumed we had a memory leak on our hands; somehow we were leaving references around to our data structures that were remaining in memory and not being garbage collected between tasks. But how do you go about investigating exactly what is happening?

**Note:** Stop everything, and make sure that you're not in `DEBUG` mode, assuming you're using Django. In that mode, every database query you make will be stored in memory, which [looks a lot like a memory leak](https://docs.djangoproject.com/en/dev/faq/models/#why-is-django-leaking-memory).

## Linux Utilities

The command line utilities [top](http://linux.about.com/od/commands/l/blcmdl1_top.htm) or the more pleasing [htop](http://htop.sourceforge.net/) should be your first stop for any CPU or memory load investigation. In our case, we had observed that the machine would run out of memory and start paging while running our tasks. So we kicked them off again, and watched the processes in htop. Indeed, the processes grew from their initial size of 100MB, slowly, all the way up to 1GB before we killed them. We could see from the logs that any individual tasks were being completed successfully along the way.

We were able to reproduce the behavior in our development environment, though we only had enough data for the process to balloon to a few hundred megabytes. Once we had the behavior reproducible in a script that could be run on it's own outside of celery (using `CELERY_ALWAYS_EAGER`), we could using the GNU `time` command to track peak memory usage, ie `/usr/bin/time -v myscript.py`.

**Note:** we're specifying the full path to time so that we get the GNU time command, and not the one built into bash.

**Note:** there is a bug in some versions of the utility that [mis-reports memory usage](https://bugzilla.redhat.com/show_bug.cgi?id=702826) by multiplying it by a factor of four. Double-check using top.

## Resource Module

You can actually get the amount of memory your process is using from inside your Python process, using the resource module.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">resource</span>
<span class="k">print</span> <span class="s">'Memory usage: %s (kb)'</span> <span class="o">%</span> <span class="n">resource</span><span class="p">.</span><span class="n">getrusage</span><span class="p">(</span><span class="n">resource</span><span class="p">.</span><span class="n">RUSAGE_SELF</span><span class="p">).</span><span class="n">ru_maxrss</span></code></pre></figure>

This can be useful for adding logging statements to your code to measure memory usage over time, or at critical junctures of a long-running process. This can help you isolate the critical section of your code that's causing the memory issue.

## Objgraph

Once you have identified a spot in your code just after the memory issue has occurred, you can query for the objects currently in memory right from Python, as well. You will probably need to do a `pip install objgraph` first.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">gc</span>
<span class="n">gc</span><span class="p">.</span><span class="n">collect</span><span class="p">()</span>  <span class="c1"># don't care about stuff that would be garbage collected properly
</span><span class="kn">import</span> <span class="nn">objgraph</span>
<span class="n">objgraph</span><span class="p">.</span><span class="n">show_most_common_types</span><span class="p">()</span>
<span class="nb">tuple</span>                      <span class="mi">5224</span>
<span class="n">function</span>                   <span class="mi">1329</span>
<span class="n">wrapper_descriptor</span>         <span class="mi">967</span>
<span class="nb">dict</span>                       <span class="mi">790</span>
<span class="n">builtin_function_or_method</span> <span class="mi">658</span>
<span class="n">method_descriptor</span>          <span class="mi">340</span>
<span class="n">weakref</span>                    <span class="mi">322</span>
<span class="nb">list</span>                       <span class="mi">168</span>
<span class="n">member_descriptor</span>          <span class="mi">167</span>
<span class="nb">type</span>                       <span class="mi">163</span></code></pre></figure>

## Heapy

Maybe you'll get lucky and see a custom class that you've defined at the top of the list. But if not, what exactly is in those generic type buckets? Enter [guppy](http://guppy-pe.sourceforge.net/), which is like `show_most_common_types` on steroids. Again, you will likely need to install this via `pip install guppy`. The great thing about guppy/heapy is that you can take a snapshot of the heap before your critical section and after, and diff them, just getting the objects that were added to the heap in between.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">guppy</span> <span class="kn">import</span> <span class="n">hpy</span>
<span class="n">hp</span> <span class="o">=</span> <span class="n">hpy</span><span class="p">()</span>
<span class="n">before</span> <span class="o">=</span> <span class="n">hp</span><span class="p">.</span><span class="n">heap</span><span class="p">()</span>

<span class="c1"># critical section here
</span>
<span class="n">after</span> <span class="o">=</span> <span class="n">hp</span><span class="p">.</span><span class="n">heap</span><span class="p">()</span>
<span class="n">leftover</span> <span class="o">=</span> <span class="n">after</span> <span class="o">-</span> <span class="n">before</span>
<span class="kn">import</span> <span class="nn">pdb</span><span class="p">;</span> <span class="n">pdb</span><span class="p">.</span><span class="n">set_trace</span><span class="p">()</span></code></pre></figure>

You probably want a [pdb](http://docs.python.org/2/library/pdb.html) session here, so you can interactively investigate the heap diff. The best heapy tutorial I have found is [How to use guppy/heapy for tracking down memory usage](http://www.smira.ru/wp-content/uploads/2011/08/heapy.html).

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;</span><span class="n">leftover</span>
<span class="n">Partition</span> <span class="n">of</span> <span class="n">a</span> <span class="nb">set</span> <span class="n">of</span> <span class="mi">134243</span> <span class="n">objects</span><span class="p">.</span> <span class="n">Total</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">65671752</span> <span class="nb">bytes</span><span class="p">.</span>
 <span class="n">Index</span>  <span class="n">Count</span>   <span class="o">%</span>     <span class="n">Size</span>   <span class="o">%</span> <span class="n">Cumulative</span>  <span class="o">%</span> <span class="n">Kind</span> <span class="p">(</span><span class="k">class</span> <span class="err">/ </span><span class="nc">dict</span> <span class="n">of</span> <span class="n">class</span><span class="p">)</span>
     <span class="mi">0</span>  <span class="mi">16081</span>  <span class="mi">12</span> <span class="mi">45332744</span>  <span class="mi">69</span>  <span class="mi">45332744</span>  <span class="mi">69</span> <span class="nb">unicode</span>
     <span class="mi">1</span>  <span class="mi">18714</span>  <span class="mi">14</span>  <span class="mi">5493360</span>   <span class="mi">8</span>  <span class="mi">50826104</span>  <span class="mi">77</span> <span class="nb">dict</span> <span class="p">(</span><span class="n">no</span> <span class="n">owner</span><span class="p">)</span>
     <span class="mi">2</span>  <span class="mi">47441</span>  <span class="mi">35</span>  <span class="mi">3925672</span>   <span class="mi">6</span>  <span class="mi">54751776</span>  <span class="mi">83</span> <span class="nb">str</span>
     <span class="mi">3</span>  <span class="mi">21300</span>  <span class="mi">16</span>  <span class="mi">1786080</span>   <span class="mi">3</span>  <span class="mi">56537856</span>  <span class="mi">86</span> <span class="nb">tuple</span>
     <span class="mi">4</span>    <span class="mi">344</span>   <span class="mi">0</span>   <span class="mi">820544</span>   <span class="mi">1</span>  <span class="mi">57358400</span>  <span class="mi">87</span> <span class="nb">dict</span> <span class="n">of</span> <span class="n">module</span>
     <span class="mi">5</span>    <span class="mi">654</span>   <span class="mi">0</span>   <span class="mi">685392</span>   <span class="mi">1</span>  <span class="mi">58043792</span>  <span class="mi">88</span> <span class="nb">dict</span> <span class="n">of</span> <span class="n">django</span><span class="p">.</span><span class="n">db</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">related</span><span class="p">.</span><span class="n">RelatedObject</span>
     <span class="mi">6</span>   <span class="mi">5543</span>   <span class="mi">4</span>   <span class="mi">665160</span>   <span class="mi">1</span>  <span class="mi">58708952</span>  <span class="mi">89</span> <span class="n">function</span>
     <span class="mi">7</span>    <span class="mi">708</span>   <span class="mi">1</span>   <span class="mi">640992</span>   <span class="mi">1</span>  <span class="mi">59349944</span>  <span class="mi">90</span> <span class="nb">type</span>
     <span class="mi">8</span>   <span class="mi">4946</span>   <span class="mi">4</span>   <span class="mi">633088</span>   <span class="mi">1</span>  <span class="mi">59983032</span>  <span class="mi">91</span> <span class="n">types</span><span class="p">.</span><span class="n">CodeType</span>
     <span class="mi">9</span>    <span class="mi">705</span>   <span class="mi">1</span>   <span class="mi">442776</span>   <span class="mi">1</span>  <span class="mi">60425808</span>  <span class="mi">92</span> <span class="nb">dict</span> <span class="n">of</span> <span class="nb">type</span>

<span class="o">&gt;</span><span class="n">leftover</span><span class="p">.</span><span class="n">byrcs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">byid</span>
<span class="n">Set</span> <span class="n">of</span> <span class="mi">16081</span> <span class="o">&lt;</span><span class="nb">unicode</span><span class="o">&gt;</span> <span class="n">objects</span><span class="p">.</span> <span class="n">Total</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">45332744</span> <span class="nb">bytes</span><span class="p">.</span>
 <span class="n">Index</span>     <span class="n">Size</span>   <span class="o">%</span>   <span class="n">Cumulative</span>  <span class="o">%</span>   <span class="n">Representation</span> <span class="p">(</span><span class="n">limited</span><span class="p">)</span>
     <span class="mi">0</span>       <span class="mi">80</span>   <span class="mf">0.0</span>        <span class="mi">80</span>   <span class="mf">0.0</span> <span class="s">'media-plugin...re20051219-r1'</span>
     <span class="mi">1</span>       <span class="mi">76</span>   <span class="mf">0.0</span>       <span class="mi">156</span>   <span class="mf">0.0</span> <span class="s">'app-emulatio...4.20041102-r1'</span>
     <span class="mi">2</span>       <span class="mi">76</span>   <span class="mf">0.0</span>       <span class="mi">232</span>   <span class="mf">0.0</span> <span class="s">'dev-php5/ezc...hemaTiein-1.0'</span>
     <span class="mi">3</span>       <span class="mi">76</span>   <span class="mf">0.0</span>       <span class="mi">308</span>   <span class="mf">0.0</span> <span class="s">'games-misc/f...wski-20030120'</span>
     <span class="mi">4</span>       <span class="mi">76</span>   <span class="mf">0.0</span>       <span class="mi">384</span>   <span class="mf">0.0</span> <span class="s">'mail-client/...pt-viewer-0.8'</span>
     <span class="mi">5</span>       <span class="mi">76</span>   <span class="mf">0.0</span>       <span class="mi">460</span>   <span class="mf">0.0</span> <span class="s">'media-fonts/...-100dpi-1.0.0'</span>
     <span class="mi">6</span>       <span class="mi">76</span>   <span class="mf">0.0</span>       <span class="mi">536</span>   <span class="mf">0.0</span> <span class="s">'media-plugin...gdemux-0.10.4'</span>
     <span class="mi">7</span>       <span class="mi">76</span>   <span class="mf">0.0</span>       <span class="mi">612</span>   <span class="mf">0.0</span> <span class="s">'media-plugin...3_pre20051219'</span>
     <span class="mi">8</span>       <span class="mi">76</span>   <span class="mf">0.0</span>       <span class="mi">688</span>   <span class="mf">0.0</span> <span class="s">'media-plugin...3_pre20051219'</span>
     <span class="mi">9</span>       <span class="mi">76</span>   <span class="mf">0.0</span>       <span class="mi">764</span>   <span class="mf">0.0</span> <span class="s">'media-plugin...3_pre20060502</span></code></pre></figure>

**Note:** memory dumps have been fabricated to protect the innocent.

## GDB

An interesting thing happened when we were using heapy. We noticed that heapy was only reporting 128MB of objects in memory, where as the resource module and top agreed that there was almost 1GB being used.

To get an idea of what was comprising the remaining 800+ MBs, we turned to gdb, specifically to a python helper called [gdb-heap](https://github.com/rogerhu/gdb-heap).

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">libc6</span><span class="o">-</span><span class="n">dev</span>
<span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">libc6</span><span class="o">-</span><span class="n">dbg</span>
<span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">python</span><span class="o">-</span><span class="n">gi</span>
<span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">libglib2</span><span class="p">.</span><span class="mi">0</span><span class="o">-</span><span class="n">dev</span>
<span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">python</span><span class="o">-</span><span class="n">ply</span>

<span class="c1"># assuming 7458 is the PID of your memory hogging python process
</span><span class="n">sudo</span> <span class="n">gdb</span> <span class="o">-</span><span class="n">p</span> <span class="mi">7458</span>
<span class="o">&gt;</span><span class="n">generate</span><span class="o">-</span><span class="n">core</span><span class="o">-</span><span class="nb">file</span>

<span class="c1"># this will save a .core file, which you can then examine in gdb
</span><span class="n">sudo</span> <span class="n">gdb</span> <span class="n">python</span> <span class="n">myfile</span><span class="p">.</span><span class="n">core</span> <span class="o">-</span><span class="n">x</span> <span class="o">~/</span><span class="n">gdb</span><span class="o">-</span><span class="n">heap</span><span class="o">-</span><span class="n">commands</span></code></pre></figure>

In our case, what we saw was mostly indecipherable. But there seemed to be a ton of tiny little objects around, like integers.

## Explanation

Long running Python jobs that consume a lot of memory while running may not return that memory to the operating system until the process actually terminates, even if everything is garbage collected properly. That was news to me, but it's true. What this means is that processes that do need to use a lot of memory will exhibit a "high water" behavior, where they remain forever at the level of memory usage that they required at their peak.

**Note:** this behavior may be Linux specific; there are anecdotal reports that Python on Windows does not have this problem.

This problem arises from the fact that the Python VM does its own internal memory management. It's commonly know as [memory fragmentation](http://revista.python.org.ar/2/en/html/memory-fragmentation.html). Unfortunately, there doesn't seem to be any fool-proof method of avoiding it.

Celery tends to bring out this behavior for a lot of users.

> AFAIK this is just how Python works. I would guess that the operating system will reuse the memory anyway, since it can just swap it out if it's not used.  If you have allocated a chunk of memory, there's a big chance that you will need it again, and it's better to delegate memory management to the operating system.
> ... There is no solution - that I know of - to make Python release the memory ... [Ask Solem, author of celery](https://groups.google.com/forum/#!topic/celery-users/jVc3I3kPtlw)


## Workarounds

For celery in particular, you can roll the celery worker processes regularly. This is exactly what the `CELERYD_MAX_TASKS_PER_CHILD` setting does. However, you may end up having to roll the workers so often that you incur an undesirable performance overhead.

For non-celery systems, you can use the `multiprocessing` module to run any function in a separate process. There is a simple looking gist called [processify](https://gist.github.com/schlamar/2311116) that does just that.

**Note:** This may have the undesirable effect of using more shared resources, like database connections.

You could also run your Python jobs using Jython, which uses the Java JVM and does not exhibit this behavior. Likewise, you could [upgrade to Python 3.3](http://bugs.python.org/issue11849),

Ultimately, the best solution is to simply use less memory. In our case, we ended up breaking the work into smaller chunks (individual days). For some tasks, this may not be possible, or may require complicated task coordination.
---
title: Switching from Ubuntu Unity to LXDE
tags: linux ubuntu lxde
---

Canonical was founded with the goal of bringing polish, consistency and _usability_ to the Linux desktop. Derided unfairly by some as not contributing much technically to the open source ecosystem, Ubuntu has been undeniably popular, becoming for the last five years the closest thing we have to a de facto standard distribution. I myself switched to using Linux full time coinciding with this phenomenon, drawn by the mixture of freedom, power and flexibility. For the first time in my experience, Linux mostly "just worked".

When Unity was announced a couple of years ago, I was more than willing to give Canonical the benefit of the doubt. Being a big believer in the value of good UX in the context of my own work projects, it seemed to me that professionals could surely improve upon the existing paradigm, which was mostly an accumulation of legacy skeuomorphs left over from the initial mouse and window scheme. I had already incorporated search into my normal work flow by using "Gnome Do", so a desktop designed around search seemed like a logical step.

When the first beta came out, the reaction from geek circles was largely negative. The bulk of users were simply reluctant to embrace change. Some of the feedback was legitimate constructive criticism, though. Making a single UI usable on touch screens and desktops leaves both sub-optimal. Search results were quickly turned into obnoxious and slow pseudo-advertisements. Multi-monitor support has was knocked back five years.

But usability sessions showed that Unity was in fact easier to use for novice users. First time users were able to more quickly figure out basic tasks. Feedback from these users was largely positive. So I told myself to give it a solid six months before coming to a verdict. Inertia turned that timetable into two years, but now finally I have had enough.

Unity is just too slow. Bringing up the Unity dash has noticeable lag. Even switching between windows with ALT-TAB doesn't feel smooth. I feel like I can see the window elements being drawn. I'm sure it's something to do with my graphics card, but I don't care. The lag has persisted over five successive releases, and several totally different pieces of hardware. My guess is that it's a combination of having three monitors (and thus having many more pixels to push than your average user), refusing to use the proprietary drivers (which have their own usability issues) and my own neurosis. But it's simply bad design to require good 3D video performance for basic UI interactions on a platform that has always been synonymous with poor graphics drivers.

Windowing in LXDE on the other hand __has__ "just worked". The very first time I booted into it, it picked up my three monitors correctly, with __no__ configuration. ALT-TAB is blazing fast, if aesthetically work-a-day. "Gnome Do" is all I need to launch apps. The status bar is easily configurable and familiar.

Not everything is rosy. The `ssh-agent` integration is manual. It was a PITA to get my sound set up with a bluetooth headset. The lock screen is decidedly fugly. It seems that the power users were right, in a way. Those niceties have not made it back into the larger Linux desktop ecosystem. But at least I don't feel like I'm operating my desktop over a 200ms VNC connection anymore.

Fuck you Canonical. You showed us how awesome a smooth, integrated Linux desktop could be. Then you went ahead and messed it up. Now the power user is back to having to fiddle again just to get a good experience. At least for me, you have managed to burn the goodwill you built in those first five years. I hope this mobile play is worth it.
---
title: Writing a cURL to Python Install Script for a Django development environment
tags: python
---

### Paste and Pray

It's pretty common to Google a technical issue, and come up with a blog or a Stackoverflow article that promises to fix the issue by copying and pasting a simple one-liner into a terminal. At first it's just a quick `chmod`, or a simple `apt-get`. It's completely unsafe unless you know exactly what the command is doing, but it's pretty damn effective. Then you graduate to doing few lines at a time without really thinking about it too much, like a bad drug habit. Repetition slowly convinces you that it's not insane.

In the last couple of years, I've seen a few extreme versions of this same idea, but taken to the next level. Homebrew can be installed by running `ruby -e "$(curl -fsSL https://raw.github.com/mxcl/homebrew/go)"`. Heroku Toolbelt for Linux is just a `wget -qO- https://toolbelt.heroku.com/install-ubuntu.sh | sh` away. I call these __Paste and Pray__ installers.

Sounds like fun, right? I thought so, so I came up with a version that installs a pretty vanilla Django development environment, _from scratch_. It doesn't assume anything except cURL and Python, both of which come pre-installed on OSX and Ubuntu. It does a fully automated install of Homebrew (including its dependency XCode on OSX), as well as pip, virtualenv, your code from GitHub, Django and any other Python requirements you have. It also sets some environment variables and updates your hosts file.

Oh, did I mention that some of these steps require root access? That's right! It's more like __Super Paste and Pray__&trade;.

_Note:_ this code is not intended to be directly re-usable. Instead, I thought I would share what I learned while writing it.

### The Code

You can view the [code and documentation](https://github.com/chase-seibert/paste-and-pray). The code itself can be run with `curl -fsS https://raw.github.com/chase-seibert/paste-and-pray/go/run.py |sudo -E python`. You should be able to modify it pretty easily for any Django app.

### Sh Stuff

Although I didn't want any dependencies required for the installer itself, I did want to take this chance to play around with [sh](https://github.com/amoffat/sh), a nice Python API for interfacing with the shell. Because I could not rely on pip to be installed, I just downloaded this dependency right in Python:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">urllib</span><span class="p">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="s">"https://raw.github.com/amoffat/sh/master/sh.py"</span><span class="p">,</span> <span class="s">"sh.py"</span><span class="p">)</span>
<span class="n">sh</span> <span class="o">=</span> <span class="n">imp</span><span class="p">.</span><span class="n">load_source</span><span class="p">(</span><span class="s">"sh"</span><span class="p">,</span> <span class="s">"sh.py"</span><span class="p">)</span></code></pre></figure>

Working with `sh` turned out to be a little trickier than I imagined. First off, many of my commands required root access. The easiest method was to start the script with `sudo`, and wrap any steps that did __not__ require root in a `sudo -E -u username` prefix. This is exactly what [sh.bake](http://amoffat.github.io/sh/#baking) is for.

I found it useful to redirect `_out` to `sys.stdout` for trouble-shooting purposes. Similarly, if you need to take user input, you will need to redirect stdin with something like:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">raw_input_tty</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">prompt</span><span class="p">):</span>
    <span class="s">''' the main use case for this script has the user piping in the results from
    a curl; which over-rides stdin. But we also want to interactively prompt the user
    for some input, so dynamcially switch back to tty. '''</span>
    <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">'/dev/tty'</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">raw_input</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span></code></pre></figure>

### Sudo Madness

Initially, I had tried calling [setuid](http://docs.python.org/2/library/os.html#os.setuid) to downgrade to a non-root user. But, I was not able to go back to root, meaning that you would need to do all the root steps in one chunk, then do all the non-root stuff. This was a show-stopper as the Homebrew install step must be run as a regular user, but subsequent `pip` and `virtualenv` tool installs required root.

Another wrinkle was getting the original username from inside sudo. This was simple, thought I did need to hunt around before I discovered that both OSX and Linux set a `SUDO_USER` environment variable for just this purpose.

### OSX Stuff

Installing Homebrew manually is straight-forward, but does require the XCode command line tools to be installed first. Normally, this is a headache as you have to create an Apple ID and hunt and peck trough their developer website for a binary installer.

It turns out there there __are__ direct download links, they just are not publicised. Here is some python code to download and install the correct version.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">install_xcode</span><span class="p">(</span><span class="n">osx_version</span><span class="p">):</span>
    <span class="c1"># see: https://devimages.apple.com.edgekey.net/downloads/xcode/simulators/index-3905972D-B609-49CE-8D06-51ADC78E07BC.dvtdownloadableindex
</span>    <span class="n">downloads</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mf">10.7</span><span class="p">:</span> <span class="s">"http://devimages.apple.com/downloads/xcode/command_line_tools_for_xcode_os_x_lion_april_2013.dmg"</span><span class="p">,</span>
        <span class="mf">10.8</span><span class="p">:</span> <span class="s">"http://devimages.apple.com/downloads/xcode/command_line_tools_for_xcode_os_x_mountain_lion_april_2013.dmg"</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">osx_version</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">downloads</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">(</span><span class="s">"Could not locate XCode download for OSX %s"</span> <span class="o">%</span> <span class="n">osx_version</span><span class="p">)</span>
    <span class="n">download_file</span> <span class="o">=</span> <span class="n">downloads</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">osx_version</span><span class="p">)</span>
    <span class="c1"># save this OUTSIDE the normal tmp dir; in case we need to restart install
</span>    <span class="n">dmg_file</span> <span class="o">=</span> <span class="s">"/tmp/xcode.dmg"</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">dmg_file</span><span class="p">):</span>
        <span class="n">urllib</span><span class="p">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">download_file</span><span class="p">,</span> <span class="n">dmg_file</span><span class="p">)</span>
    <span class="n">volume_dir</span> <span class="o">=</span> <span class="s">"/tmp/xcode"</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">volume_dir</span><span class="p">):</span>
        <span class="n">sh</span><span class="p">.</span><span class="n">hdiutil</span><span class="p">(</span><span class="s">"attach"</span><span class="p">,</span> <span class="s">"-mountpoint"</span><span class="p">,</span> <span class="n">volume_dir</span><span class="p">,</span> <span class="n">dmg_file</span><span class="p">)</span>
    <span class="n">mpkg_file</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">volume_dir</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">".mpkg"</span><span class="p">)][</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">sh</span><span class="p">.</span><span class="n">installer</span><span class="p">(</span><span class="s">"-pkg"</span><span class="p">,</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">volume_dir</span><span class="p">,</span> <span class="n">mpkg_file</span><span class="p">),</span> <span class="s">"-target"</span><span class="p">,</span> <span class="s">"/"</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">sh</span><span class="p">.</span><span class="n">ErrorReturnCode</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span> <span class="n">e</span><span class="p">.</span><span class="n">stderr</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">sh</span><span class="p">.</span><span class="n">hdiutil</span><span class="p">(</span><span class="s">"detach"</span><span class="p">,</span> <span class="n">volume_dir</span><span class="p">)</span></code></pre></figure>

As I said, the Homebrew install is easy. I did have to include a small [fixup gist](https://gist.github.com/rpavlik/768518/raw/fix_homebrew.rb) to get it to work on one of my test machines.

### Python Stuff

Working with Python made a lot of the script easier. Specifically, working with `sh` made tasks like getting the list of items currently installed by brew pretty simple:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">installed</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">sh</span><span class="p">.</span><span class="n">brew</span><span class="p">(</span><span class="s">"list"</span><span class="p">,</span> <span class="s">"-1"</span><span class="p">).</span><span class="n">split</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="n">not_installed</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dependencies</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">installed</span><span class="p">))</span></code></pre></figure>

It also allows chaining, just like in a bash shell:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">sh</span><span class="p">.</span><span class="n">grep</span><span class="p">(</span><span class="n">sh</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="s">"/etc/hosts"</span><span class="p">),</span> <span class="n">HOST_NAME</span><span class="p">)</span></code></pre></figure>

One area that was a little tricky was activating virtualenv from inside Python. However, there is actually a [supported method](http://www.virtualenv.org/en/latest/#using-virtualenv-without-bin-python) for this.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">activate_this</span> <span class="o">=</span> <span class="s">'/path/to/env/bin/activate_this.py'</span>
<span class="nb">execfile</span><span class="p">(</span><span class="n">activate_this</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="n">__file__</span><span class="o">=</span><span class="n">activate_this</span><span class="p">))</span></code></pre></figure>

### GitHub Stuff

For the shortest URL possible, you can set the [default branch](https://help.github.com/articles/setting-the-default-branch) in GitHub to be something short, like "go". Looking at the homebrew installer, I noticed that it's possible to leave the file name off, too. GitHub's raw file service apparently picks the first file by alphabetical order if you do this. However, I didn't want to mess with trying to get my code to show up alphabetically before `.gitignore`, so I skipped this optimization.

### Final Thoughts

Random other thoughts on writing installers:

- Create a temp directory if you need to do anything like download files.
- Make the installer idempotent; you want to be able to retry cleanly.
- Checking whether a step has already been run is a good idea.
- Don't try to hide the debug output; at least for a development audience.
---
title: Getting started with Splunk as an Engineer
tags: splunk
---

What is [Splunk](http://www.splunk.com/), and how can you make the best use of it as an engineer? Splunk is first and foremost a hosted web-based tool for your log files. It gives you the following:

- Aggregates all your logs in one place
- Search
- Extract meaning
- Group, join, pivot and format results
- Visualization
- Email alerts


### Architecture

How does it work? You setup all your servers to forward certain log statements to a centralized cluster of Splunk servers via rsyslog. Alternatively, you can use SFTP, NFS, etc. I don't want to re-hash [install steps](http://chase-seibert.github.io/blog/2010/12/31/splunkrsyslogapacheubuntu-quickstart.html) here. Instead, let's focus on what you can **DO** with it once all your logs are in there.


### Hosts and Sources

The first thing you will notice is that your log files are broken down by host, and optionally by source. This means that you can quickly search all the logs from one machine, or from a logical group of machines (ex: web servers versus database servers).

![splunk sources](/blog/images/splunk2.png)


### Searching

Searching is pretty easy to use, just type in a search string! You can also do more complicated stuff. Examples:

- `sourcetype="hsl-prod-fe" "Chase Seibert"`
- `sourcetype="hsl-prod-fe" e186f85c914261eec9e54d3767fdd3cc BEGIN`
- `sourcetype="hsl-prod-crawl" |regex _raw="fanmgmt\.(analytics|metrics)"`
- `sourcetype="hsl-prod-crawl" facebook OR twitter NOT linkedin`
- `sourcetype="hsl-prod-crawl" facebook OR linkedin OR twitter earliest=-24h`

![splunk sources](/blog/images/splunk3.png)


### Extracting

This is where is starts to get interesting. You can use a [built-in GUI](http://docs.splunk.com/Documentation/Splunk/5.0.4/Knowledge/Addfieldsatsearchtime) to define a custom regex to pull pieces of your lines into variables, which you can then filter, group and aggregate by.

If you can change your log format, you can get this to happen automatically without defining a regex by putting key/value pairs into `key=value` format. Here is some example code:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">key_value</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">log_level</span><span class="o">=</span><span class="s">'info'</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">log_message</span> <span class="o">=</span> <span class="s">'%s: %s'</span> <span class="o">%</span> <span class="p">(</span>
        <span class="n">prefix</span><span class="p">,</span> <span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">([</span><span class="s">'%s="%s"'</span> <span class="o">%</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">items</span><span class="p">()]))</span>
    <span class="nb">getattr</span><span class="p">(</span><span class="n">logging</span><span class="p">,</span> <span class="n">log_level</span><span class="p">)(</span><span class="n">log_message</span><span class="p">.</span><span class="n">strip</span><span class="p">())</span></code></pre></figure>


### Piping

Just like in Unix, you can pipe various commands together to produce more complicated behavior in your searches.  Example: `sourcetype="hsl-prod-crawl" succeeded |stats count perc95(task_seconds) by python_module |sort count desc |head 10`.

Other basic [pipe commands](http://docs.splunk.com/Documentation/Splunk/5.0.4/SearchReference/SearchCheatsheet):

- `| uniq`
- `| script python myscript myarg1 myarg2`
- `| bucket _time span=5m`
- `| eval name=coalesce(firstName, lastName)`
- `| rare, | anomalous`
- `| spath output=commit_author path=commits.author.name  # extract xml/json values`

Other [functions](http://docs.splunk.com/Documentation/Splunk/5.0.4/SearchReference/SearchCheatsheet) you can call:

- `avg()`
- `| eval description=case(error == 404, "Not found", error == 200, "OK")`
- `floor(), ceiling()`
- `len()`
- `isbool(), isint(), etc`
- `upper(), lower()`
- `trim(), ltrim(), rtrim()`
- `md5()`
- `now()`
- `random()`
- `replace()`
- `split()`
- `substr()`


### Visualizations

It's easy to create graphs and charts by pointing and clicking your way through their GUI builder. You can also do the same thing with raw commands and functions, assuming you know what to call. You can create pie charts, bar, columns and line graphs, as well as more complicated stuff like gauges.

![splunk dashboard](/blog/images/splunk1.png)


### Email Alerts

Finally, you can configure Splunk to send you email alerts based on a cron schedule when certain searches either produce or do not produce certain results. Alerts:

- Start with a regular search
- Take a time window
- Can be scheduled (supports cron syntax)
- Can alert always, or when num rows > N
- Can remember not to re-alert on the same items
- Sends an email, or publishes to an RSS feed


### Next steps

- [Official Documentation](http://docs.splunk.com/Documentation/Splunk)
- [Help Forums](http://answers.splunk.com/)
- [Getting Started Videos](http://www.splunk.com/view/education-videos)
---
title: Using grep from inside vim
tags: vim
---

> This is my rifle. There are many like it, but this one is mine.  - [Rifleman's Creed](http://en.wikipedia.org/wiki/Rifleman's_Creed)

There are a thousand ways to grep over files. Most developers I have observed keep a separate command line open just for searching. A few use an IDE that has file search built-in. Personally, I use a couple of vim macros.

In vim, you can execute a cross-file search with something like: `:vimgrep /dostuff()/j ../**/*.c`. I don't know about you, but the first time I saw that syntax my brain simply refused.

Instead, I have the following in my .vimrc file:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="s">" opens search results in a window w/ links and highlight the matches
command! -nargs=+ Grep execute 'silent grep! -I -r -n --exclude *.{json,pyc} . -e &lt;args&gt;' | copen | execute 'silent /&lt;args&gt;'
"</span> <span class="n">shift</span><span class="o">-</span><span class="n">control</span><span class="o">-*</span> <span class="n">Greps</span> <span class="k">for</span> <span class="n">the</span> <span class="n">word</span> <span class="n">under</span> <span class="n">the</span> <span class="n">cursor</span>
<span class="p">:</span><span class="n">nmap</span> <span class="o">&lt;</span><span class="n">leader</span><span class="o">&gt;</span><span class="n">g</span> <span class="p">:</span><span class="n">Grep</span> <span class="o">&lt;</span><span class="n">c</span><span class="o">-</span><span class="n">r</span><span class="o">&gt;=</span><span class="n">expand</span><span class="p">(</span><span class="s">"&lt;cword&gt;"</span><span class="p">)</span><span class="o">&lt;</span><span class="n">cr</span><span class="o">&gt;&lt;</span><span class="n">cr</span><span class="o">&gt;</span></code></pre></figure>

The first command is just a simple alias for the above mentioned native grep. Like all custom commands, it must start with a capital letter (to differentiate it from native commands). You simply type `:Grep foobar`, and it will search in your current directory through all file extensions (except json and pyc; you can add more to the blacklist).

It also displays the results in a nice little buffer window, which you can navigate through with normal HJKL keys, and open matches in the main editor window.

![vim Grep](/blog/images/vimgrep.png)

The second line is a key mapping that will grep for the word currently under the cursor. You can just navigate to a word and hit `Leader-g` to issue the Grep command.

---
title: Joining a software startup right out of school
tags: startups reading-list
---

## About me

I joined a tech startup ([Bullhorn, Inc](http://www.bullhorn.com)) in Boston right after graduating college in 2003. After nearly 10 years, they were acquired (w00t) and everyone got their stock options cashed out. Since then, I've started at another startup ([Hearsay Social](http://www.hearsaysocial.com)), this time in San Francisco.

When I started at Bullhorn, there were eight employees. That's if you don't count the two sales people that got fired on my first day. In the subsequent nine years I learned a ton, and had a great time working for an awesome company. I highly recommend a startup as your first job. Like corporate life in general, there is a ton of stuff you don't know when you're just graduating, but even more stuff that is startup specific. Here's the real deal.

## No one gets rich except the founders

By founders, I literally mean the two or three people who started the company from nothing. By rich, I mean "fuck you" money, which is a highly technical industry term meaning you could never work again if you wanted to. [Salaried employees do not get rich](http://www.tonywright.com/2008/a-newbies-guide-to-startup-compensation-or-stock-options-will-make-me-rich/), nor should they; the founders took all the risk.

First of all, [75% of startups fail](http://www.bizjournals.com/sanjose/blog/2012/09/most-startups-fail-says-harvard.html) to have a successful liquidity event where your stock is actually worth money. Of the ones that do succeed, the [average sale price](http://techcrunch.com/2012/01/03/crunchbaseexits/) is $200M for an acquisition, and $600M for an IPO. As an employee, you typically want to be acquired to maximize your payday. That is a ton of money, but your percentage is going to be very small. Even if you're a director, you might get .5%, or $1M. Engineers can expect to have a .1% stake, or $200k.

That's before you pay your strike price, and taxes. Oh, and this will put you in the highest marginal tax rate for that calendar year, so you're paying about 40% in taxes. All this can vary with how generous your options were, the final price of the sale and MANY other factors. For the average success story, you can expect to make somewhere between one and three times a decent yearly salary. Not chump change, but not life-altering, either.

To get to real "fuck you" money, the company would need to get sold for $1B or more. That is a vanishingly small percentage of successful startups. In fact, it's [about .03%](http://www.quora.com/What-is-the-truth-behind-9-out-of-10-startups-fail).

All of this assumes that all your options are vested, which typically takes four years, and that no squirely shenanigans happen during the sale. The options you have can be diluted along the way as the company takes on more investors. Investors usually have preferred stock options as well, which is another way you can be screwed. If you go IPO, there is usually a 180 day lockout period, during which if the stock tanks you a out of luck.

My opinion is that stock options are a highly complicated, incredibly risky investment. I think of them as lottery tickets for a lottery you're not even sure is going to happen. Maybe they will be worth something someday, but don't bank on it. Remember, this is coming from someone who had a successful cash out.

If you want to estimate the potential value of your options, which the founders are typically happy to [let you think are really valuable](http://www.tonywright.com/2008/a-newbies-guide-to-startup-compensation-or-stock-options-will-make-me-rich/), you MUST get the [cap table information](http://www.brucephenry.com/2009/05/startup-pay-and-equity.html). How many total shares are there, and what is the current valuation of the company? From there, you can plug in the values to [these formulas](http://www.danshapiro.com/blog/2010/11/how-much-are-startup-options-worth/).

## Why working at a startup is awesome

With that out of the way, I'm still recommending that you consider a startup as your first job. Why?

### Responsibility and Growth

At a startup, you will by necessity get a lot of responsibility put on your shoulders, even as a fresh faced 22 year old. In my first month, I got to write a mission critical email server that was processing a hundred thousand emails a day the next week. Six months later, I was racking up $200k in purchase orders for new hardware, driving them down to the datacenter in the trunk of my 1995 Saturn, and installing them in cages.

I got to personally scale a web site from two web servers and one database to 50 web servers and dozens of databases, serving 30M hits a day. I got invaluable hands-on experience with diverse topics like database optimization, caching, networking, telephony, search indexing, deployment tools and email deliverability. I wrote features by myself encompassing instant messaging, email, calendars, search, social networking, mobile, configurable reporting, processing credit cards and message queuing.

At a typical large software company, you can expect to be stuck on one team, working on one piece of one project for years at a time.

### Culture

There is nothing like working on a really small team of great people. We had drinks after work all the time, not because it was a company event, but because we actually wanted to hang out together. Every day, the hungriest person would walk around the office and collect lunch orders and cash, usually resulting in an armful of brown paper bags filled with cheeseburgers.

Some of the people who took those lunch orders started at the company by taking customer support calls, and eventually ended up as directors in a 300 person organization, starting whole new departments more than once. There is a tremendous opportunity when you get in at the ground floor of a startup to take your career in whatever direction you want.

The perks are usually pretty good, especially in the Bay Area, where hiring is so competitive that companies can't afford to NOT offer free lunches/dinners, endless diet coke and yoga once a week. Healthcare and 401k are absolutely standard.

In general, you end up with a young crowd of smart people, who love to work and play hard. There is no better networking you could be doing than to become a trusted member of a team like that.

### Zero Bullshit

At a small company, especially as an early employee, you have complete freedom. You will feel totally comfortable walking into the CEO's office and trash talking them for something stupid that they did. There is no process to get in the way of getting stuff done. At first there is no process at all. You may very well be deploying code by literally hitting Control-S and saving the file you're currently editing to the production machine (nothing delights a customer on the phone like telling them you JUST fixed their bug).

It's absolutely fascinating to watch a small company grown. Every time you double in size, there will be new challanges and new opportunities. The process you were using will no longer work, and suddenly people will be asking YOU what the new process should be. After all, you're a trusted veteran employee.

## It's not all roses

One thing that you will learn very quickly is that you can and will be asked to do absolutely everything at some point. In my time at Bullhorn (with the title of Software Engineer), at various times I had to answer the phones, take out the garbage, clean the kitchen, install new desktop PCs and phones, setup a wireless router in someone's house, baby sit a dog, feed a meter, cater lunch and fetch coffee. Don't EVER say "That's not my job".

**Protip**: I once told a customer on the phone that we only had one database and no backups. I was never asked to answer the phones again.

You will definitely have to get up in the middle of the night because the website is down. People you like will be fired, though typically turn-over for engineers should not be high. The company may pivot, and all of a sudden you find yourself working on a totally different product. You may never really know if the company is succeeding, or if everyone is just putting on a happy face. Typically you won't have luxuries like graphic designers and QA people to make your work look good (but you should push for them ASAP).

Finally, the code quality at startups is notoriously low. Personally, I think code quality is pretty low industry wide, but at a startup there is even more pressure to ship fast. Indeed, startups that don't sacrifice quality for speed may not be in business very long. That's often what it takes to survive the early years.

The culture generally gets worse as you grow. I think this is the inevitable result of just having more people. Over time, you will notice that company events no longer serve hard alcohol. Everyone will be required to attend sexual harassment training. All kinds of new security policies will pop up. Most importantly, all of a sudden you will not feel like you know all the people who work there. Eventually, you might not even know all their names.

## Myths


### Startups pay less than other companies

Certainly if you start before the company gets any funding, they will not be paying you. But unless you're a founder, I'm not sure why you would do that. No one except the founders gets rich.

After the first round of funding, companies can afford to pay you a market salary. My advice is to not take an offer where a lot of the compensation is in stock options. At least, you should [know what they're worth](http://www.danshapiro.com/blog/2010/11/how-much-are-startup-options-worth).

More importantly, know what **you** are worth. The average salary for an entry level software engineer in the Bay Area is [$77k](http://www.indeed.com/salary/q-Entry-Level-Software-Engineer-l-Silicon-Valley,-CA.html)  according to Indeed.com. As a hiring manager, that is in line with what I see. I don't see any reason why you should take a substantial discount to work at a startup.


### Startups are "Fast Paced"

So they are, but so are most companies. This is just code for "we change our minds all the damn time". The good news is that at a startup, you can be the one initiating change.


### Everyone works 60 hour weeks

The last thing my predecessor told me at Bullhorn was "don't start out working tons of hours; you'll never stop". I took his advice and worked on average just over 40 hours a week. There were tons of nights and weekends in the beginning, but it never felt like a burden when I gave myself permission to kick off at 5pm on a weekday.

Founders WILL work those crazy hours. They should; they have much more to gain in the bargain.


### Every employee has control over the success of the company

This is a platitude. It's true that you have an effect over the success of the company, but the company can and will totally tank no matter how hard you're working. There are just too many things you don't control. Sales, marketing and company leadership from the top typically have more impact on final success or failure than the engineers in my experience.


### A company is built on a great idea

No way. Execution trumps the idea any day. Successful companies need both, but great ideas are a dime a dozen.


### You can own the side project you're working on

Double-check the laws in your state before you assume this is true. In [California, you're ok](http://answers.onstartups.com/questions/19422/if-im-working-at-a-company-do-they-have-intellectual-property-rights-to-the-st).


### Unlimited vacation time is an awesome perk

It's [very popular](http://answers.onstartups.com/questions/40954/does-the-concept-of-unlimited-vacation-increase-productivity-and-attract-talen) these days to not give employees a specific amount of vacation time. IMO, this is typically done because managers hate tracking how much time people are taking off. The company is also hoping that on average employees take less time off with this scheme.

Take a reasonable 3-4 weeks off every year, but never more than a week at a time.


### Founders are making bank in salary

While founders stand to make a killing if the company has a successful liquidity event, on average they often [make less in salary](http://www.inc.com/noam-wasserman/four-myths-about-startup-pay.html) than executives that come in later.


## Tips

- Move to the California Bay Area. Salaries are higher (so is cost of living, but as a new graduate, your cost of living will never be lower). There are more jobs, better networking opportunities and IMO the caliber of the engineers out here is higher.
- Know what you're worth, and command a [market salary](http://www.indeed.com/salary?q1=Entry+Level+Software+Engineer).
- Choose the people you work with carefully, especially the founders. You MUST trust the founders. After that, smart employees are the next most important thing.
- You don't necessarily need to be passionate about what the company does, as long as you're passionate about software.
- People generally don't get fired for incompetence (except very early), but DO get fired for attitude and integrity.
- Everything is completely negotiable. Don't buy lines from startups about pay bands or limits on how much equity they can give out. At your review, don't be afraid to ask for more money. Get other offers to make sure you're being payed market value.
- There is a vast range of salaries inside a single company for people who do the same job. It's usually not correlated to what they are worth at all. Be a good negotiator.
- If you're not going to be a founder, consider a [mid-stage startup](http://www.builtinchicago.org/blog/myths-and-realities-about-working-startup) that has had some proven success and secured at least one round of funding.
- SaaS (Software as a service) companies are awesome; they guarantee steady revenue which makes the company more stable and allows them be more aggressive hiring people.
- Your relationship with the person you report your most important job.
- Your relationship with everyone else at the company is just behind that.
- Be cool under pressure. If the site is down, stay calm and get it back up. It's not as big a deal as you think.
- Don't bank on your stock options; max that 401k.
---
title: Subclassing Django's runserver causes command to be run twice
tags: django
---

This week I was creating my own version of the Django management command `runserver`. Like the built-in, I wanted to run a lightweight development only web server for my Django app. In addition, I was looking to run a syncdb, load some initial data and start up the server with that. The goal was to use an in-memory sqlite3 database, and use this command as a way to spin up an interactive test instance.

This is easily achieved by using `call_command` to call `syncdb` and `runserver` in sequence. The in-memory sqlite3 config I would leave to a Django settings file. But I noticed that `syncdb` seemed to be getting called twice, leading to double the start up time.

Rooting around in the Django bug database, I found that this was a [known issue](https://code.djangoproject.com/ticket/8085) with the default auto-reloading functionality of `runserver`. The solution turned out to be a simple as disabling that functionality by passing an option to the native `runserver` when you use `call_command`.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">django.core.management</span> <span class="kn">import</span> <span class="n">call_command</span>
<span class="kn">from</span> <span class="nn">django.core.management.base</span> <span class="kn">import</span> <span class="n">BaseCommand</span>
<span class="kn">from</span> <span class="nn">django.core.management.commands.runserver</span> <span class="kn">import</span> <span class="n">BaseRunserverCommand</span>


<span class="k">class</span> <span class="nc">Command</span><span class="p">(</span><span class="n">BaseCommand</span><span class="p">):</span>
    <span class="s">''' Runs the built-in Django runserver with initial schema and fixtures
    Note: cannot use auto-reloading because that casues resetdb to be called
    twice.
    '''</span>

    <span class="c1"># remove this option from the --help for this command
</span>    <span class="n">option_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">BaseRunserverCommand</span><span class="p">.</span><span class="n">option_list</span> <span class="k">if</span> <span class="n">o</span><span class="p">.</span><span class="n">dest</span> <span class="o">!=</span> <span class="s">'use_reloader'</span><span class="p">)</span>
    <span class="n">help</span> <span class="o">=</span> <span class="s">'Starts a lightweight web server testing.'</span>
    <span class="n">args</span> <span class="o">=</span> <span class="s">'[optional port number, or ipaddr:port] appname appname'</span>

    <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">addrport</span><span class="o">=</span><span class="s">''</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
        <span class="n">call_command</span><span class="p">(</span><span class="s">'syncdb'</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">)</span>
        <span class="c1"># BUG: runserver gets called twice if --noreload is False
</span>        <span class="c1"># https://code.djangoproject.com/ticket/8085
</span>        <span class="n">options</span><span class="p">[</span><span class="s">'use_reloader'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">call_command</span><span class="p">(</span><span class="s">'runserver'</span><span class="p">,</span> <span class="n">addrport</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">)</span></code></pre></figure>
---
title: Hacking Django runserver to run multiple Django instances
tags: django
---

Recently at work we've been on a "servicifying" kick, meaning we're slowly converting our monolithic Django app into separate services. To start, this just means breaking up the existing runtime into pieces. Instead of one logical web process, we now have different ones for the web app, admin, login, apis, etc.

For production, this doesn't change the deployment model all that much. We just have separate servers for various roles. For development, we still want to be able to easily run all the services at once on your local machine.

Enter `runservices`, an extension of the Django `runserver` command that just launches a bunch of processes in a `screen` session (a natural choices for us, as we're already using `screen` intensively). It turns out that screen has the ability to [launch multiple windows on startup](http://superuser.com/questions/386059/how-can-i-start-multiple-screen-sessions-automatically) using a custom `.screenrc` file, passed on the command-line with `-c my.screenrc`. The format needed inside the `.screenrc` file is as follows:

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># create two windows, called "TODO" and "coding" in vim</span>
screen <span class="nt">-t</span> TODO vim TODO.txt
screen <span class="nt">-t</span> coding vim ~/code</code></pre></figure>

When I first saw this, I thought we could use it to launch our various services. What I did was write a custom Django management command that dynamically writes a `.screenrc` file and executes it. You can run all the services, or specify just a few. Our services are launched by settings corresponding environment variables, which can be done on the command line itself.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">django.template.base</span> <span class="kn">import</span> <span class="n">Template</span>
<span class="kn">from</span> <span class="nn">django.template.base</span> <span class="kn">import</span> <span class="n">Context</span>
<span class="kn">from</span> <span class="nn">django.core.management.base</span> <span class="kn">import</span> <span class="n">BaseCommand</span>


<span class="n">SERVICES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="c1"># nothreading == to help with sqlite locking issues
</span>    <span class="p">(</span><span class="s">'web'</span><span class="p">,</span> <span class="s">'./manage.py runserver 0:8000 --nothreading'</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'admin'</span><span class="p">,</span> <span class="s">'ADMIN=True ./manage.py runserver 0:8001 --nothreading'</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'login'</span><span class="p">,</span> <span class="s">'LOGIN=True ./manage.py runserver 0:8002 --nothreading'</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'celery'</span><span class="p">,</span> <span class="s">'./manage.py celery beat'</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">ACTIVATE_REL_PATH</span> <span class="o">=</span> <span class="s">'../virtualenv/bin/activate'</span>

<span class="n">DEFAULT_SERVICES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s">'web'</span><span class="p">,</span>
    <span class="s">'admin'</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">class</span> <span class="nc">Command</span><span class="p">(</span><span class="n">BaseCommand</span><span class="p">):</span>

    <span class="n">help</span> <span class="o">=</span> <span class="s">'Runs all the services'</span>
    <span class="n">args</span> <span class="o">=</span> <span class="s">'appname appname'</span>

    <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
        <span class="n">allowed_services</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">DEFAULT_SERVICES</span> <span class="o">+</span> <span class="n">args</span><span class="p">))</span>
        <span class="n">services</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">SERVICES</span> <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">allowed_services</span><span class="p">]</span>
        <span class="n">multiplexer</span> <span class="o">=</span> <span class="n">ScreenMultiplexer</span><span class="p">(</span><span class="n">services</span><span class="p">)</span>
        <span class="n">multiplexer</span><span class="p">.</span><span class="n">write_rc_file</span><span class="p">()</span>
        <span class="n">multiplexer</span><span class="p">.</span><span class="n">run</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">render_template</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">context_dict</span><span class="p">):</span>
    <span class="n">template</span> <span class="o">=</span> <span class="n">Template</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">Context</span><span class="p">(</span><span class="n">context_dict</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">template</span><span class="p">.</span><span class="n">render</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ScreenMultiplexer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">services</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">services</span> <span class="o">=</span> <span class="n">services</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rc_file</span> <span class="o">=</span> <span class="s">'.screenrc'</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rc_template</span> <span class="o">=</span> <span class="s">'''
caption always "%{= kw}%-w%{= BW}%n %t%{-}%+w %-= Exit: C-a \ %c"
{% for name, command in services %}
screen -t {{ name }} bash -c "{{ venv }}; {{ command }}"
{% endfor %}
select 0
'''</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">venv</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">activate</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">ACTIVATE_REL_PATH</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">activate</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="s">"Could not locate virtualenv script %s"</span> <span class="o">%</span> <span class="n">activate</span><span class="p">)</span>
        <span class="k">return</span> <span class="s">'source %s'</span> <span class="o">%</span> <span class="n">activate</span>

    <span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">render_template</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">rc_template</span><span class="p">,</span> <span class="p">{</span>
            <span class="s">'venv'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">venv</span><span class="p">,</span>
            <span class="s">'services'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">services</span><span class="p">,</span>
        <span class="p">})</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">rc_path</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="bp">self</span><span class="p">.</span><span class="n">rc_file</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">write_rc_file</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">rc_path</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">render</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">os</span><span class="p">.</span><span class="n">execlp</span><span class="p">(</span><span class="s">'screen'</span><span class="p">,</span> <span class="s">'screen'</span><span class="p">,</span> <span class="s">'-c'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">rc_path</span><span class="p">)</span></code></pre></figure>

The resulting `.screenrc` files ends up looking like (added comments):

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># set a nice screen footer</span>
caption always <span class="s2">"%{= kw}%-w%{= BW}%n %t%{-}%+w %-= Exit: C-a </span><span class="se">\ </span><span class="s2">%c"</span>

<span class="c"># run web</span>
screen <span class="nt">-t</span> web bash <span class="nt">-c</span> <span class="s2">"source /Users/myuser/code/myproject/virtualenv/bin/activate; ./manage.py runserver 0:8000 --nothreading"</span>

<span class="c"># run admin</span>
screen <span class="nt">-t</span> admin bash <span class="nt">-c</span> <span class="s2">"source /Users/myuser/code/myproject/virtualenv/bin/activate; ADMIN=True ./manage.py runserver 0:8000 --nothreading"</span>

<span class="c"># select the first window (web) by default</span>
<span class="k">select </span>0</code></pre></figure>

The only tricky bit was getting `virtualenv` to activate properly inside the screen session. Because it uses the bash `source` command, I needed to have `screen` actually execute bash directly, and pass the source command.

The result is a single session of multiple windows that you can toggle through with the standard screen commans like `C-a`.

![screen django](/blog/images/screen.png)

*Note*: I'm also running into intermittent issues with `sqlite3` locking the database, due to many processes trying to access it. Running with `--nothreading` and reducing celery to one worker seems to have helped, but we may need to move to `mysql` for development.
---
title: Faster Django/sqlite runserver with /dev/shm
tags: django linux
---

When you're writing software, the feedback loop is king. Whether you're implementing new functionality, changing server configuration or writing unit tests, the speed of your feedback cycle is critical. The ideal scenario is that you make a change, and you can immediately see and effect. For web apps, this means that you should be able to save your Python/HTML/CSS/Javascript code, `ALT-TAB` to your browser, hit refresh, and see the changes. Any lag introduced in that cycle is bad; the more lag, the worse off you are.

A while back, I found out that when running Django unit tests, you can use sqlite in-memory mode to greatly speed up your test runs. Because Django unit tests create your schema from scratch and insert fixtures every time you run them, this can mean the difference between tens of seconds per change, and just one or two seconds. You can edit your `settings.py` and set the `TEST_NAME` to `None` or `:memory:` to tell sqlite to use and in-memory database.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">DATABASES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'default'</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">'ENGINE'</span><span class="p">:</span> <span class="s">'django.db.backends.sqlite3'</span><span class="p">,</span>
        <span class="s">'NAME'</span><span class="p">:</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATABASE_DIR</span><span class="p">,</span> <span class="s">'sqlite3.db'</span><span class="p">),</span>
        <span class="s">'TEST_CHARSET'</span><span class="p">:</span> <span class="s">'UTF8'</span><span class="p">,</span>
        <span class="s">'TEST_NAME'</span><span class="p">:</span> <span class="bp">None</span>  <span class="c1"># in-memory sqlite db
</span>    <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

I just recently discovered that you can also use an in-memory database for your actual `runserver` development server process. You can't just set the `NAME` to `None`; Django will not start in this mode. Presumably this is because it would not work very well in practice; every time the process started, it would create a schema-less database with no data. This is because `runserver` has no facility to run `syncdb` first. If you ran those two commands back-to-back, it would still not work because the second command-line is a separate process, where the in-memory sqlite database is again created from scratch.

The solution is to save your `sqlite3.db` file in `/dev/shm` or `/run/shm` on Linux. These are what's known as `tmpfs` file systems on Linux. They act as normal directories, except they are entirely [stored in RAM](http://www.cyberciti.biz/tips/what-is-devshm-and-its-practical-usage.html), which is [about 10,000 times faster](http://stackoverflow.com/questions/1371400/how-much-faster-is-the-memory-usually-than-the-disk) than disk for random access. One notable side-effect is that they are also completely wiped out every time you power down. For Django development, this may be OK. Depending on how good a test fixture you have, you may already be regularly blowing away and re-creating your database.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="err">$</span> <span class="n">mount</span>
<span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">sda1</span> <span class="n">on</span> <span class="o">/</span> <span class="nb">type</span> <span class="n">ext4</span> <span class="p">(</span><span class="n">rw</span><span class="p">,</span><span class="n">errors</span><span class="o">=</span><span class="n">remount</span><span class="o">-</span><span class="n">ro</span><span class="p">)</span>
<span class="p">...</span>
<span class="n">none</span> <span class="n">on</span> <span class="o">/</span><span class="n">run</span><span class="o">/</span><span class="n">shm</span> <span class="nb">type</span> <span class="n">tmpfs</span> <span class="p">(</span><span class="n">rw</span><span class="p">,</span><span class="n">nosuid</span><span class="p">,</span><span class="n">nodev</span><span class="p">)</span></code></pre></figure>

These directories are already world-writable. To configure Django to save its database there, you can do the following:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">DATABASES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'default'</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">'ENGINE'</span><span class="p">:</span> <span class="s">'django.db.backends.sqlite3'</span><span class="p">,</span>
        <span class="s">'NAME'</span><span class="p">:</span> <span class="s">'/dev/shm/sqlite3.db'</span><span class="p">,</span>
        <span class="s">'TEST_CHARSET'</span><span class="p">:</span> <span class="s">'UTF8'</span><span class="p">,</span>
        <span class="s">'TEST_NAME'</span><span class="p">:</span> <span class="bp">None</span>  <span class="c1"># in-memory sqlite db
</span>    <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

On my system, this took a `syncdb` command from to 15 seconds (mostly creating indexes) to 3 seconds flat. The actual app also feels much faster.

For OSX, you can do the same thing, but there is no in-memory file system mounted by default. Instead, you can use the [mount-ram.sh gist](https://gist.github.com/koshigoe/822455). Simply download the gist and run `bash mount-ram.sh /tmp/shm 128` to create a 128MB RAM-backed folder.
---
title: Writing a Python decorator that can be called as a function or a callable
tags: python
---

A [Python decorator](http://en.wikipedia.org/wiki/Python_syntax_and_semantics#Decorators) wraps a function with another function. Classing examples are a `@cache` decorator or a `@log` decorator, which call the wrapped function and either cache its results or log the fact that it was called, respectively. Decorators can be implemented as functions or as classes; they just need to be callable.

Here is the basic decorator pattern. This one does nothing but prints that it was called.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>
<span class="kn">import</span> <span class="nn">random</span>


<span class="k">def</span> <span class="nf">my_decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="o">@</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">print</span> <span class="s">'called decorator'</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">wrapped</span>


<span class="o">@</span><span class="n">my_decorator</span>
<span class="k">def</span> <span class="nf">function_to_wrap</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">getrandbits</span><span class="p">(</span><span class="n">bits</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">function_to_wrap</span><span class="p">()</span>  <span class="c1"># prints 'called decorator'</span></code></pre></figure>

Here is an example of a `@cache` decorator implemented in this fashion (as a function). It uses Django's caching layer as the actual cache implementation.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">django.core.cache</span> <span class="kn">import</span> <span class="n">cache</span> <span class="k">as</span> <span class="n">_cache</span>


<span class="k">def</span> <span class="nf">cache</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="o">@</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">cache_key</span> <span class="o">=</span> <span class="p">[</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_cache</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">cache_key</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">result</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">_cache</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">cache_key</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>
    <span class="k">return</span> <span class="n">wrapped</span>


<span class="o">@</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">function_to_wrap</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">getrandbits</span><span class="p">(</span><span class="n">bits</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="k">print</span> <span class="n">function_to_wrap</span><span class="p">()</span>  <span class="c1"># prints '47141457794590517513826129394479136255'
</span>    <span class="k">print</span> <span class="n">function_to_wrap</span><span class="p">()</span>  <span class="c1"># prints '47141457794590517513826129394479136255' also (cached)</span></code></pre></figure>

This uses a very simplistic cache key generation scheme. It assumes that the args and kwargs that your wrapped function will be passed are all castable to strings. Django's default [cache key generator](https://docs.djangoproject.com/en/dev/topics/cache/#cache-key-transformation) looks like:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">make_key</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">key_prefix</span><span class="p">,</span> <span class="n">version</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">':'</span><span class="p">.</span><span class="n">join</span><span class="p">([</span><span class="n">key_prefix</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">version</span><span class="p">),</span> <span class="n">key</span><span class="p">])</span></code></pre></figure>

There are also a number of caveats. For example, Django will throw an exception if the cache key is over 250 characters. Writing your own key generation is out of the scope of this post.


You will also notice that I'm using a [functools.wraps](http://docs.python.org/2/library/functools.html#functools.wraps). This ensures that when callers introspect the `function_to_wrap` function, it shows its `__name__` attribute as `function_to_wrap` and not `cache`. This is especially useful for not mucking up your logging and performance stacktraces (for example, New Relic stats).

Here is an example of the same decorator written as a class:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">django.core.cache</span> <span class="kn">import</span> <span class="n">cache</span> <span class="k">as</span> <span class="n">_cache</span>


<span class="k">class</span> <span class="nc">cache</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">func</span>
        <span class="n">functools</span><span class="p">.</span><span class="n">update_wrapper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">cache_key</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_cache</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">cache_key</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">result</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">_cache</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">cache_key</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>


<span class="o">@</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">function_to_wrap</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">getrandbits</span><span class="p">(</span><span class="n">bits</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="k">print</span> <span class="n">function_to_wrap</span><span class="p">()</span>  <span class="c1"># prints '47141457794590517513826129394479136255'
</span>    <span class="k">print</span> <span class="n">function_to_wrap</span><span class="p">()</span>  <span class="c1"># prints '47141457794590517513826129394479136255' also (cached)</span></code></pre></figure>

Both implementations have the same usage syntax. You just _decorate_ the function definition that you want to wrap with the @ syntax.

## Passing Parameters

Sometimes you want to pass parameters to your decorators. The trick here is to add another layer of indirection and create a function that takes parameters and returns your original decorator. As you can see, the naming also gets a little mind-bending here; as we struggle to propery name what should really be anonymous functions for the callable we're returning, and the function that defines the logic of our decorator.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">django.core.cache</span> <span class="kn">import</span> <span class="n">cache</span> <span class="k">as</span> <span class="n">_cache</span>


<span class="k">def</span> <span class="nf">cache</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">callable</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="o">@</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="n">cache_key</span> <span class="o">=</span> <span class="p">[</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">]</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">_cache</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">cache_key</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">result</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">_cache</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">cache_key</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">seconds</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">result</span>
        <span class="k">return</span> <span class="n">wrapped</span>

    <span class="k">return</span> <span class="nb">callable</span>


<span class="o">@</span><span class="n">cache</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">function_to_wrap</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">getrandbits</span><span class="p">(</span><span class="n">bits</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="k">print</span> <span class="n">function_to_wrap</span><span class="p">()</span>  <span class="c1"># prints '47141457794590517513826129394479136255'
</span>    <span class="k">print</span> <span class="n">function_to_wrap</span><span class="p">()</span></code></pre></figure>

Of course, you can also do the same thing in the class style. Again, the trick is that a decorator can be a callable, _or return a callable_.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">django.core.cache</span> <span class="kn">import</span> <span class="n">cache</span> <span class="k">as</span> <span class="n">_cache</span>


<span class="k">class</span> <span class="nc">cache</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seconds</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">seconds</span> <span class="o">=</span> <span class="n">seconds</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>

        <span class="o">@</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">callable</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="n">cache_key</span> <span class="o">=</span> <span class="p">[</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">]</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">_cache</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">cache_key</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">result</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">_cache</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">cache_key</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">seconds</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">result</span>

        <span class="k">return</span> <span class="nb">callable</span>


<span class="o">@</span><span class="n">cache</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">function_to_wrap</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">getrandbits</span><span class="p">(</span><span class="n">bits</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="k">print</span> <span class="n">function_to_wrap</span><span class="p">()</span>  <span class="c1"># prints '47141457794590517513826129394479136255'
</span>    <span class="k">print</span> <span class="n">function_to_wrap</span><span class="p">()</span>  <span class="c1"># prints '47141457794590517513826129394479136255' also (cached)</span></code></pre></figure>

## Optional Parameters

Now, a whole in the design of decorators, in my opinion, is that while you're deciding to make your decorator a callable or return a callable, you may also be struggling with how to make it do both at once.

What if I don't want the `seconds` argument to be mandatory? With either the functional or class based implementations, you will end up using your decorator like so:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">@</span><span class="n">cache</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">function_to_wrap</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">getrandbits</span><span class="p">(</span><span class="n">bits</span><span class="p">)</span></code></pre></figure>

This is just ugly. It introduces a source of errors (leaving off the `()` will throw a somewhat mysterious exception:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nb">TypeError</span><span class="p">:</span> <span class="n">__call__</span><span class="p">()</span> <span class="n">takes</span> <span class="n">exactly</span> <span class="mi">2</span> <span class="n">arguments</span> <span class="p">(</span><span class="mi">1</span> <span class="n">given</span><span class="p">)</span></code></pre></figure>

With a little ingenuity, you can have your callable and return it, too. Here is a functional decorator that can be used as `@cache(seconds=60)`, or just `@cache`.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">django.core.cache</span> <span class="kn">import</span> <span class="n">cache</span> <span class="k">as</span> <span class="n">_cache</span>


<span class="k">def</span> <span class="nf">cache</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

    <span class="n">func</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">__builtins__</span><span class="p">.</span><span class="nb">callable</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">func</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">func</span><span class="p">:</span>
        <span class="n">seconds</span> <span class="o">=</span> <span class="mi">60</span>  <span class="c1"># default values
</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">func</span><span class="p">:</span>
        <span class="n">seconds</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'seconds'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">callable</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="o">@</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="n">cache_key</span> <span class="o">=</span> <span class="p">[</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">]</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">_cache</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">cache_key</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">result</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">_cache</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">cache_key</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">seconds</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">result</span>
        <span class="k">return</span> <span class="n">wrapped</span>

    <span class="k">return</span> <span class="nb">callable</span><span class="p">(</span><span class="n">func</span><span class="p">)</span> <span class="k">if</span> <span class="n">func</span> <span class="k">else</span> <span class="nb">callable</span>


<span class="o">@</span><span class="n">cache</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">function_to_wrap</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">getrandbits</span><span class="p">(</span><span class="n">bits</span><span class="p">)</span>

<span class="o">@</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">function_to_wrap2</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">getrandbits</span><span class="p">(</span><span class="n">bits</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="k">print</span> <span class="n">function_to_wrap</span><span class="p">()</span>  <span class="c1"># prints '47141457794590517513826129394479136255'
</span>    <span class="k">print</span> <span class="n">function_to_wrap</span><span class="p">()</span>  <span class="c1"># prints '47141457794590517513826129394479136255' also (cached)
</span>    <span class="k">print</span> <span class="n">function_to_wrap2</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>  <span class="c1"># prints '2202905596'
</span>    <span class="k">print</span> <span class="n">function_to_wrap2</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>  <span class="c1"># prints '2202905596' also (cached)</span></code></pre></figure>

First, you decide whether your decorator has been called as a callable or not. If not, you pull out your optional parameters (and default them if needed). Then you dynamically return either your decorator or a callable. Admittedly this is pretty ugly, but the resulting API is nice and clear. I've also failed repeatedly to produce a class based version of this. Submissions welcome!

---
title: Development on a Mac versus Linux
tags: osx linux
---

I love the Mac computing experience. Their laptop hardware, like their phones, feel solid and elegant. OSX usability is excellent, though it is myopically geared towards the lowest common denominator of user. For most tasks, I appreciate that trade-off of affordability versus advanced power features. But not when I'm doing development. Even though I use a Mac as my home laptop, I prefer a Linux machine for work. I also keep a Windows desktop at home for gaming. To me, that's just using the right tool for the job.

# Package Management

By far my biggest complaint with OSX (and Windows) is the state of package management. Once you have experienced installing a piece of software in totally automated fashion from the command line, it's hard to go back to Googling for a binary (or worse, the source) and struggling on from there. I feel dirty, like I'm hanging out on a BBS trawling for warez. That's why almost every new language these days has package management built-in. Would you consider adopting a language without one? Then why do you put up with that for software?

Whereas on Windows there is literally nothing (that I have heard of, anyway), on a Mac at least there is the App Store, and brew. The App Store is not what I would consider a package manager for development purposes; generally you can't install stuff like MySQL, Redis, Python, etc from there. That's what Brew is for. But their "packages" consist of essentially bash scripts that go out an download a binary from the same website you would get to if you Googled it, and they have per-package code that checks for things like dependencies. Often the veil is lifted and you will be presented with a GUI installer; exactly not the point.

In my experience, brew fails to properly install something too often. Whether it's a 404 on the download, a missing dependency it did not offer to install or just plain botched logic, it's frustrating just as often as it's useful. Blind rage ensued when installing brew itself, which required me to go register for an apple.com account and download a **MASSIVE** XCode installer manually. Arg! A good effort, but about what you would expect from trying to bolt-on a package manager after the fact.

APT, on the other hand, is extremely reliable, and is much faster. It has binaries ready to go for almost every conceivable platform. It's dependency management is excellent. You can even upgrade and remove packages reliably. Imagine that!

# System Updates

Recently I upgraded a Mac from OS 1.7 to 1.8. All of a sudden my Python dependencies are broken due to some path change. XCode isn't even installed anymore. All of my project's virtualenvs needs to be re-installed from scratch.

This is one problem I have never run into on Linux. APT knows exactly what I have installed, and makes sure everything is up to date, even if you're doing a full distro upgrade. Because package management is built-in and continuously exercised on production machines, you have a much better chance of not having to manually futz with anything.

# Passive Devops Education

If your only experience with Linux starts with asking Amazon to clone an existing image, then you're missing out on a whole bunch of ancillary knowledge. Configuring X is it's own little world, complete with kernel panics and un-bootable systems. You will probably learn how to emergency boot into different run levels, and REISUB restart a seemingly dead system, both of which could come in handy if you even need to rescue a physical server. Maybe your video is skipping, so you learn all about nice levels and end up using that later on your production database. If you learn all about PPAs by tracking down a custom fork of GVIM with a transparent background, you may end up using that knowledge to easily install a commercial HBase version.

The more exposure you get to the platform, the broader your knowledge base about Linux in general. This is especially valuable if you're not really in a devops role trouble-shooting production systems every day. Sure, seemingly mundane tasks can take an unreasonable amount of time on Linux, especially for new users. But it builds character ;)

# Trade-offs

Now, you might rightly point out that you don't spend a very large percentage of your time installing packages. Nor do you update your operating system very often. You already learn a remarkable amount about Linux by simply SSHing into servers. True enough. That's why, despite my rant, I don't feel super strongly about this preference. Certainly there are times when I'm struggling to get multiple monitors working in X that I would just as soon not be wasting my time. In the end, I just like running all three of the major operating systems regularly, keeping my toe in the water for each.

Graphics display configuration is definitely not the only downside to using Linux as your desktop. Some software may not be compatible at all, such as XCode. Some, like Hipchat, have half-baked Linux versions. God help you if you try to hook up a projector to a Linux laptop, or try to get the damn thing to sleep properly. Oh, and I hope you didn't want any kind of remote desktop solution that doesn't perform like you're on a 14.4bps dial-up connection, even though Windows had that nailed back in 2000.

On the other hand, at least you have your Linux desktop merit badge. Always impresses at parties.
---
title: Saving for Retirement as a Software Engineer
tags: 401k reading-list
---

> Take 100 young Americans starting out at age 25. By age 65, one will be rich and four will be financially independent. The remaining 95 will reach the traditional retirement age unable to self-sustain the lifestyle to which they have become accustomed. - [The Bogleheads' Guide to Investing](http://www.amazon.com/Bogleheads-Guide-Investing-Taylor-Larimore/dp/0470067365)

This is intended as a bare-bones intro to saving for retirement, targeted at software engineers. When I got my very first paycheck after college, I was fortunate enough to have a friend pull me aside and give me the benefit of their experience regarding saving. I didn't know what a 401k was! If this describes you too, then listen up!

The average salary for a software engineer in the Bay Area ranges from [$73k at entry level](http://www.indeed.com/salary/q-Entry-Level-Software-Engineer-l-San-Francisco,-CA.html) to [$124k for a senior software engineer](http://www.indeed.com/salary/q-Senior-Developer-l-San-Francisco,-CA.html). Ballpark, you make about $100k a year in your 20s. If you want to have that same income in retirement, you need about [four million dollars](http://money.cnn.com/calculator/retirement/retirement-need/) saved up. Anything less, and your income will be lower than it was in your 20s (adjusted for inflation).

That's a BIG number. If you are a dual income family, you may very well need twice that amount. The good news is that you can do it!

## Compound Interest

If you save approximately 25% of your gross income (pre-tax), then you can easily get there. But if you work until you're 65, that will be "only" $25k * 45 years, or $1.125m. Where does the other $3m come from? Stop me if you've heard this before. The secret is the power of compound interest.

>Compound interest arises when interest is added to the principal of a deposit or loan, so that, from that moment on, the interest that has been added also earns interest. -Wikipedia

The stock market has historically returned [about 9.5% annually](http://observationsandnotes.blogspot.com/2009/03/average-annual-stock-market-return.html) over the last 100 years. Some years it's negative, but other years are way up. A more conservative estimate that you often hear is 7%.

![compound interest](http://mindyourdecisions.com/blog/wp-content/uploads/2013/01/myth-compound-interest-magic-20-years.png)

Over time, that 7% interest will overtake your actual cash deposits into your savings. Your savings will be growing more and more due to interest than anything else.

![compound interest 2](/blog/images/compound.png)

If you save $25k a year for 45 years at 7% interest, you will end up with $7.6m. Start just 10 years later at age 30, and you will have $3.6m. Feel free to play around with a [compound interest calculator](http://www.thecalculatorsite.com/finance/calculators/compoundinterestcalculator.php) yourself.

## 401ks

A 401k is a service offered by many employers where a set dollar amount is deducted from your paycheck before taxes and sent to a financial institution, where it is automatically invested into funds of your choice. An individual can contribute up to $17.5k a year to a 401k. Many larger companies also offer a dollar match, which can put you above the $17.5k limit.

If you work at a company the offers a match, that's free money! It's definitely something to think about the next time you get a job offer.

The money you contribute will grow, tax free, until you turn 59.5. At that point, you can start withdrawing the money, and you will pay taxes on it at that time. You can also make early withdrawals for specific things like buying a house and going back to school. When you leave your current company, you can roll your 401k over to your new employer, or into a third-party financial institution.

The main advantages of a pre-tax savings are:

- It reduces your taxable income for that year.
- You can afford to save more, so your savings will reach a higher peak value.
- When you do pay taxes on the money in retirement, your tax rate will likely be lower.
- You can choose to retire in a state with lower taxes.

For me, the primary advantage is that it's automatic. It happens before you even see the money.

### How Fees Kill You

Most companies do not manage their own 401k plans. Instead, they outsource it to someone like Fidelity, Schwab or Vanguard. You might think that, like a bank, just having the utility of your money would be compensation enough for these companies. But in reality, they charge high maintenance fees. Most companies pass these costs on to their employees.

As exposed in a [60 minutes episode](http://www.youtube.com/watch?v=JhW0uXlOQJs) last year, most employees don't realize they are paying these fees. Indeed, if you ask your 401k provider point blank about costs, they will likely tell you that there are no fees.

This is technically true, but ingenuous. What is happening is that you elect to put your savings into one or more mutual funds. Those mutual funds have what's called a [load](http://www.fool.com/school/mutualfunds/costs/loads.htm), also known as an **expense ratio**. The load is a fee; it's a percentage that the fund managers charge you every year. [Average mutual fund loads](http://www.investopedia.com/university/mutualfunds/mutualfunds2.asp) are in the 1.3% to 1.5% range. Your typical 401k will raises these loads by as much as a full percentage, to 2.3% to 2.5%, and pocket the difference.

That's the difference between making a 7% return on your investments over time, or making 6%. Here again the magic of compound interest is at work, but this time it's to your disadvantage. A 1% difference in return rates could cost you [30% of your retirement savings](http://abcnews.go.com/blogs/business/2012/05/401k-fees-may-cut-30-pct-from-retirement-balance/) over 45 years.

![fees](http://quantumcapitalinvestments.com/wp-content/uploads/2011/09/401k.png)

This is just over 20 years. Notice that the gap is widening over time! Not all providers charge this much. Vanguard, for example, is known for low fees. However, the employee at a company usually has no control over which provider the employer uses.

### Index Funds

Even if your 401k provider does not add much load to your funds, you still have the base load of the fund itself. This is the aforementioned 1.3% to 1.5%. These fees go primarily to the bottom line profit of the fund managers themselves, with some portion going to trading fees overhead.

You're effectively paying these managers 1% of your savings every year to invest your money for you. These are known as **actively managed funds**. But there are also **index funds**.

Index funds are not managed by a person, but by an algorithm. They closely follow some well-known index like the [S&P 500](http://en.wikipedia.org/wiki/S%26P_500). As a result, they provide a return rate very close to the overall return rate of the stock market in general, and do it with much lower fees, typically in the 0.1% range.

Do actively managed funds out perform index funds, to justify their extra fees? Many industry experts don't think so:

>Most investors, both institutional and individual, will find that the best way to own common stocks is through an index fund that charges minimal fees. Those following this path are sure to beat the net results (after fees and expenses) delivered by the great majority of investment professionals. - Warren Buffett

>Only about one out of every four equity funds outperforms the stock market. That's why I'm a firm believer in the power of indexing. - Charles Schwab

Mr. Schwab is talking about a single year. The chances of an actively managed fund beating an index vanish to almost zero over larger times scales:

>If you go back to 1970, there were only 355 equity funds. Only 169 of them survive today, so right away you are dramatically skewing the numbers by not counting the losers. Of those 169 survivors, only nine beat the S&P 500 through 1999. - John Bogle

## Other Vehicles

There are many other types of retirement savings accounts. Here are a few:

### Traditional IRA

Also pre-tax, but limited to $5k a year. There are also [income limits](http://en.wikipedia.org/wiki/Traditional_IRA#Income_limits). For example, if are married and make over $115k combined, then your contributions to a traditional IRA are no longer tax deductible.

### Roth IRA

With a Roth IRA, you put money in after taxes, but the proceeds are not taxed when you withdraw them in retirement. There is a limit of $5.5k. One interesting difference is that you can take funds out of a Roth IRA at any time with no penalty. You can also convert a traditional IRA to a Roth IRA at any time by paying the taxes due. Roth IRAs have their own income limits; you can't contribute at all as a married couple making over $178k.

### 403b, SEP, Pensions

These are not commonly available to software engineers.

## How to Start

>The best time to plant a tree was 20 years ago. The second best time is now. – Chinese Proverb

If you can't swing $17.5k a year just yet, you can surely do 1% of your gross pay. Getting started is as easy as logging in to your 401k provider's website, setting a dollar amount and a percentage split of one or more funds. Hopefully you now have some idea of what funds to pick. But you can hardly go wrong! Saving something in ANY account is better than saving nothing.

Many providers allow you to automatically increase your percentage over time, say by 1% a year. Another common tactic is to start low, but when you get a raise put the additional amount into your 401k.

## More Reading

For those interested, I would recommend the following resources:

- [The Bogleheads' Guide to Investing](http://www.amazon.com/Bogleheads-Guide-Investing-Taylor-Larimore/dp/0470067365)
- [The Millionaire Next Door](http://www.amazon.com/Millionaire-Next-Door-Thomas-Stanley/dp/0671015206)
- [The Bogleheads Forum](http://www.bogleheads.org/forum/index.php)
---
title: Print to the console in Python without UnicodeEncodeErrors
tags: python
---

I can't believe I just found out about this! If you use Python with unicode data, such as Django database records, you may have seen cases where you print a value to the console, and if you hit a record with an extended (non-ascii) character, your program crashes with the following:

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
  File <span class="s2">"foobar.py"</span>, line 792, <span class="k">in</span> &lt;module&gt;
    print value
UnicodeEncodeError: <span class="s1">'ascii'</span> codec can<span class="s1">'t encode character u'</span><span class="se">\x</span>a0<span class="s1">' in position 20: ordinal not in range(128)</span></code></pre></figure>

This often comes up when you have a Django model instance with a user-entered field in the `__unicode__` return value. In the past, I have solved this by changing the print statement to print something that cannot be unicode, such as a database ID. Alternatively, you could do something like `value.encode('ascii', 'ignore')`. But just this week, I realized that there is a much better solution.

Your terminal can typically handle UTF8 characters just fine. The issue is actually that Python is just getting confused about what encoding the terminal accepts. However, you can [explicitly set this value](http://docs.python.org/2/using/cmdline.html#envvar-PYTHONIOENCODING).

The `PYTHONIOENCODING` environment variable controls what encoding stdout/stderr uses. If you do an `export PYTHONIOENCODING=UTF-8`, it will solve the problem. You can also prefix any given single python command-line invocation with this value, such as `PYTHONIOENCODING=UTF-8 ./manage.py runserver`.

Alternatively, you can set this value in your actual code, say in your `settings.py` file:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">codecs</span>
<span class="n">sys</span><span class="p">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">codecs</span><span class="p">.</span><span class="n">getwriter</span><span class="p">(</span><span class="s">'utf8'</span><span class="p">)(</span><span class="n">sys</span><span class="p">.</span><span class="n">stdout</span><span class="p">)</span>
<span class="n">sys</span><span class="p">.</span><span class="n">stderr</span> <span class="o">=</span> <span class="n">codecs</span><span class="p">.</span><span class="n">getwriter</span><span class="p">(</span><span class="s">'utf8'</span><span class="p">)(</span><span class="n">sys</span><span class="p">.</span><span class="n">stderr</span><span class="p">)</span></code></pre></figure>
---
title: Python script to delete merged git branches
tags: git python
---

One of the great things about git is how fast it is. You can create a new branch, or switch to another branch, almost as fast as you can type the command. This tends to lower the impedance of branching. As a result, many individuals and teams will naturally converge on a process where they create many, many branches.

If you're like me, you may have 30 branches at any given time. This can make viewing all the branches unwieldy. Once I week or so, I would go on a branch deletion spree by manually copying and pasting multiple branch names into a `git branch -D` statement.

The basic use case is that you want to delete any branches that are already merged into master. Here is a python script that automated just that.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">subprocess</span> <span class="kn">import</span> <span class="n">check_output</span>
<span class="kn">import</span> <span class="nn">sys</span>


<span class="k">def</span> <span class="nf">get_merged_branches</span><span class="p">():</span>
    <span class="s">''' a list of merged branches, not couting the current branch or master '''</span>
    <span class="n">raw_results</span> <span class="o">=</span> <span class="n">check_output</span><span class="p">(</span><span class="s">'git branch --merged upstream/master'</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">b</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">raw_results</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">b</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">b</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'*'</span><span class="p">)</span> <span class="ow">and</span> <span class="n">b</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">!=</span> <span class="s">'master'</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">delete_branch</span><span class="p">(</span><span class="n">branch</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">check_output</span><span class="p">(</span><span class="s">'git branch -D %s'</span> <span class="o">%</span> <span class="n">branch</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">strip</span><span class="p">()</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">dry_run</span> <span class="o">=</span> <span class="s">'--confirm'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="p">.</span><span class="n">argv</span>
    <span class="k">for</span> <span class="n">branch</span> <span class="ow">in</span> <span class="n">get_merged_branches</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">dry_run</span><span class="p">:</span>
            <span class="k">print</span> <span class="n">branch</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span> <span class="n">delete_branch</span><span class="p">(</span><span class="n">branch</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dry_run</span><span class="p">:</span>
        <span class="k">print</span> <span class="s">'*****************************************************************'</span>
        <span class="k">print</span> <span class="s">'Did not actually delete anything yet, pass in --confirm to delete'</span>
        <span class="k">print</span> <span class="s">'*****************************************************************'</span></code></pre></figure>

To print the branches that would be deleted, just execute `python delete_merged_branches.py`. To actually delete the branches, execute `python delete_merged_branches.py --confirm`.
---
title: Limiting Project Scope and Optimizing for Shipability
tags: process reading-list
---

>Observe that for the programmer, as for the chef, the urgency of the patron may govern the scheduled completion of the task, but it cannot govern the actual completion. An omelette, promised in two minutes, may appear to be progressing nicely. But when it has not set in two minutes, the customer has two choices--wait or eat it raw. Software customers have had the same choices. - The Mythical Man-Month

Too often software engineers find themselves 1:50 into this process thinking, "Oh god, I hope this omelet sets in the next ten seconds". You probably know this feeling. In the kitchen, it's called "being in the weeds". In a software project, it's the begrudged natural state for pretty much all project time outside the initial few week "honeymoon" phase.

This should scare the shit out of you.

Instead, I see most engineers take it in stride. Their attitude is, "This is the inevitable nature of software projects". Never mind that at any decent company, the engineers themselves came up with the schedule. They blame unrealistic requirements, never mind that they knew the requirements going into it. What they should be thinking is, "Damn, all these variables were in my control and we're still going to be late".

The variable that many engineers ignore is scope. When asked to make a 6 egg omelet that may not set in time, start thinking about how to make a one egg omelet that will definitely set quickly. That's your minimum viable breakfast, aka a *potentially shippable* omelet, and it's your golden ticket.


# Moving past breakfast

OK, the cooking analogy is breaking down. An omelet with six eggs that set at six different times is going to be totally gross. But that's not true of software. Your stake holders only care about scope (does it have mushrooms?), cost (why is this omelet $18.50?) and time (is it done yet?). Cost depends on headcount, and is usually not under your control. Scope should be something that you control, and you should seek to reduce the scope you are *committing* to as much as possible, putting most features in the "maybe" pile. Time is seen as knob to turn only in so far as you are willing to work harder, i.e. the overtime death march. But it's actually much more of a function of priorities.

Your top priority should be to get your new thing into a shippable state. As soon as possible.

Most engineers want to start with a refactoring task of unknown length, or maybe a research project with a nebulous goal. Those are fun. By doing them first, you are treating them as your top priority. While I agree that they do need to happen as part of a healthy project life cycle, they rarely need to happen first. You may be thinking of a particular project, and saying, "But those were the *necessary* pieces of the project. Nothing else would have worked without them. That was the whole point!"

But that's just it. Those items are *not* the point. The point of the entire exercise is to deliver customer value. I assure you they don't give a shit about refactoring or wiz-bang internals. What they care about is the functionality. And I bet if you put shipability first, you will see that there is likely a way to get to a shippable state with much less effort. In many cases, almost immediately.


# Born on third base

The empowering bit is that once you reach a shippable state, you're virtually assured of hitting your project timeline. In the remaining 80% of the time, go ahead and do whatever refactoring and prototyping you were planning on. Tack on every feature you have time to get to. Just keep bringing the project back to shippable at short intervals, like a week or two.
---
title: Celery fail silently context manager
tags: python celery
---

[Celery](http://www.celeryproject.org/) ships with an configuration option called [CELERY_ALWAYS_EAGER](http://celery.readthedocs.org/en/latest/configuration.html#celery-always-eager) which causes all tasks to be executed immediately instead of being asynchronously executed on workers. This can be very useful for unit tests. Instead of running a real message queue and separate worker processes, your unit tests can execute all in one process and still run the necessary tasks.

But in many cases, those tasks are not necessary for the unit tests to succeed. Say you have a task that fires when you create a user that sends a welcome email. You don't want the caller to wait while a worker composes a MIME message and contacts the SMTP server; that could take a little time. It's more of a fire and forget model. Actually, this is ALWAYS the case if you don't use the celery results backend; your callers have no way of receiving a result from the task, so it can't depend on it in a strict sense, unless it's doing some kind of database polling and waiting.

If this matches your usage pattern for celery, it may make you wish that you have a setting called CELERY_ALWAYS_FAIL_SILENTLY, instead. To save time on your unit tests, you could tell celery to simply discard any calls to `.delay()` or `.apply_async()`. It would be just as if celery had accepted the task, but it hadn't got around to running just yet. Except that it would never run.

It turns out that you can implement this yourself by monkeypatching celery. Here is a context manager that does just that:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">celery</span> <span class="kn">import</span> <span class="n">Celery</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">Celery</span><span class="p">(</span><span class="s">'tasks'</span><span class="p">,</span> <span class="n">broker</span><span class="o">=</span><span class="s">'amqp://guest:guest@localhost:5672//'</span><span class="p">)</span>

<span class="o">@</span><span class="n">app</span><span class="p">.</span><span class="n">task</span>
<span class="k">def</span> <span class="nf">mytask</span><span class="p">():</span>
    <span class="k">print</span> <span class="s">'Inside mytask'</span>

<span class="o">@</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">celery_fail_silenty</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="s">''' short-circuit all tasks unless we are in eager mode '''</span>

    <span class="kn">from</span> <span class="nn">celery.app</span> <span class="kn">import</span> <span class="n">current_app</span>
    <span class="n">app</span> <span class="o">=</span> <span class="n">current_app</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">send_task</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">app</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">CELERY_ALWAYS_EAGER</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">original_send_task</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">original_send_task</span> <span class="o">=</span> <span class="n">app</span><span class="p">.</span><span class="n">send_task</span>
    <span class="n">app</span><span class="p">.</span><span class="n">send_task</span> <span class="o">=</span> <span class="n">send_task</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span> <span class="mi">1</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">app</span><span class="p">.</span><span class="n">send_task</span> <span class="o">=</span> <span class="n">original_send_task</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="c1">#app.conf.update(CELERY_ALWAYS_EAGER=True)
</span>    <span class="k">with</span> <span class="n">celery_fail_silenty</span><span class="p">():</span>
        <span class="n">mytask</span><span class="p">.</span><span class="n">delay</span><span class="p">()</span></code></pre></figure>

As an added feature, it will execute the task in process if CELERY_ALWAYS_ENABLED is set. That way, you can use something like Django's [override settings](https://docs.djangoproject.com/en/1.4/topics/testing/#overriding-settings) if you want just a small subset of your unit tests to actually execute their tasks.

I originally tried to mock `apply_async` directly, but that is a bound method per-task, and they are bound on import, so you can't easily change them all at runtime.
---
title: Speed up your Vagrant NFS shares with cachefilesd
tags: vagrant
---

Most web based tech startups are deploying to cloud hosted Linux machines. At the same time, only a small percentage of engineers actually run Linux on their development desktops. Enter [Vagrant](http://www.vagrantup.com/), an easy way to provision local Linux development environments in a virtual machine.

At my current startup, we're using Vagrant with our regular [Chef](http://www.getchef.com/chef/) recipes so that the configuration of development matches production as closely as possible. The primary difference is the way the code is loaded. Instead of being saved on the VM itself, it's mounted via a shared directory with the host. That way, the 1/4 or so of the engineers on the team that use a graphical IDE as their editor get the super fast file access they require.

In that setup, although we're editing files on the host system, we run git inside the guest VM. This is done to support [git hooks](http://git-scm.com/book/en/Customizing-Git-Git-Hooks) that rely on our dependencies that are only installed inside the VM.

# VirtualBox Shared Folders are Slow

Right at the start, we noticed considerable lag running git commands in this setup. At this point we were using the default [synced folder](http://docs.vagrantup.com/v2/synced-folders/basic_usage.html) mechanism, VirtualBox shared folders. Our `Vagrantfile` had a section like this:

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">synced_folder</span> <span class="s2">"~/projects"</span><span class="p">,</span> <span class="s2">"/home/vagrant/projects"</span></code></pre></figure>

After a little research, we found that there were [known issues](http://jsosic.wordpress.com/tag/shared-folders/) with VirtualBox shared folders being [very slow for large folders](https://forums.virtualbox.org/viewtopic.php?f=6&t=55044). The recommended solution was to use [NFS](http://en.wikipedia.org/wiki/Network_File_System).

# Using NFS with Vagrant

Vagrant makes switching to NFS fairly easy. It's all pretty seamless with a simple configuration change, except for the fact that folder permissions owner and group can only be set to the vagrant user. For us, this involved some chef recipe tweaking; as we normally place the files in the developer's home directory.

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">synced_folder</span> <span class="s2">"~/projects"</span><span class="p">,</span> <span class="s2">"/home/vagrant/projects"</span><span class="p">,</span> <span class="ss">type: </span><span class="s2">"nfs"</span></code></pre></figure>

You will need to do a `vagrant reload` to update the config.

*Note: don't try to use NFS4 with `mount_options: ['vers=4']` on a Mac host. The Mac NFS4 implementation is [not ready for primetime](http://dfusion.com.au/wiki/tiki-index.php?page=NFSv4+on+Apple+OS+X).*.

# Futher Performance Improvements with cachefilesd

Git commands were still not running instantly, which is part of the benefit of using git in the first place. Enter [cachefilesd](http://linux.die.net/man/8/cachefilesd), a Linux service that caches NFS file access.

You can install it manually in your Vagrant instance, just to see what the performance difference is.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">sudo </span>apt-get <span class="nb">install </span>cachefilesd
<span class="nb">sudo echo</span> <span class="s2">"RUN=yes"</span> <span class="o">&gt;</span> /etc/default/cachefilesd</code></pre></figure>

Then, update your `Vagrantfile` as follows:

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">synced_folder</span> <span class="s2">"~/projects"</span><span class="p">,</span> <span class="s2">"/home/vagrant/projects"</span><span class="p">,</span> <span class="ss">type: </span><span class="s2">"nfs"</span><span class="p">,</span> <span class="ss">mount_options: </span><span class="p">[</span><span class="s1">'rw'</span><span class="p">,</span> <span class="s1">'vers=3'</span><span class="p">,</span> <span class="s1">'tcp'</span><span class="p">,</span> <span class="s1">'fsc'</span><span class="p">]</span>  <span class="c1"># the fsc is for cachedfilesd</span></code></pre></figure>

You will need to do a `vagrant reload` to update the config. You can check that cachefilesd is actually working by listing the cache directory inside the VM:

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">cd</span> /var/cache/fscache/
<span class="nb">sudo du</span> <span class="nt">-sh</span></code></pre></figure>

If you are using Chef, you can install cachefilesd with something like the following:

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">package</span> <span class="s2">"cachefilesd"</span> <span class="k">do</span>
  <span class="n">action</span> <span class="ss">:install</span>
<span class="k">end</span>

<span class="n">file</span> <span class="s2">"/etc/default/cachefilesd"</span> <span class="k">do</span>
  <span class="n">content</span> <span class="o">&lt;&lt;-</span><span class="no">EOS</span><span class="sh">
RUN=yes
</span><span class="no">  EOS</span>
  <span class="n">action</span> <span class="ss">:create</span>
  <span class="n">mode</span> <span class="mo">0755</span>
<span class="k">end</span></code></pre></figure>

# Leveraging Git's preloadindex setting

I also found a git setting specifically designed to [increase peformance over NFS](http://git-scm.com/docs/git-config). Hopefully this will be made a [default git setting](http://git.661346.n2.nabble.com/git-status-takes-30-seconds-on-Windows-7-Why-td7580816.html#a7580853) soon. Simply run the following.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">git config core.preloadindex <span class="nb">true</span></code></pre></figure>

# Measuring the Performance Difference

Here are the stats for our git repository. This was in a repo with approximately 33,000 files. The host machine is OSX 10.9 with an SSD. The guest VM was configured with 2GB RAM and 2 CPUs. My methodology was to run `git status` four times for each configuration. The first time is noted separately, and the last three times are averaged. Time was measured using the `time` command.

![performance numbers](/blog/images/nfsgit.png)

# Still looking for a better solution

Git over NFS is [not ideal](http://git.661346.n2.nabble.com/hosting-git-on-a-nfs-td1489016.html), even when everything is "local". The main problem seems to be very large repos (tens of thousands of files or more). Smaller repos still perform in the hundreds of milliseconds.

Other stuff to try:

- Git 1.7 added [sparse checkouts](http://jasonkarns.com/blog/subdirectory-checkouts-with-git-sparse-checkout/)
- Git also has an [--assume-unchanged](http://git-scm.com/docs/git-update-index#_using_%60%60assume_unchanged%27%27_bit) that can be used to exlcude directories that don't often change.
---
title: Multi-level argparse in Python (parsing commands like git)
tags: python
---

It's a common pattern for command line tools to have multiple subcommands that run off of a single executable. For example, `git fetch origin` and `git commit --amend` both use the same executable `/usr/bin/git` to run. Each subcommand has its own set of required and optional parameters.

This pattern is fairly easy to implement in your own Python command-line utilities using [argparse](http://docs.python.org/3.4/library/argparse.html). Here is a script that pretends to be git and provides the above two commands and arguments.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#!/usr/bin/env python
</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">sys</span>


<span class="k">class</span> <span class="nc">FakeGit</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
            <span class="n">description</span><span class="o">=</span><span class="s">'Pretends to be git'</span><span class="p">,</span>
            <span class="n">usage</span><span class="o">=</span><span class="s">'''git &lt;command&gt; [&lt;args&gt;]

The most commonly used git commands are:
   commit     Record changes to the repository
   fetch      Download objects and refs from another repository
'''</span><span class="p">)</span>
        <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'command'</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">'Subcommand to run'</span><span class="p">)</span>
        <span class="c1"># parse_args defaults to [1:] for args, but you need to
</span>        <span class="c1"># exclude the rest of the args too, or validation will fail
</span>        <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">command</span><span class="p">):</span>
            <span class="k">print</span> <span class="s">'Unrecognized command'</span>
            <span class="n">parser</span><span class="p">.</span><span class="n">print_help</span><span class="p">()</span>
            <span class="nb">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># use dispatch pattern to invoke method with same name
</span>        <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">command</span><span class="p">)()</span>

    <span class="k">def</span> <span class="nf">commit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
            <span class="n">description</span><span class="o">=</span><span class="s">'Record changes to the repository'</span><span class="p">)</span>
        <span class="c1"># prefixing the argument with -- means it's optional
</span>        <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--amend'</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s">'store_true'</span><span class="p">)</span>
        <span class="c1"># now that we're inside a subcommand, ignore the first
</span>        <span class="c1"># TWO argvs, ie the command (git) and the subcommand (commit)
</span>        <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
        <span class="k">print</span> <span class="s">'Running git commit, amend=%s'</span> <span class="o">%</span> <span class="n">args</span><span class="p">.</span><span class="n">amend</span>

    <span class="k">def</span> <span class="nf">fetch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
            <span class="n">description</span><span class="o">=</span><span class="s">'Download objects and refs from another repository'</span><span class="p">)</span>
        <span class="c1"># NOT prefixing the argument with -- means it's not optional
</span>        <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'repository'</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
        <span class="k">print</span> <span class="s">'Running git fetch, repository=%s'</span> <span class="o">%</span> <span class="n">args</span><span class="p">.</span><span class="n">repository</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">FakeGit</span><span class="p">()</span></code></pre></figure>

The argparse library gives you all kinds of great stuff. You can run `./git.py --help` and get the following:

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">usage: git &lt;<span class="nb">command</span><span class="o">&gt;</span> <span class="o">[</span>&lt;args&gt;]

The most commonly used git commands are:
   commit     Record changes to the repository
   fetch      Download objects and refs from another repository

Pretends to be git

positional arguments:
  <span class="nb">command     </span>Subcommand to run

optional arguments:
  <span class="nt">-h</span>, <span class="nt">--help</span>  show this <span class="nb">help </span>message and <span class="nb">exit</span></code></pre></figure>

You can get help on a particular subcommand with `./git.py commit --help`.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">usage: git.py <span class="o">[</span><span class="nt">-h</span><span class="o">]</span> <span class="o">[</span><span class="nt">--amend</span><span class="o">]</span>

Record changes to the repository

optional arguments:
  <span class="nt">-h</span>, <span class="nt">--help</span>  show this <span class="nb">help </span>message and <span class="nb">exit</span>
  <span class="nt">--amend</span></code></pre></figure>

Want bash completion on your awesome new command line utlity? Try [argcomplete](https://github.com/kislyuk/argcomplete), a drop in bash completion for Python + argparse.
---
title: Reload Python inside uWSGI as fast as humanely possible
tags: python uwsgi
---

The Django development web server you get when you execute `./manage.py runserver` optimizes for one thing; fast hot reloading when you change your Python code. It does almost nothing else well, by design.

>DO NOT USE THIS SERVER IN A PRODUCTION SETTING. It has not gone through security audits or performance tests. (And that’s how it’s gonna stay. We’re in the business of making Web frameworks, not Web servers, so improving this server to be able to handle a production environment is outside the scope of Django.)...
>The development server automatically reloads Python code for each request, as needed. You don’t need to restart the server for code changes to take effect. -  [The Django Docs](https://docs.djangoproject.com/en/dev/ref/django-admin/#runserver-port-or-address-port)

If you're running [uWSGI](http://projects.unbit.it/uwsgi/) in production, you may decide that you want to run it in development, as well. But you'll quickly notice that by default, code is not hot-reloaded in uWSGI. You can enable that with the [py-autoreload setting](http://uwsgi-docs.readthedocs.org/en/latest/Options.html#py-auto-reload-py-autoreload-python-auto-reload-python-autoreload). It works by polling the code every X seconds.

Polling is already not ideal, but the story gets even worse if your code is on a remote file system, like an NFS share. This is a common setup if your development environment is running in a VM, like [Vagrant](http://www.vagrantup.com/). The problem is that polling is relatively slow over NFS. It can take 10 seconds on a medium sized code base to pick up changes. This is a bummer if you're coming from a virtually instantaneous runserver reload.

Recent versions of uwsgi also support [inotify](https://github.com/unbit/uwsgi-docs/blob/master/Changelog-1.9.14.rst#filesystem-monitoring-interface-fsmon) to pick up changes more quickly. But that also doesn't work over NFS.

The good news is that uWSGI also supports touch reloading. Basically you set a `touch-reload` file in your config, and if you do a `touch thatfile`, your Python code gets reloaded immediately. This works well over NFS because uWSGI polls that file much more quickly.

So how can we get an OSX host machine to update the touch file in the guest Linux host every time your code changes? Basically, you can use an inotify replacement for OSX called [fswatch](https://github.com/alandipert/fswatch). In this case, we're going to have it run a command to open a socket to the guest VM, and tell a custom daemon to touch the reload file.

First, you need to install fswatch locally.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">brew <span class="nb">install </span>fswatch</code></pre></figure>

Then, fork off a process watching your code directory, and pinging the remote daemon. I'm assuming that your VM has an entry in your hosts file called `vagrant-vm`. You can also use a static IP. The daemon will be listening on port 9001. *Note: fswatch does not currently take an extension filter, so this will restart uWSGI on ANY file change.*

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">fswatch ~/mycode <span class="s2">"echo '.' |nc vagrant-vm 9001"</span> &amp;</code></pre></figure>

Then, on the Linux VM guest, you fork off the daemon listener.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">nc <span class="nt">-l</span> 9001 <span class="nt">-k</span> <span class="o">&gt;</span> ~/mycode/.reload</code></pre></figure>

This is assuming you have the uWSGI `touch-reload` file set to `~/mycode/.reload`. That's it! When you update a file in `~/mycode` locally, you should see the following immediately in the uWSGI logs:

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">...gracefully killing workers...
Gracefully killing worker 1 <span class="o">(</span>pid: 25505<span class="o">)</span>...
worker 1 buried after 1 seconds
binary reloading uWSGI...
...
WSGI app 0 <span class="o">(</span><span class="nv">mountpoint</span><span class="o">=</span><span class="s1">''</span><span class="o">)</span> ready <span class="k">in </span>0 seconds on interpreter 0x239b270 pid: 18792 <span class="o">(</span>default app<span class="o">)</span>
gracefully <span class="o">(</span>RE<span class="o">)</span>spawned uWSGI master process <span class="o">(</span>pid: 18792<span class="o">)</span>
spawned uWSGI worker 1 <span class="o">(</span>pid: 25512, cores: 1<span class="o">)</span>
Sun Mar 30 18:01:26 2014 - <span class="o">[</span>emperor] vassal is ready to accept requests
Python auto-reloader enabled</code></pre></figure>
---
title: Loading classes from modules with reflection in Python (imp module)
tags: python
---

For a dynamic language, it's more difficult than it needs to be to import a module dynamically in Python. It's very easy to just `from foo import bar`, but what if you want to load a list of things and all you have is a string representation of each one, for example `foo.bar`?

One use case for this is for configuration. Django uses this pattern to initialize apps via its `INSTALLED_APPS` setting. For example, the default settings looks like this:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">INSTALLED_APPS</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s">'django.contrib.admin'</span><span class="p">,</span>
    <span class="s">'django.contrib.auth'</span><span class="p">,</span>
    <span class="s">'django.contrib.contenttypes'</span><span class="p">,</span>
    <span class="s">'django.contrib.sessions'</span><span class="p">,</span>
    <span class="s">'django.contrib.messages'</span><span class="p">,</span>
    <span class="s">'django.contrib.staticfiles'</span><span class="p">,</span>
    <span class="s">'polls'</span><span class="p">,</span>
<span class="p">)</span></code></pre></figure>

At some point, Django does the equivalent of a `from django.contrib import admin` and then starts poking around for `urls` and `models` modules. I'm guessing this is done primarily to avoid circular import issues; if Django imports your apps as normal, but your apps turn around and import Django, then you've got a problem.

I wanted to reproduce this pattern myself, and it was a little harder than I expected. Python provides an [imp](https://docs.python.org/2/library/imp.html) for just this occasion. But, from the docs:

>>> This function does not handle hierarchical module names (names containing dots). In order to find P.M, that is, submodule M of package P, use find_module() and load_module() to find and load package P, and then use find_module() with the path argument set to P.__path__. When P itself has a dotted name, apply this recipe recursively.

So, it's basically is a pain the balls to deal with. Here is a working example:

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">imp</span>


<span class="n">THINGS_TO_IMPORT</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s">'utils.misc.MyClass1'</span><span class="p">,</span>   <span class="c1"># class
</span>    <span class="s">'utils.misc.foo'</span><span class="p">,</span>  <span class="c1"># constant
</span>    <span class="s">'utils.bar'</span><span class="p">,</span>  <span class="c1"># from __init__.py
</span>    <span class="s">'utils.misc'</span><span class="p">,</span>  <span class="c1"># module
</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">import_from_dotted_path</span><span class="p">(</span><span class="n">dotted_names</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">""" import_from_dotted_path('foo.bar') -&gt; from foo import bar; return bar """</span>
    <span class="n">next_module</span><span class="p">,</span> <span class="n">remaining_names</span> <span class="o">=</span> <span class="n">dotted_names</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'.'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">fp</span><span class="p">,</span> <span class="n">pathname</span><span class="p">,</span> <span class="n">description</span> <span class="o">=</span> <span class="n">imp</span><span class="p">.</span><span class="n">find_module</span><span class="p">(</span><span class="n">next_module</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">imp</span><span class="p">.</span><span class="n">load_module</span><span class="p">(</span><span class="n">next_module</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">pathname</span><span class="p">,</span> <span class="n">description</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">remaining_names</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">remaining_names</span><span class="p">)</span>
    <span class="k">if</span> <span class="s">'.'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">remaining_names</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">module</span>
    <span class="k">return</span> <span class="n">import_from_dotted_path</span><span class="p">(</span><span class="n">remaining_names</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="n">module</span><span class="p">.</span><span class="n">__path__</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">name_of_thing</span> <span class="ow">in</span> <span class="n">THINGS_TO_IMPORT</span><span class="p">:</span>
        <span class="n">thing</span> <span class="o">=</span> <span class="n">import_from_dotted_path</span><span class="p">(</span><span class="n">name_of_thing</span><span class="p">)</span>
        <span class="k">print</span> <span class="s">'%s =&gt; %r'</span> <span class="o">%</span> <span class="p">(</span><span class="n">name_of_thing</span><span class="p">,</span> <span class="n">thing</span><span class="p">)</span></code></pre></figure>

And the output as expected:

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">utils.misc.MyClass1 <span class="o">=&gt;</span> &lt;class <span class="s1">'misc.MyClass1'</span><span class="o">&gt;</span>
utils.misc.foo <span class="o">=&gt;</span> <span class="o">{</span><span class="s1">'bar'</span>: 7<span class="o">}</span>
utils.bar <span class="o">=&gt;</span> <span class="s1">'foo'</span>
utils.misc <span class="o">=&gt;</span> &lt;module <span class="s1">'utils'</span> from <span class="s1">'myapp/utils/__init__.pyc'</span><span class="o">&gt;</span></code></pre></figure>

Here is a directory listing of my test modules, in case this isn't clear.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">./utils/__init__.py:

bar <span class="o">=</span> <span class="s1">'foo'</span>


./utils/misc.py:

class MyClass1<span class="o">(</span>object<span class="o">)</span>:
    pass

foo <span class="o">=</span> <span class="o">{</span><span class="s1">'bar'</span>: 7<span class="o">}</span></code></pre></figure>
---
title: Better debugging through healthy living
tags: process debugging reading-list
---

Debugging code is more of an art than a science. It's an art form that you will have plenty of time to practice as a software engineer.

> Reworking defects in requirements, design, and code consumes 40-50% of the total cost of software development. - [Improving Software Productivity](http://programmers.stackexchange.com/questions/91758/debugging-facts-and-statistics)


## Fallacies about bugs

Bugs are failures. Failures to predict a scenario, or failures in assuming that this case would never happen. The first step is admitting that you have a problem. Related cognitive dissonance:

1. "It's not my fault." False. 99% of the time, it is your fault.
2. "It worked yesterday, and nothing changed." Yes, something changed. You just don't know what. It could be a change in data or a change in a dependency, but something changed.
3. "It must be the tools/frameworks/universe out to get me." See #1. Don't attribute to malice what can be explained by your own stupidity.
4. "It's a bug, but it's not MY fault". Famous last words.

## Debugging Mindset

Don't go into a debugging session looking to assign blame to someone else (see #4). Most bugs are caused by assumptions, your first task is to consciously question the assumptions are you making as you read the code. Don't assume a variable has the value you think it does. Don't assume the database returned the data you're expecting. Verify. Contrary thinking is useful here.

This includes being skeptical of the assumptions made in the original bug report. Whether it's from a customer or a QA person or another developer, follow the creed "trust, but verify".

## Before you start

In fact, before you even begin looking at code, make sure you can reproduce the issue. 98% of the time, a bug should have a repo case. Every issue can be reproduced, it's just a matter of isolating the variables. That said, in the 2% case where coming up with a repo case is too difficult, it's reasonable to start debugging without one. But you should expend a serious amount of effort yourself trying to reproduce it before you give up on a repo case. After all, if you have no repo case, how are you going to know if you fixed it?

Once you have a repo case, trim it down to the bare essentials. Try reproducing the issue without some of the initial steps, and see if it still happens. If so, omit those steps. For more specific particulars of the repo case, make sure that it does not also happen in the general case; those too are erroneous details that can cloud your thinking.

## Techniques

The basic debugging stages, in order, are:

1. Reproduce. Don't skip this step. Trust but verify.
2. Gathering info (stack traces, logs, screen shots, IDs, accounts, etc.)
3. Isolating the code causing the problem. It was OK up until a particular line of code.
4. Fix it. Usually this is easy after you know what the problem is.

Stack traces are you best friend here. In many cases, they are all you need to isolate the issue to a single line of code. At worst, you've isolated the issue to something that calls this code. But if you don't have a stack trace, what do you do?

Step through the code. This is boring, but extremely time efficient. It's the fastest way to validate your assumptions. You can speed things up significantly with some intelligent guessing about your initial breakpoint. This is where your intuition comes in.

Sometimes you can't step through the code. Maybe you're in an older web browser that doesn't support breakpoints. Maybe you're trapped in 1999 debugging PHP 3. Whatever. You can simulate stepping through the code with logging, print statements, alert() calls, etc.

A couple of other techniques that sometimes come in handy:

- Binary search/bisect. If you've isolated it down to a piece of offending code that's fairly long, you can try deleting half of the code and see if the problem still happens. If it does, you know the problem was with the other half. Repeat. Of course, this is only applicable in the cases where deleting a large chunk leaves you with a viable running program.
- Rubber duck debugging. This is a good one. If you're ever really stuck, trying explaining the problem to someone else. If no one is around, try explaining it to an inanimate object. You might be surprised how often this leads you to a solution.

##  Debugging Tools

What tools should you use when debugging? Anything you can get your hands on. Graphical IDEs with debuggers are great, but not necessary. Their primary utility is that they let you "explore" the variables in scope, and evaluate expressions. You can get the same thing out of a good interactive interpreter.

No matter which tools you use, you want to get your "debug cycle" as fast as possible. This is the time it takes between you changing the code and running through the entire repo case to see if it's still happening. Again, the ultimate short debug cycle is being able to repo the issue with a single statement in an interactive interpreter.

If you have used a language with an interactive shell such as python, node, etc, you will miss them when you're using a language that doesn't have them such as Java, C#, etc.

## Skills

Personally, I feel that generalists are better at debugging. The more pieces of the software stack you understand, the quicker you will be able to isolate where the issue is occurring. General experience in a particular code base or stack will similarly speed your debugging. Unfortunately, both of those skills simply take time to develop.

## Prevention

Because debugging does take a significant amount of time, anything you can do to reduce it will pay off. Writing good unit tests are huge here. They will help you think about the assumptions you are making when you're writing the code, as well as force you to explicitly deal with invalid assumptions.
---
title: Distributing Vagrant base boxes securely
tags: vagrant
---

[Vagrant](http://www.vagrantup.com) is typically used to provide local, uniform and *repeatable* development virtual machines. Repeatable is given little importance in the documentation, but since I've been using vagrant, I've been surprised at just how often I end up destroying and re-creating my boxes.

# Why custom base boxes

Our typical provisioning takes about 30 minutes. Half of that is downloading and installing `apt` dependencies, and half is downloading, compiling (ugh) and installing `pip` dependencies for Python. What's worse, it's not particularly robust; compiling Python libraries will fail approximately 10% of the time. Just enough to be really annoying.

Base boxes to the rescue. The included Ubuntu boxes `precise32` and `precise64` are about 300MB, and contain only the operating system and minimal other dependencies. But you can build your own base box files from a previously configured virtual machine.

# Shrinking images

Running the standard `vagrant package --output /tmp/mydev.box` command creates a base box of 2.5GB in my case. How did we go from 300MB to 2.5GB? It's actually mostly bloat. You can save quite a bit of space by running [this gist](https://gist.githubusercontent.com/adrienbrault/3775253/raw/da59136ef0414af151b917bd25a06882f0107947/purge.sh) inside your VM *before* you package it.

In my case, this trimmed it down to 800MB.

# Do you want your dev image public?

In our case, we didn't want to make this image publicly available. Although the box does not have actual code on it (it's loaded later via NFS), it *does* have development API keys/passwords. Why? A while back we removed these from the github repo and moved them into `/etc` config files. Besides, I'm not sure we want to make anything about our service config public, either.

This is a problem because Vagrant assumes your `box_url` is going to be using HTTP/HTTPS. It does support Basic Authentication, but that's hardly secure.

# What about Vagrant cloud?

Vagrant is starting a service called [Vagrant Cloud](https://vagrantcloud.com/) aimed precisely at this use case. However, it's in early beta, and doesn't support actually hosting files at this stage. You need to upload your box somewhere else and give them a URL, so you're back to square one.

# Plugins

I could not find a vagrant plugin for my use case, either. There is an interesting [vagrant-fog-box-storage](https://github.com/natlownes/vagrant-fog-box-storage) plugin that allows you to download a box image off S3. However, it doesn't work with Vagrant versions past 1.1. Plus, I'm pretty sure I don't want to include AWS access tokens in my Vagrantfile.

I'm also *very* sure I don't want to write by own plugin in ruby.

# Vagrant actually uses cURL

While searching around, I randomly ran into the vagrant [download code](https://github.com/mitchellh/vagrant/blob/master/lib/vagrant/util/downloader.rb), where I quickly realized they were using cURL to do the actual download. Great! A little known feature of cURL is that it supports many different protocols: `dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtsp scp sftp smtp smtps telnet tftp`.

This actually turns out to be a [concious decision](https://github.com/mitchellh/vagrant/pull/1041) to support scp. All you have to do is use a scp URL like `scp://my-server:/tmp/dev.box`.

# cURL doesn't have this option by default in OSX

One big problem. Our development machines are all Apples, and the `curl` command built-in to OSX Mavericks is not compiled with the scp option. So we turn to homebrew.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">brew <span class="nb">install </span>curl <span class="nt">--with-ssh</span>
brew <span class="nb">link </span>curl <span class="nt">--force</span></code></pre></figure>

**Note: This will over-ride the built-in curl, which is fairly dangerous. If the arguments don't match something another app on your system is expecting, things could very well break. [Learn more.](http://stackoverflow.com/questions/4691403/keg-only-homebrew-formulas)**

# Example vagrant file

Using a scp URL in your `Vagrantfile` is simple:

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># -*- mode: ruby -*-</span>
<span class="no">Vagrant</span><span class="p">.</span><span class="nf">configure</span><span class="p">(</span><span class="s2">"2"</span><span class="p">)</span> <span class="k">do</span> <span class="o">|</span><span class="n">config</span><span class="o">|</span>
  <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">hostname</span> <span class="o">=</span> <span class="s2">"dev-vagrant"</span>
  <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">box</span> <span class="o">=</span> <span class="s2">"dev"</span>
  <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">box_url</span> <span class="o">=</span> <span class="s2">"scp://my-server:/tmp/dev.box"</span>
<span class="k">end</span></code></pre></figure>

# Future Work

Ideally, Vagrant itself would optionally call `scp` directly to side-step the OSX issue. It would also be ideal if they supported S3 directly, as well. In fact, just let us write our own shell command/script to download the file.
---
title: Rich client side web apps gone too far!
tags: web reading-list newboss
---

## I have seen some Javascript in my time...

I've been getting paid to write web apps using Javascript for about 12 years. I
wrote a single page web based email client in 2003, years before the launch of
Gmail. I wrote a javascript instant message client for IE 5.5. I wrote a full
window manager inside the browser, complete with a dock, using IFRAMEs.

All of that stuff was extremely painful to get working back in the dark ages
before prototype and jQuery. One browser in particular absolutely sucked to write
Javascript for. Never mind debug tools, we didn't even have stack traces! About a
dozen times a day you would get an error that would just say "Object doesn't
support this property or method". That was literally it - no line number or
anything.

All this is to say that Javascript got a bad reputation. A lot of that was not
due to the language, but the platform. In recent years, the platform has gotten
a lot of love as we realized we are going to be stuck with this language. Tooling
has smoothed out remaining browser inconsistencies. But don't fool yourself; the
language still sucks.


## ... and I don't like what I see

It's just not a well designed language. Basic object comparison frequently results
in [mind-blowing violations of the principle of least astonishment](http://wtfjs.com/).
We have piled dozens of different and mutually exclusive hacks on top of the
language to support name spacing. To this day, I still stumble over the fact that
`this` will frequently not be pointing to what you think it is.

Which is why it is so unfortunate that this is the defacto language of the future.
It's the only option for cross browser client side code. Literally every other
option does not work with all browsers or is basically a language that's compiled
down to Javascript. Good luck debugging that.

To add insult to injury, web development has become more and more dominated by
rich client side work over the last 5 years. Javascript used to be a tool that
you used as sparingly as possible, sprinkling in just the barest hint of dynamic
behavior where it was absolutely necessary. Now it's common to start a new project
and assume that 50% or more of the code will be Javascript.


## At least make your fancy app act like a web page

Of course, everyone wants web apps that are responsive and interactive. For apps
like the aforementioned instant messenger and email clients, that makes total
sense. But 90% of the stuff out there does not fall under the same use case. Most
apps that are heavily Javascript based are just doing it wrong, in my opinion.
That's not how a web browser is *supposed* to work, a distinction obviously lost on
the authors of these applications when they continually break basic functionality
that we used to be able to take for granted.

I'm sure you've seen sites that refuse to record their state in the URL. Oh thanks,
I didn't want to bookmark this anyway. Too often these days I can't even tab through
form fields. If for some bat-shit crazy reason you find yourself using editable
`DIV` elements instead of actual form inputs, please at least remember to provide
the functionality we have been using since the '80s.

Then there are willful substitutions of the developer's judgement for that of the user.
I have never actually wanted a modal window to appear. Every time I see one, I just think
"well, fuck you too". Maybe you have alt-clicked on a link somewhere
only to have the link open in the current tab instead - or perhaps not open at all.
Presumably there was a good reason to re-implement the wheel by creating your own
special thing that's text you can click on, but that is not a real `A` element. NOT!

The 10th circle of hell should be for web developers who break the back button.


## How we ended up here

My pet theory is that the rise of dominance of rich client side apps is actually
mostly an accident of history. Right around 2005, with the launch of Rails and Django,
we had finally come up with ways to write server-side web apps that were not like
sticking a fork in your eye. We finally had separation of controller and view, along
with features like flash messages, session variables, and form validation, which
made it easy to implement a rich interaction with state persisted between server
side renders. On broadband, the server round-trips were so fast it felt like you
were serving up these dynamic pages from your local machine.

But at the same time, mobile was just starting to come on. All these pages we were
writing that rendered quickly on broadband absolutely sucked on mobile. No matter
how fast your server was, latency was killing you on these shit early wireless
networks. But we had a few years of AJAX under our belts, so we knew we could
deliver the illusion of responsiveness by simply not reloading the page.

I remember trying to write an app for IE7 that was 100% rendered on the client
side from a pure JSON server backend. That shit was ridiculously slow back in the
day on a broadband connected PC simply because IE7 could not render a decently
complicated HTML chunk into a previously rendered DOM without going out to lunch
for several hundred milliseconds doing god knows what - while at the same time
completely locking the UI of the browser.

But it was actually *better* on mobile. The latency was such a killer that even
though phones were even slower at rendering Ajax responses, the over-all UX felt
more responsive. So we started boldly going into the future of 100% javascript
apps.

Then a funny thing happened. Mobile users roundly rejected web apps. These days,
you are an absolute second class citizen on mobile if you don't have a native app.
Even Apple got this wrong. Their initial plan for the iPhone was to only have web
apps at the platform for third-party developers. IMO, for all our progress in
the last 5 years, wireless networks and phones are still shit, at least compared
to where we hope they will be in 10 years. Maybe then we will finally be able to
deliver write once run anywhere mobile web apps.


## Back to the future

The latest generation of developers don't even realize there is another way to
write web apps. Instead of writing just server side code, HTML and CSS, it's accepted
that we also need to write a JSON REST API, client side templates, duplicate client
side models and a whole mess of Javascript view code, hopefully on some kind of
sane framework.

I'm here to tell you that you're doing a lot more work this way. Most apps can
provide the same experience with about 5% of the client side code. Plus, you're
slowing down your development team throughput significantly. All that extra
complexity is not free. All of a sudden you have to hire people who specialize
in front-end and people who specialize in back-end. It used to be that virtually
all web devs could do both well.

The key to delivering awesome client server HTML page render apps is graceful
degradation, also called [progressive enhancement](http://en.wikipedia.org/wiki/Progressive_enhancement).
The basic idea is that your app should be fully functional even if Javascript is
disabled. This also is the thing that saves your ass when your giant blob of
minified JS is totally broken; at least the app still works.

Now if you deliver apps this way, it is absolutely critical that you serve
page renders fast. With client-side apps this is not as critical, which means
people have been getting lazy on this.

Besides development speed, you get a host of other benefits. Code is less
complicated. I have seen over and over again in my work that UI event code tends
to be the hardest to write and maintain. If it's not total spaghetti, you're doing
pretty well. Don't write that category of code if you don't have to. Your app
will be easier to debug; no more hunting down what view is calling what API to
deliver what data to your browser in a given HTML chunk. *Side note: for the love
of god put model IDs and other human-readable attributes in your client-side
generated DOM if you value your sanity*. You also reduce the compatibility
issues between browsers to relatively simple CSS differences.

Now get off my lawn!

&gt; END RANT &lt;

*Edit:* This just got posted to Hacker News. Thanks! I guess this hit a minor chord.
If you want to hear more, I'm going to do a whole episode on it on my new podcast,
the [Software Engineering Fireside Chat](http://t.co/jqnKhxF1K8). Look for it next Friday.
---
title: Debugging an IE7 browser crash (manual git bisect)
tags: ie7 javascript git
---

Every once in a while, you have to put in a heroic effort to diagnose a bug. When
you finally figure it out, you want to run around the office singing "We are the
champions", even if it turns out to be a trivial issue. Because that doesn't mean
it took a trivial amount of effort.

Recently I had a bug report cross my desk that our web app was crashing IE7.
Luckily IE7 is only about 3% of our overall userbase. As of May 2014, it's only
[about .2% of global browser market share](http://www.w3schools.com/browsers/browsers_explorer.asp),
but we have a lot of enterprise customers who are slower to upgrade. Regardless,
definitely a high severity bug.

As an aside, I find browser crashes to be ridiculous. I know this browser was
written back in the dark ages of 2005, but still. If you're implementing a browser,
you know you are basically going to be taking random markup and javascript from
essentially an untrusted user. You would think you would be as meticulous as
possible to trap every conceivable exception.

But no. In the case of a browser crash, you don't get a stack trace. You don't even
get the dreaded "Object Expected" error (the single most useless error of all time).
What puny developer tools that Microsoft deigned to provide back in 2005 are of
absolutely no use. Instead, all you see is a white screen and the following:

![ie7 not responding](/blog/images/ie7_not_responding.png)

So it begins. In this case, I assume that we were not crashing IE7 at some point in
the recent past. Otherwise, we would have presumably received this bug report
earlier.

The first step is always to get a reproducible. I can reproduce using my trusty
IE7 VirtualBox image provisioned from [xdissent/ievms](https://github.com/xdissent/ievms).
Knowing I'm in for a long haul, I also make some effort to get my reproducible steps
down to the smallest quickest nugget I can. First, I create a shortcut on my Windows
desktop called "clear cache", with the following Target: `RunDll32.exe InetCpl.cpl,ClearMyTracksByProcess 255`.
I know from experience that if you don't clear your cache between tests, you will be
hunting phantom cached files for hours. I make another shortcut with a direct link
to the page that crashes on my dev box.

I proceed to spend about an hour deleting random chunks of suspect code to see if I
can get it to stop crashing. If I'm successful, I start putting some of the deleted
code back in until it crashes again. This is a slow process. Sometimes it helps to
go old school and put actual `alert()` calls in between suspect lines. I finally
narrow it down to a require.js call to import a completely different file. Basically,
I have to start over on a new file.

So I change tactics and start what is essentially a binary search through our git
history to locate the exact commit where this occurs. *Note: if you can repo your
issue purely on the command line, you should use git bisect for this*.  I start
rolling back 10, 100, 1000 commits until I get a working build. I have to manually
refresh a browser each time.

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">git checkout upstream/master~10
git checkout upstream/master~100
git checkout upstream/master~1000</code></pre></figure>

At 1,000 commits, I get a working build again. So I try 500 commits ago. Not working.
750 commits. Working. 625 commits, etc... After a surprisingly few iterations, I'm
left with a small set of potential breaking commits, which I manually browse through until
I see this:

![ie7 extra comma](/blog/images/ie7_extra_comma.png)

From experience, I suspected all along that this was likely either an unclosed HTML
tag in a javascript template, or an extra comma. I have seen both [crash old versions](http://blog.bottomlessinc.com/2010/01/ie-bug-dont-put-extra-commas-in-your-js-arrays/)
of IE. What's super-frustrating is that this is actually something that our automation
catches:

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="o">&gt;</span><span class="nx">jshint</span> <span class="nx">bad_file</span><span class="p">.</span><span class="nx">js</span>
<span class="kd">static</span><span class="sr">/js/</span><span class="nx">bad_file</span><span class="p">.</span><span class="nx">js</span><span class="p">:</span> <span class="nx">line</span> <span class="mi">611</span><span class="p">,</span> <span class="nx">col</span> <span class="mi">45</span><span class="p">,</span> <span class="nx">Extra</span> <span class="nx">comma</span><span class="p">.</span> <span class="p">(</span><span class="nx">it</span> <span class="nx">breaks</span> <span class="nx">older</span> <span class="nx">versions</span> <span class="k">of</span> <span class="nx">IE</span><span class="p">)</span></code></pre></figure>

But we rely on individual engineers to install the automation and run it, which they
can also opt-out on for individual commits and files. In any case, it turned out to be
a one character fix after about 3 hours of work. In the future, I would save some
time by just running jshint on the entire repo.
---
title: Enabling SOLR autocommit with a custom Haystack backend
tags: python haystack solr
---

By default [Django Haystack](http://haystacksearch.org/) makes updates to your Solr index available for
searching immediately. It does this in the simplest way possible, it commits every single update individually.
That can be quite slow. I have an index with 35 million records, and under heavy write load commits of 1,000
records can slow down and take up to 5 seconds for each chunk. In extreme cases, Solr can refuse to accept
that much write load at once, and throw an exception like the following:

<figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="cp">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span>
<span class="nt">&lt;response&gt;</span>
    <span class="nt">&lt;lst</span> <span class="na">name=</span><span class="s">"responseHeader"</span><span class="nt">&gt;</span>
        <span class="nt">&lt;int</span> <span class="na">name=</span><span class="s">"status"</span><span class="nt">&gt;</span>503<span class="nt">&lt;/int&gt;</span>
        <span class="nt">&lt;int</span> <span class="na">name=</span><span class="s">"QTime"</span><span class="nt">&gt;</span>1492<span class="nt">&lt;/int&gt;</span>
    <span class="nt">&lt;/lst&gt;</span>
    <span class="nt">&lt;lst</span> <span class="na">name=</span><span class="s">"error"</span><span class="nt">&gt;</span>
        <span class="nt">&lt;str</span> <span class="na">name=</span><span class="s">"msg"</span><span class="nt">&gt;</span>Error opening new searcher. exceeded limit of maxWarmingSearchers=2, try again later.<span class="nt">&lt;/str&gt;</span>
        <span class="nt">&lt;int</span> <span class="na">name=</span><span class="s">"code"</span><span class="nt">&gt;</span>503<span class="nt">&lt;/int&gt;</span>
    <span class="nt">&lt;/lst&gt;</span>
<span class="nt">&lt;/response&gt;</span></code></pre></figure>

Investigating this error, I turned up a [Stackoverflow post](http://stackoverflow.com/questions/7512945/solr-error-opening-new-searcher-exceeded-limit-of-maxwarmingsearchers-2-try)
basically saying to not make so many commits. That turned up a [Haystack pull request](https://github.com/toastdriven/django-haystack/pull/624)
to make manual commits optional.

You can see the basic issue by looking at the logs that Haystack creates each time it issues a write request to the
Solr REST API:

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">Finished <span class="s1">'http://localhost:8080/solr/my_index/update/?commit=true'</span> <span class="o">(</span>post<span class="o">)</span> with body <span class="s1">'u'</span>&lt;add&gt;...<span class="s1">' in 0.010 seconds.</span></code></pre></figure>

As of Solr 4.0, we have much more performant options for bulk indexing. A [common setup](http://wiki.apache.org/solr/NearRealtimeSearch)
is to use `autocommit` (set by default to 15 seconds) and abstain from manually committing by passing `commit=false` on
the REST API URL. Though Haystack supports passing a commit boolean to the various back-end implementations of `update`,
`remove` and `clear`, this parameter is never explicitly set. Instead, you can [implement your own](http://www.wellfireinteractive.com/blog/custom-haystack-elasticsearch-backend/)
search back-end subclass to pass this value.


<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">haystack.backends.solr_backend</span> <span class="kn">import</span> <span class="n">SolrEngine</span><span class="p">,</span> <span class="n">SolrSearchBackend</span>


<span class="k">class</span> <span class="nc">AutoCommitSolrSearchBackend</span><span class="p">(</span><span class="n">SolrSearchBackend</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">iterable</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AutoCommitSolrSearchBackend</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">update</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">iterable</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="n">commit</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">remove</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_or_string</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AutoCommitSolrSearchBackend</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">remove</span><span class="p">(</span><span class="n">obj_or_string</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="n">commit</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">clear</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">models</span><span class="o">=</span><span class="p">[],</span> <span class="n">commit</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AutoCommitSolrSearchBackend</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">clear</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="n">commit</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AutoCommitSolrEngine</span><span class="p">(</span><span class="n">SolrEngine</span><span class="p">):</span>
    <span class="s">''' the built-in Solr engine in Haystack performs a manual commit after each update/add/remove/clear. This
    is really slow. Solr is configured by default to auto-commit changes every 15 seconds, so there is no need to
    commit manually on every update.
    '''</span>
    <span class="n">backend</span> <span class="o">=</span> <span class="n">AutoCommitSolrSearchBackend</span></code></pre></figure>

Then you can use this new `AutoCommitSolrEngine` in your `HAYSTACK_CONNECTIONS` setting.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">HAYSTACK_CONNECTIONS</span> <span class="o">=</span> <span class="p">{</span>
     <span class="s">'default'</span><span class="p">:</span> <span class="p">{</span>
         <span class="s">'ENGINE'</span><span class="p">:</span> <span class="s">'myapp.serach.AutoCommitSolrEngine'</span><span class="p">,</span>
         <span class="s">'URL'</span><span class="p">:</span> <span class="s">'http://localhost:8080/solr/my_index'</span><span class="p">,</span>
     <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

**Note: By default, indexed items will not show up in searches right away. That's what soft-commit is for.**

> Hard commits are about durability, soft commits are about visibility. [Understanding Transaction Logs, Soft Commit and Commit in SolrCloud - Erick Erickson](http://searchhub.org/2013/08/23/understanding-transaction-logs-softcommit-and-commit-in-sorlcloud/)

To make your auto-committed items available to search in a timely fashion, you must set a `autoSoftCommit.maxTime`
in your Solr config. This is *NOT* set by default.

<figure class="highlight"><pre><code class="language-xml" data-lang="xml">    <span class="c">&lt;!-- softAutoCommit is like autoCommit except it causes a
         'soft' commit which only ensures that changes are visible
         but does not ensure that data is synced to disk.  This is
         faster and more near-realtime friendly than a hard commit.
      --&gt;</span>
    <span class="nt">&lt;autoSoftCommit&gt;</span>
      <span class="nt">&lt;maxTime&gt;</span>1000<span class="nt">&lt;/maxTime&gt;</span>
    <span class="nt">&lt;/autoSoftCommit&gt;</span></code></pre></figure>

Alternately, you can set `autoCommit.openSearcher` to `true`, which will cause a new searcher worker to be instantiated
every time you auto-commit. This could slow down the first searches that come in after an auto commit, however.
---
title: Designing a Dimensional Schema for a hierarchy that can change over time
tags: datawarehouse
---

I've recently been reading about dimensional schemas, aka star schemas. The whole idea seems to be to optimize for fast queries that are also simple to write, at the expense of extra storage in the form of denormalization. In contrast to standard third normal form relational database schema, there is little to no emphasis on how complicated it will be to write data into the system. This trade-off makes sense, as star schemas are employed primarily for data warehouses, where the user will be a business intelligence analyst writing ad-hoc queries to pull analytics, not an ORM layer or another piece of pre-vetted code hitting previously defined and approved indexes.

The schema itself can be counter intuitive for engineers who have only worked on normal RDMS schemas. First off, data is broken down into fact tables, which are the events that the analytics are aggregating, and dimension tables, which are the attributes that the events will be rolled up by. For example, we are going to use the fact of social media posts as an example, while the dimensions will be the group that the posting user works at in their company, together with that groups place in the company org chart. To up the level of difficulty, we are also going to talk about how to model an org chart that is changing over time.


## The Problem

![hierarchy](/blog/images/hierarchy.png)

Here we have a somewhat complicated org chart, albeit a small one. Group 10 happens to roll up to both the State and Role hierarchies. This is known as a multi-hierarchy system. The company wants to be able to report on posts made by that group from two different perspectives, depending on the specific report they are trying to run.

Two particular challenges talked about in the excellent [The Data Warehouse ETL Toolkit](http://www.amazon.com/The-Data-WarehouseETL-Toolkit-Techniques/dp/0764567578) are modeling hierarchies (which are essentially trees, and also difficult to model in traditional RDMS systems) and slowly changing dimensions. For example, what happens if a group moves to a different spot in one of the hierarchies, but the company still wants to report on their posts according to the hierarchy they were actually in at the time? More on that later.


## Test Schema and Data

Here is one model for the hierarchy information, aka the Region Bridge Table, or `test_region_bridge`. For each of these tables, I will also give the SQL to create the table and insert the same test data.

<table>
    <thead>
        <th>parent_distdict_id</th>
        <th>parent_distdict_name</th>
        <th>child_distdict_id</th>
        <th>child_distdict_name</th>
        <th>depth_from_parent</th>
    </thead>
    <tr>
        <td>1</td>
        <td>All Regions</td>
        <td>2</td>
        <td>State Hierarchy</td>
        <td>1</td>
    </tr>
    <tr>
        <td>1</td>
        <td>All Regions</td>
        <td>5</td>
        <td>MA</td>
        <td>2</td>
    </tr>
    <tr>
        <td>2</td>
        <td>State Hierarchy</td>
        <td>5</td>
        <td>MA</td>
        <td>1</td>
    </tr>
    <tr>
        <td>1</td>
        <td>All Regions</td>
        <td>3</td>
        <td>Role Hierarchy</td>
        <td>1</td>
    </tr>
    <tr>
        <td>1</td>
        <td>All Regions</td>
        <td>6</td>
        <td>Sales</td>
        <td>2</td>
    </tr>
    <tr>
        <td>3</td>
        <td>Role Hierarchy</td>
        <td>6</td>
        <td>Sales</td>
        <td>1</td>
    </tr>
    <tr>
        <td>1</td>
        <td>All Regions</td>
        <td>7</td>
        <td>Eng</td>
        <td>2</td>
    </tr>
    <tr>
        <td>3</td>
        <td>Role Hierarchy</td>
        <td>7</td>
        <td>Eng</td>
        <td>1</td>
    </tr>
</table>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_region_bridge</span> <span class="p">(</span><span class="n">parent_district_id</span> <span class="nb">integer</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span> <span class="n">parent_district_name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">255</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span> <span class="n">child_district_id</span> <span class="nb">integer</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span> <span class="n">child_district_name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">255</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span> <span class="n">depth_from_parent</span> <span class="nb">integer</span> <span class="k">not</span> <span class="k">null</span><span class="p">);</span>

<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">test_region_bridge</span> <span class="p">(</span><span class="n">parent_district_id</span><span class="p">,</span> <span class="n">parent_district_name</span><span class="p">,</span> <span class="n">child_district_id</span><span class="p">,</span> <span class="n">child_district_name</span><span class="p">,</span> <span class="n">depth_from_parent</span><span class="p">)</span>
           <span class="k">SELECT</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'All Regions'</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'State Hierarchy'</span><span class="p">,</span> <span class="mi">1</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'All Regions'</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">'MA'</span><span class="p">,</span> <span class="mi">2</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'State Hierarchy'</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">'MA'</span><span class="p">,</span> <span class="mi">1</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'All Regions'</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">'Role Hierarchy'</span><span class="p">,</span> <span class="mi">1</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'All Regions'</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">'Sales'</span><span class="p">,</span> <span class="mi">2</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">'Role Hierarchy'</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">'Sales'</span><span class="p">,</span> <span class="mi">1</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'All Regions'</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="s1">'Eng'</span><span class="p">,</span> <span class="mi">2</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">'Role Hierarchy'</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="s1">'Eng'</span><span class="p">,</span> <span class="mi">1</span><span class="p">;</span></code></pre></figure>

Notice that every node has a row for each of their children, whether they are direct children (depth 1) or further down. This is intended to make it possible to query for all groups under a certain level without explicitly listing all the children in a dependent sub-query. The parent and child names are given just to make it easier to see what your queries are doing.

Here is the `test_post` table. Notice that it has a `group_id`, but not a `district_id`.

<table>
    <thead>
        <th>post_id</th>
        <th>group_id</th>
        <th>text</th>
        <th>date</th>
    </thead>
    <tr>
        <td>1</td>
        <td>8</td>
        <td>foo</td>
        <td>2014-06-01</td>
    </tr>
    <tr>
        <td>2</td>
        <td>9</td>
        <td>bar</td>
        <td>2014-06-05</td>
    </tr>
    <tr>
        <td>3</td>
        <td>11</td>
        <td>bat</td>
        <td>2014-06-20</td>
    </tr>
    <tr>
        <td>4</td>
        <td>11</td>
        <td>baz</td>
        <td>2014-06-30</td>
    </tr>
    <tr>
        <td>5</td>
        <td>10</td>
        <td>far</td>
        <td>2014-06-10</td>
    </tr>
</table>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_post</span> <span class="p">(</span><span class="n">hss_post_id</span> <span class="nb">integer</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span> <span class="n">group_id</span> <span class="nb">integer</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span> <span class="nb">text</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">255</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span> <span class="nb">date</span> <span class="nb">timestamp</span> <span class="k">not</span> <span class="k">null</span><span class="p">);</span>

<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">test_post</span> <span class="p">(</span><span class="n">hss_post_id</span><span class="p">,</span> <span class="n">group_id</span><span class="p">,</span> <span class="nb">text</span><span class="p">,</span> <span class="nb">date</span><span class="p">)</span>
           <span class="k">SELECT</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="s1">'foo'</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2014-06-01'</span><span class="p">)</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="s1">'bar'</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2014-06-05'</span><span class="p">)</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="s1">'bat'</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2014-06-20'</span><span class="p">)</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="s1">'baz'</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2014-06-30'</span><span class="p">)</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">'far'</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2014-06-10'</span><span class="p">);</span></code></pre></figure>

Finally, here is the `test_group_region_bridge` table, which joins `group_id` to `district_id`:

<table>
    <thead>
        <th>group_id</th>
        <th>district_id</th>
    </thead>
    <tr>
        <td>8</td>
        <td>5</td>
    </tr>
    <tr>
        <td>9</td>
        <td>6</td>
    </tr>
    <tr>
        <td>10</td>
        <td>5</td>
    </tr>
    <tr>
        <td>10</td>
        <td>6</td>
    </tr>
    <tr>
        <td>11</td>
        <td>7</td>
    </tr>
</table>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_group_region_bridge</span><span class="p">(</span><span class="n">group_id</span> <span class="nb">integer</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span> <span class="n">district_id</span> <span class="nb">integer</span> <span class="k">not</span> <span class="k">null</span><span class="p">);</span>

<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">test_group_region_bridge</span> <span class="p">(</span><span class="n">group_id</span><span class="p">,</span> <span class="n">district_id</span><span class="p">)</span>
           <span class="k">SELECT</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">6</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">6</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">7</span><span class="p">;</span></code></pre></figure>


## Example Queries

Given this schema, how would we write a query to answer the question *How many posts were there in each region under the Role Hierarchy between Jun 1, 2014 and Jun 30, 2014?*

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_name</span><span class="p">,</span>
       <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
  <span class="k">FROM</span> <span class="n">test_post</span> <span class="n">p</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_group_region_bridge</span> <span class="n">gr</span>
    <span class="k">ON</span> <span class="n">gr</span><span class="p">.</span><span class="n">group_id</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">group_id</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_region_bridge</span> <span class="n">r</span>
    <span class="k">ON</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_id</span> <span class="o">=</span> <span class="n">gr</span><span class="p">.</span><span class="n">district_id</span>
 <span class="k">WHERE</span> <span class="n">r</span><span class="p">.</span><span class="n">parent_district_name</span> <span class="o">=</span> <span class="s1">'Role Hierarchy'</span>
   <span class="k">AND</span> <span class="n">r</span><span class="p">.</span><span class="n">depth_from_parent</span> <span class="o">=</span> <span class="mi">1</span>
   <span class="k">AND</span> <span class="n">p</span><span class="p">.</span><span class="nb">date</span> <span class="k">BETWEEN</span> <span class="s1">'2014-06-01'</span> <span class="k">AND</span> <span class="s1">'2014-06-30'</span>
 <span class="k">GROUP</span> <span class="k">BY</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_name</span><span class="p">;</span></code></pre></figure>

Note that I'm including a `depth_from_parent = 1`, otherwise my `JOIN` would include a row for any regions that might be under either Sales or Eng, and count posts in the parent regions multiple times.

<table>
    <thead>
        <th>child_district_name</th>
        <th>COUNT(*)</th>
    </thead>
    <tr>
        <td>Eng</td>
        <td>2</td>
    </tr>
    <tr>
        <td>Sales</td>
        <td>2</td>
    </tr>
</table>

We could also ask *How many posts per day were there under the State Hierarchy?*

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="nb">DATE</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nb">date</span><span class="p">),</span>
       <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
  <span class="k">FROM</span> <span class="n">test_post</span> <span class="n">p</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_group_region_bridge</span> <span class="n">gr</span>
    <span class="k">ON</span> <span class="n">gr</span><span class="p">.</span><span class="n">group_id</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">group_id</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_region_bridge</span> <span class="n">r</span>
    <span class="k">ON</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_id</span> <span class="o">=</span> <span class="n">gr</span><span class="p">.</span><span class="n">district_id</span>
 <span class="k">WHERE</span> <span class="n">r</span><span class="p">.</span><span class="n">parent_district_name</span> <span class="o">=</span> <span class="s1">'State Hierarchy'</span>
 <span class="k">GROUP</span> <span class="k">BY</span> <span class="nb">DATE</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nb">date</span><span class="p">);</span></code></pre></figure>

Which results in:

<table>
    <thead>
        <th>date</th>
        <th>COUNT(*)</th>
    </thead>
    <tr>
        <td>2014-06-01</td>
        <td>1</td>
    </tr>
    <tr>
        <td>2014-06-10</td>
        <td>1</td>
    </tr>
</table>

Also, *Which posts were there under both Sales and MA?*

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SElECT</span> <span class="n">p</span><span class="p">.</span><span class="o">*</span>
  <span class="k">FROM</span> <span class="n">test_post</span> <span class="n">p</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_group_region_bridge</span> <span class="n">gr</span>
    <span class="k">ON</span> <span class="n">gr</span><span class="p">.</span><span class="n">group_id</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">group_id</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_region_bridge</span> <span class="n">r</span>
    <span class="k">ON</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_id</span> <span class="o">=</span> <span class="n">gr</span><span class="p">.</span><span class="n">district_id</span>
   <span class="k">AND</span> <span class="n">r</span><span class="p">.</span><span class="n">parent_district_name</span> <span class="o">=</span> <span class="s1">'All Regions'</span>
 <span class="k">WHERE</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_name</span> <span class="o">=</span> <span class="s1">'Sales'</span>
<span class="k">INTERSECT</span>
<span class="k">SElECT</span> <span class="n">p</span><span class="p">.</span><span class="o">*</span>
  <span class="k">FROM</span> <span class="n">test_post</span> <span class="n">p</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_group_region_bridge</span> <span class="n">gr</span>
    <span class="k">ON</span> <span class="n">gr</span><span class="p">.</span><span class="n">group_id</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">group_id</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_region_bridge</span> <span class="n">r</span>
    <span class="k">ON</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_id</span> <span class="o">=</span> <span class="n">gr</span><span class="p">.</span><span class="n">district_id</span>
   <span class="k">AND</span> <span class="n">r</span><span class="p">.</span><span class="n">parent_district_name</span> <span class="o">=</span> <span class="s1">'All Regions'</span>
 <span class="k">WHERE</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_name</span> <span class="o">=</span> <span class="s1">'MA'</span><span class="p">;</span></code></pre></figure>

This one is tricky, but shows the flexibility of SQL. It returns the correct single row.

<table>
    <thead>
        <th>post_id</th>
        <th>group_id</th>
        <th>text</th>
        <th>date</th>
    </thead>
    <tr>
        <td>5</td>
        <td>10</td>
        <td>far</td>
        <td>2014-06-10</td>
    </tr>
</table>


## Taking it up a notch

Now, I promised to address what happens when we also need to account for changes in the hierarchy over time. For example, what if group 11 moved from district 7 (Eng) to district 6 (Sales) on 2014-06-18, and then moved back to district 7 (Eng) on 2014-06-22? There should still be four posts, but now there were three in Sales and one in Eng.

In order to record the relevant dates, I'll add `change_effective` and `change_end` dates to the table, as well as a `change_current` to track whether this is the current state of the group. Here is our new `test_group_region_bridge` table.

<table>
    <thead>
        <th>group_id</th>
        <th>district_id</th>
        <th>change_effective</th>
        <th>change_end</th>
        <th>change_current</th>
    </thead>
    <tr>
        <td>8</td>
        <td>5</td>
        <td>2000-01-01</td>
        <td>2100-01-01</td>
        <td>1</td>
    </tr>
    <tr>
        <td>9</td>
        <td>6</td>
        <td>2000-01-01</td>
        <td>2100-01-01</td>
        <td>1</td>
    </tr>
    <tr>
        <td>10</td>
        <td>5</td>
        <td>2000-01-01</td>
        <td>2100-01-01</td>
        <td>1</td>
    </tr>
    <tr>
        <td>10</td>
        <td>6</td>
        <td>2000-01-01</td>
        <td>2100-01-01</td>
        <td>1</td>
    </tr>
    <tr>
        <td>11</td>
        <td>7</td>
        <td>2000-01-01</td>
        <td>2014-06-17</td>
        <td>0</td>
    </tr>
    <tr>
        <td>11</td>
        <td>6</td>
        <td>2014-06-18</td>
        <td>2014-06-21</td>
        <td>0</td>
    </tr>
    <tr>
        <td>11</td>
        <td>7</td>
        <td>2014-06-22</td>
        <td>2100-01-01</td>
        <td>1</td>
    </tr>
</table>

Notice that I'm using dates in the far past and far future to denote the beginning of time and the end of time, respectively. This is to make queries easier to write.

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_group_region_bridge</span><span class="p">(</span><span class="n">group_id</span> <span class="nb">integer</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span> <span class="n">district_id</span> <span class="nb">integer</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span> <span class="n">change_effective</span> <span class="nb">timestamp</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span> <span class="n">change_end</span> <span class="nb">timestamp</span> <span class="k">null</span><span class="p">,</span> <span class="n">change_current</span> <span class="nb">boolean</span> <span class="k">not</span> <span class="k">null</span><span class="p">);</span>

<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">test_group_region_bridge</span> <span class="p">(</span><span class="n">group_id</span><span class="p">,</span> <span class="n">district_id</span><span class="p">,</span> <span class="n">change_effective</span><span class="p">,</span> <span class="n">change_end</span><span class="p">,</span> <span class="n">change_current</span><span class="p">)</span>
           <span class="k">SELECT</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2000-01-01'</span><span class="p">),</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2100-01-01'</span><span class="p">),</span> <span class="mi">1</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2000-01-01'</span><span class="p">),</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2100-01-01'</span><span class="p">),</span> <span class="mi">1</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2000-01-01'</span><span class="p">),</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2100-01-01'</span><span class="p">),</span> <span class="mi">1</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2000-01-01'</span><span class="p">),</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2100-01-01'</span><span class="p">),</span> <span class="mi">1</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2000-01-01'</span><span class="p">),</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2014-06-17'</span><span class="p">),</span> <span class="mi">0</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2014-06-18'</span><span class="p">),</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2014-06-21'</span><span class="p">),</span> <span class="mi">0</span>
<span class="k">UNION</span> <span class="k">ALL</span> <span class="k">SELECT</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2014-06-22'</span><span class="p">),</span> <span class="nb">DATE</span><span class="p">(</span><span class="s1">'2100-01-01'</span><span class="p">),</span> <span class="mi">1</span><span class="p">;</span></code></pre></figure>

Now we can run the same region count query as before, but have it take into account that fact that the post has to have been made while the group was still in that region. You can also use the same schema to fall back to the previous behavior of looking at just the current state of the org chart with `change_current = 1`.

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_name</span><span class="p">,</span>
       <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
  <span class="k">FROM</span> <span class="n">test_post</span> <span class="n">p</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_group_region_bridge</span> <span class="n">gr</span>
    <span class="k">ON</span> <span class="n">gr</span><span class="p">.</span><span class="n">group_id</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">group_id</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_region_bridge</span> <span class="n">r</span>
    <span class="k">ON</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_id</span> <span class="o">=</span> <span class="n">gr</span><span class="p">.</span><span class="n">district_id</span>
 <span class="k">WHERE</span> <span class="n">r</span><span class="p">.</span><span class="n">parent_district_name</span> <span class="o">=</span> <span class="s1">'Role Hierarchy'</span>
   <span class="k">AND</span> <span class="n">r</span><span class="p">.</span><span class="n">depth_from_parent</span> <span class="o">=</span> <span class="mi">1</span>
   <span class="k">AND</span> <span class="n">p</span><span class="p">.</span><span class="nb">date</span> <span class="k">BETWEEN</span> <span class="s1">'2014-06-01'</span> <span class="k">AND</span> <span class="s1">'2014-06-30'</span>
   <span class="k">AND</span> <span class="n">p</span><span class="p">.</span><span class="nb">date</span> <span class="k">BETWEEN</span> <span class="n">gr</span><span class="p">.</span><span class="n">change_effective</span> <span class="k">and</span> <span class="n">gr</span><span class="p">.</span><span class="n">change_end</span>
 <span class="k">GROUP</span> <span class="k">BY</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_name</span><span class="p">;</span></code></pre></figure>


## Smoothing out the kinks

You may have noticed that these queries are all repeating the same `JOIN` syntax. If you think this is going to be a common query pattern, you can create a view as a shortcut.

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">VIEW</span> <span class="n">test_post_historical_region</span> <span class="k">AS</span>
<span class="k">SELECT</span> <span class="n">p</span><span class="p">.</span><span class="o">*</span><span class="p">,</span>
       <span class="n">r</span><span class="p">.</span><span class="n">parent_district_name</span><span class="p">,</span>
       <span class="n">r</span><span class="p">.</span><span class="n">child_district_id</span><span class="p">,</span>
       <span class="n">r</span><span class="p">.</span><span class="n">child_district_name</span>
  <span class="k">FROM</span> <span class="n">test_post</span> <span class="n">p</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_group_region_bridge</span> <span class="n">gr</span>
    <span class="k">ON</span> <span class="n">gr</span><span class="p">.</span><span class="n">group_id</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">group_id</span>
 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">test_region_bridge</span> <span class="n">r</span>
    <span class="k">ON</span> <span class="n">r</span><span class="p">.</span><span class="n">child_district_id</span> <span class="o">=</span> <span class="n">gr</span><span class="p">.</span><span class="n">district_id</span>
 <span class="k">WHERE</span> <span class="n">p</span><span class="p">.</span><span class="nb">date</span> <span class="k">BETWEEN</span> <span class="n">gr</span><span class="p">.</span><span class="n">change_effective</span> <span class="k">and</span> <span class="n">gr</span><span class="p">.</span><span class="n">change_end</span><span class="p">;</span></code></pre></figure>

This will make your previous query much simpler.

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="nb">DATE</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nb">date</span><span class="p">),</span>
       <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
  <span class="k">FROM</span> <span class="n">test_post_historical_region</span> <span class="n">p</span>
 <span class="k">WHERE</span> <span class="n">p</span><span class="p">.</span><span class="n">child_district_name</span> <span class="o">=</span> <span class="s1">'Sales'</span>
   <span class="k">AND</span> <span class="n">r</span><span class="p">.</span><span class="n">depth_from_parent</span> <span class="o">=</span> <span class="mi">1</span>
 <span class="k">GROUP</span> <span class="k">BY</span> <span class="nb">DATE</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nb">date</span><span class="p">);</span></code></pre></figure>

This also abstracts the existence of the `change_*` fields from the user. The hard part is communicating what the view is doing, so query authors know when to use it, and when *NOT* to use it. If you want to maximize the performance of the system at the cost of extra storage, you can make this a materialized view.
---
title: Creating International CSV files with Python
tags: csv
---

You might assume that reading and writing files using the CSV format, being a simple and human-readable implementation,
would be relatively pain free. You would be wrong. It turns out that even though there
[is a standard](http://tools.ietf.org/html/rfc4180), many applications out there do not conform, or require their own
additions and modifications. Most notable among the outliers is Microsoft Excel, which forces you to do some crazy things
if you want to produce CSV files that can be read by any edition of Windows/Office, including international versions.

Yes, you heard me right. Whether you're running the US or European version of Windows can effect whether you can
seemlessly open a CSV file with commas as the delimiter. Apparently some countries prefer their comma delimited text
to be delimited with semicolons. More on that later.

> The name "CSV" indicates the use of the comma to separate data fields. Nevertheless, the term "CSV" is widely used
to refer a large family of formats, which differ in many ways. For example, many so-called "CSV" files in fact use
the tab character instead of comma (such files can be more precisely referred to as "TSV" for tab separated values);
some implementations allow or require single or double quotation marks around some or all fields; and some reserve
the very first record as a header containing a list of field names. The character set being used is undefined:
some applications require a Unicode BOM to enforce Unicode interpretation - [Wikipedia](http://en.wikipedia.org/wiki/Comma-separated_values#Lack_of_a_standard)


## Diacriticly Delicious

The first hurdle when producing international CSV files with Python is the `csv` module's
[lack of native unicode support](https://docs.python.org/2/library/csv.html):

>The csv module doesn’t directly support reading and writing Unicode, but it is 8-bit-clean save for some problems
with ASCII NUL characters. So you can write functions or classes that handle the encoding and decoding for you as
long as you avoid encodings like UTF-16 that use NULs. UTF-8 is recommended.

Instead, they suggest that you use a a custom `UnicodeWriter` class.

```python
class UnicodeWriter:
    """
    A CSV writer which will write rows to CSV file "f",
    which is encoded in the given encoding.
    """

    def __init__(self, f, dialect=csv.excel, encoding="utf-8", **kwds):
        # Redirect output to a queue
        self.queue = cStringIO.StringIO()
        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
        self.stream = f
        self.encoder = codecs.getincrementalencoder(encoding)()

    def writerow(self, row):
        self.writer.writerow([s.encode("utf-8") for s in row])
        # Fetch UTF-8 output from the queue ...
        data = self.queue.getvalue()
        data = data.decode("utf-8")
        # ... and reencode it into the target encoding
        data = self.encoder.encode(data)
        # write to the target stream
        self.stream.write(data)
        # empty queue
        self.queue.truncate(0)

    def writerows(self, rows):
        for row in rows:
            self.writerow(row)
```

However, even with this handy class, you will find that Excel 2007, 2010 and 2013 will
[fail to detect the UTF-8 encoding](http://stackoverflow.com/questions/155097/microsoft-excel-mangles-diacritics-in-csv-files#answer-155176).
I should note that Google Docs and LibreOffice have no problems detecting the encoding. If you're using earlier versions
of Excel, it's even worse. Excel 97, 2000 and 2003 will simply fail to open the file at all unless you use the import
wizard and manually specify the encoding.

## Dropping Bombs

For the versions of Excel from the modern era, you need to explicitly denote the encoding using a BOM, or
[Byte Order Mark](http://en.wikipedia.org/wiki/Byte_order_mark).

>The byte order mark (BOM) is a Unicode character used to signal the endianness (byte order) of a text file or stream.
It is encoded at U+FEFF byte order mark (BOM). BOM use is optional, and, if used, should appear at the start of the
text stream. Beyond its specific use as a byte-order indicator, the BOM character may also indicate which of the
several Unicode representations the text is encoded in.

Fun. I find that meditating on my own endianness is a very relaxing morning ritual. In any case, it's fairly simple to
write the UTF-8 BOM to the very start of the file:

<figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="kn">import</span> <span class="nn">codecs</span>


    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">dialect</span><span class="o">=</span><span class="n">csv</span><span class="p">.</span><span class="n">excel</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">"utf-8"</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
        <span class="p">...</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">stream</span> <span class="o">=</span> <span class="n">f</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">stream</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">codecs</span><span class="p">.</span><span class="n">BOM_UTF8</span><span class="p">)</span>
        <span class="p">...</span></code></pre></figure>

*Note: You need to write the BOM directly to the file buffer, and not via the UnicodeWriter.writer object, otherwise
the BOM itself will be encoded as UTF-8, when it needs to be raw bytes.*


## Separating the Men from the Boys

It turns our that there is another insidious issue with international versions of Windows. There is actually a setting
in Windows [Regional and Language](http://hotware.wordpress.com/2009/12/16/trouble-with-opening-csv-files-with-excel-the-comma-and-semicolon-issue-in-excel-due-to-regional-settings-for-europe/)
control panel called "List Separator". In the US version of Windows, the default is a comma (`,`). In other locales,
specifically the European version, the default is often a semi-colon (`;`).

![regional settings](/blog/images/regional.png)

There are several hack solutions to this. The first is to tell all your European users to change their setting. The second
is to save the file as a .txt file, and tell the users to choose "Open with Excel", which will then run the import wizard
and detect the delimiter. The third is to tell them to always use launch the import wizard manually, and not to simply
double click on the file to open it.

The fourth, [slightly less gross hack](http://superuser.com/questions/180964/how-to-open-semicolon-delimited-csv-files-in-us-version-of-excel#answer-420025)
to is put `sep=,` as the first line in your csv file, which will signal to Excel that you want to explicitly use commas as
the delimiter.

<figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="kn">import</span> <span class="nn">os</span>


    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">dialect</span><span class="o">=</span><span class="n">csv</span><span class="p">.</span><span class="n">excel</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">"utf-8"</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
        <span class="p">...</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">stream</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">'sep=,'</span> <span class="o">+</span> <span class="n">os</span><span class="p">.</span><span class="n">linesep</span><span class="p">)</span>
        <span class="p">...</span></code></pre></figure>

*Note: this line will show up in most spreadsheet applications as the first line of your data. So users may have to
manually delete the first two every time they start working with a file.*


## More Fun

After some more research, it turns out that Mac Excel simply [does not support UTF-8](http://answers.microsoft.com/en-us/mac/forum/macoffice2011-macexcel/mac-excel-converts-utf-8-characters-to-underlines/7c4cdaa7-bfa3-41a2-8482-554ae235227b?msgId=c8295574-a053-48a6-b419-51523ce2a247)
at all. So forget it ever working there.

Also, writing BOTH the BOM and the `sep=,` will cause Windows Excel to [forget about the BOM](http://stackoverflow.com/questions/20395699/sep-statement-breaks-utf8-bom-in-csv-file-which-is-generated-by-xsl#answer-23513342)
you just specified. So, you basically need to choose whether you want unicode characters to show up correctly, or
whether you want to have the columns separated automatically.

---
title: Why software engineers should maintain a blog
tags: reading-list career
---

I have a few pieces of standard advice that I give to anyone starting out in software engineering.
[Join a startup](http://chase-seibert.github.io/blog//2013/10/04/what-to-know-before-starting-at-a-startup.html),
[max out your 401k](http://chase-seibert.github.io/blog//2014/01/01/saving-for-retirement-as-a-software-engineer.html)
and *start a blog*. Why should you start blogging?


## Personal Branding

![calvin and hobbes](http://assets.amuniversal.com/8f870570df960131725e005056a9545d)

Early on in your career, your biggest asset is your future earning potential. *Aside: this is why you should also
consider buying disability insurance, which is super cheap*. You should be continually re-investing in yourself and
your career. Part of that is building a personal brand. When a prospective employer Googles your name, they expect
to find your LinkedIn profile and your Facebook page. If they also find a personal blog full of your writing and
your code, you just jumped ahead of 95% of applicants.

Most people on your interview loop with take the time to at least read the latest entries in your blog. They will
probably ask you questions about something you wrote about recently. *Hint: if you're actively interviewing, think
carefully about which posts you want to at the top of the list*. These are the absolute easiest interviews you will
ever have, and some of the most productive. The likely hood that you know more about the subject than the interviewer
is high.

Don't worry too much about the SEO aspect. If you produce good content and put your name in the title of all the pages,
the rest will take care of itself.


## Career Growth

When I sit down and think about what to write, I usually ask myself "What was the hardest problem I solved in the
last week"? This is a great reality check. Am I still growing and learning in my current job? If you have honestly been
putting in the effort to write, but you look back at the last three months of posts and don't see anything particularly
interesting, then you have your answer.

More and more recently, I have begun to blog about the soft side of engineering. I still write posts that explain a
particular technical issue I ran into, but I also write down my thoughts on my *philosophy* of engineering. I find these
particularly useful when growing your leadership skills. Often I will have a conversation with a new engineer, and then
reinforce my points by following up with an article I had written in the past.

You'll be doing a lot of communication during your career, both verbal and written. Honing your ability to communicate
well is going to pay dividends on whatever time you can spend on it. A surprising number of times I have found that it
also pays more direct dividends; searching for a solution and finding the answer in a blog post you had previously
written and forgotten about always makes me want to do a victory lap around the office.


## Giving Back

> Writing isn't about making money, getting famous, getting dates, getting laid, or making friends. In the end, it's
about enriching the lives of those who will read your work, and enriching your own life, as well. It's about getting
up, getting well, and getting over. Getting happy, okay? Getting happy.” ― Stephen King,
[On Writing: A Memoir of the Craft](https://www.goodreads.com/work/quotes/150292)

How many times a week do you find the perfect StackOverflow article that solves your problem and saves you hours of
banging your head against a wall? How many times do you spend hours solving a problem, but no one else ever benefits
from your solution? Help out other poor software engineers by posting your findings! The vast majority of the traffic
coming to my blog is from engineers searching for specific solutions, such as "python memory leak" or "mysql drop
column if exists".

You'll be surprised at the number of comments you will get. Most will just be a simple thank you. Yes - occasionally
someone will comment just to call you an idiot. Dealing with aggressive and confrontational people is also a
part of career growth.


## Good for your company

Personal blogging is also good for your company. Typical corporate blogs suck at conveying the personality of the actual
people on the team. Your blog can [make a bigger impact](http://www.inc.com/magazine/20100301/lets-take-this-offline.html)
just by being authentic. If you want to combine your blog, other coworker blogs and the company blog into one site and
RSS feed, you can use [Advocoders](http://advocoders.herokuapp.com/). There are many services that can then take that
RSS feed and turn it into a self-updating Facebook page or a Twitter profile.


## How to get started

> I used to tell people who asked me for advice about blogging that if they couldn't think about one interesting thing
to write about every week, they weren't trying hard enough.
[Coding Horror](http://blog.codinghorror.com/10-years-of-coding-horror/)

The hardest part is writing. The logistics of setting up blog should be easy. I would recommend looking for a platform
that supports syntax highlighting of code snippets that you include in your posts. Having an RSS feed of your posts is
critical.

Personally, I use [GitHub Pages](https://pages.github.com/) and a static site generator called
[Jekyll](http://jekyllrb.com/). The beauty of the system is that your posts are regular GitHub markdown, just like
when you're making comments on a pull request. You publish new articles with a simple `git push`. I also use a theme called [minimal-mistakes](https://mmistakes.github.io/minimal-mistakes/).

It's a good use of time to start from scratch and follow the various docs. Or,
you can just fork my repository:

```bash
git clone git@github.com:chase-seibert/blog.git
cd blog
rm -f _posts/* _drafts/* images/* reading-list.md manager.md resume.pdf favicon.ico
rm -fR .git
bundle install  # make need: gem install bundler
make server
```

A few minutes after you push your `master` branch to GitHub under a new repo
called `blog`, you should be able to see your blog at
http://$username.github.io/blog.
---
title: Picking a place to live in the East Bay
tags: housing
---

My wife and I recently bought a house in the San Francisco bay area. Having no family in the area, and no real ties to any neighborhood, it was surprisingly difficult to simply pick a town. Initially we just went to open houses at random, often travelling to three or four completely different locales in the same Sunday.

Our experience buying a condo in Boston told us that you can spend months looking and not really getting excited about any of the places. But if all of a sudden you find the neighborhood that's right for you, then every place starts looking pretty good. For us that was the South End in Boston. In the bay area, it turned out to be [Alameda](http://www.streetadvisor.com/search/neighborhoods-in-alameda-alameda-county-california).

![alameda](/blog/images/alameda.jpg)

## Picking a town

We made a spreadsheet. A glorious spreadsheet of the things that were important to us. Things like reasonable prices, good schools and low crime.

<table style="font-size: 70%;">
	<thead>
		<th>Name</th>
		<th>$/sqft</th>
		<th>Schools</th>
		<th>Violent Crime</th>
		<th>Other Crime</th>
		<th>Public Trans</th>
		<th>Driving</th>
		<th>Walking</th>
		<th>Weather</th>
		<th>Earthquake</th>
		<th>Score</th>
	</thead>
	<tr>
		<td>Alameda</td>
		<td>413</td>
		<td>8</td>
		<td>0.31</td>
		<td>2.6</td>
		<td>44</td>
		<td>25</td>
		<td>69</td>
		<td>9</td>
		<td>5</td>
		<td>97</td>
	</tr>
	<tr>
		<td>Orinda</td>
		<td>509</td>
		<td>10</td>
		<td>0.06</td>
		<td>1.79</td>
		<td>49</td>
		<td>24</td>
		<td>22</td>
		<td>7</td>
		<td>1</td>
		<td>96</td>
	</tr>
	<tr>
		<td>Walnut Creek</td>
		<td>367</td>
		<td>9</td>
		<td>0.17</td>
		<td>3.81</td>
		<td>63</td>
		<td>33</td>
		<td>65</td>
		<td>6</td>
		<td>1</td>
		<td>94</td>
	</tr>
	<tr>
		<td>Rockridge</td>
		<td>599</td>
		<td>8</td>
		<td>0.4</td>
		<td>4.5</td>
		<td>38</td>
		<td>21</td>
		<td>81</td>
		<td>7</td>
		<td>1</td>
		<td>93</td>
	</tr>
	<tr>
		<td>Piedmonte</td>
		<td>623</td>
		<td>10</td>
		<td>0.18</td>
		<td>2.84</td>
		<td>65</td>
		<td>22</td>
		<td>50</td>
		<td>8</td>
		<td>1</td>
		<td>93</td>
	</tr>
	<tr>
		<td>Moraga</td>
		<td>398</td>
		<td>10</td>
		<td>0.09</td>
		<td>1.26</td>
		<td>66</td>
		<td>33</td>
		<td>20</td>
		<td>7</td>
		<td>1</td>
		<td>92</td>
	</tr>
	<tr>
		<td>Dublin</td>
		<td>341</td>
		<td>9</td>
		<td>0.21</td>
		<td>1.82</td>
		<td>61</td>
		<td>38</td>
		<td>42</td>
		<td>6</td>
		<td>1</td>
		<td>90</td>
	</tr>
	<tr>
		<td>Upper Rockridge</td>
		<td>501</td>
		<td>8</td>
		<td>0.3</td>
		<td>2.65</td>
		<td>60</td>
		<td>22</td>
		<td>50</td>
		<td>8</td>
		<td>1</td>
		<td>89</td>
	</tr>
	<tr>
		<td>Lafayette</td>
		<td>507</td>
		<td>10</td>
		<td>0.09</td>
		<td>1.97</td>
		<td>68</td>
		<td>31</td>
		<td>33</td>
		<td>6</td>
		<td>3</td>
		<td>80</td>
	</tr>
	<tr>
		<td>Potrero Hill</td>
		<td>756</td>
		<td>2.2</td>
		<td>0.32</td>
		<td>2.7</td>
		<td>28</td>
		<td>7</td>
		<td>88</td>
		<td>9</td>
		<td>1</td>
		<td>80</td>
	</tr>
	<tr>
		<td>San Mateo</td>
		<td>563</td>
		<td>7</td>
		<td>0.35</td>
		<td>2.71</td>
		<td>45</td>
		<td>24</td>
		<td>67</td>
		<td>5</td>
		<td>1</td>
		<td>79</td>
	</tr>
	<tr>
		<td>Albany</td>
		<td>494</td>
		<td>9</td>
		<td>0.33</td>
		<td>4.9</td>
		<td>64</td>
		<td>23</td>
		<td>86</td>
		<td>4</td>
		<td>5</td>
		<td>74</td>
	</tr>
	<tr>
		<td>Daly City</td>
		<td>447</td>
		<td>6</td>
		<td>0.29</td>
		<td>2.32</td>
		<td>50</td>
		<td>11</td>
		<td>63</td>
		<td>2</td>
		<td>1</td>
		<td>71</td>
	</tr>
	<tr>
		<td>Montclair</td>
		<td>450</td>
		<td>6</td>
		<td>0.2</td>
		<td>3</td>
		<td>69</td>
		<td>25</td>
		<td>64</td>
		<td>7</td>
		<td>5</td>
		<td>70</td>
	</tr>
	<tr>
		<td>San Bruno</td>
		<td>462</td>
		<td>6</td>
		<td>0.32</td>
		<td>2.63</td>
		<td>55</td>
		<td>16</td>
		<td>58</td>
		<td>4</td>
		<td>3</td>
		<td>67</td>
	</tr>
	<tr>
		<td>Palo Alto</td>
		<td>1028</td>
		<td>10</td>
		<td>0.11</td>
		<td>3.25</td>
		<td>94</td>
		<td>36</td>
		<td>63</td>
		<td>9</td>
		<td>1</td>
		<td>67</td>
	</tr>
	<tr>
		<td>Bernal Heights</td>
		<td>719</td>
		<td>2.4</td>
		<td>0.5</td>
		<td>2.7</td>
		<td>44</td>
		<td>13</td>
		<td>87</td>
		<td>7</td>
		<td>1</td>
		<td>64</td>
	</tr>
	<tr>
		<td>Pleasanton</td>
		<td>395</td>
		<td>9</td>
		<td>0.1</td>
		<td>2.31</td>
		<td>131</td>
		<td>46</td>
		<td>50</td>
		<td>6</td>
		<td>1</td>
		<td>64</td>
	</tr>
	<tr>
		<td>San Ramon</td>
		<td>354</td>
		<td>9</td>
		<td>0.13</td>
		<td>1.8</td>
		<td>116</td>
		<td>40</td>
		<td>32</td>
		<td>6</td>
		<td>3</td>
		<td>63</td>
	</tr>
	<tr>
		<td>San Leandro</td>
		<td>281</td>
		<td>4</td>
		<td>0.82</td>
		<td>5.32</td>
		<td>47</td>
		<td>30</td>
		<td>65</td>
		<td>7</td>
		<td>5</td>
		<td>61</td>
	</tr>
	<tr>
		<td>North Berkeley</td>
		<td>503</td>
		<td>7</td>
		<td>0.64</td>
		<td>7.2</td>
		<td>54</td>
		<td>24</td>
		<td>85</td>
		<td>4</td>
		<td>5</td>
		<td>58</td>
	</tr>
	<tr>
		<td>Berkeley Hills</td>
		<td>497</td>
		<td>7</td>
		<td>0.45</td>
		<td>5.5</td>
		<td>110</td>
		<td>29</td>
		<td>45</td>
		<td>8</td>
		<td>1</td>
		<td>57</td>
	</tr>
	<tr>
		<td>Berkeley</td>
		<td>508</td>
		<td>7</td>
		<td>0.64</td>
		<td>7.2</td>
		<td>48</td>
		<td>23</td>
		<td>82</td>
		<td>3</td>
		<td>5</td>
		<td>56</td>
	</tr>
	<tr>
		<td>Twin Peaks</td>
		<td>606</td>
		<td>4.4</td>
		<td>0.32</td>
		<td>2.7</td>
		<td>46</td>
		<td>17</td>
		<td>63</td>
		<td>2</td>
		<td>1</td>
		<td>55</td>
	</tr>
	<tr>
		<td>Sausalito</td>
		<td>575</td>
		<td>5</td>
		<td>0.12</td>
		<td>2.35</td>
		<td>57</td>
		<td>29</td>
		<td>20</td>
		<td>4</td>
		<td>1</td>
		<td>52</td>
	</tr>
	<tr>
		<td>Temescal</td>
		<td>468</td>
		<td>8</td>
		<td>1.91</td>
		<td>6.1</td>
		<td>46</td>
		<td>19</td>
		<td>87</td>
		<td>5</td>
		<td>5</td>
		<td>51</td>
	</tr>
	<tr>
		<td>Richmond</td>
		<td>198</td>
		<td>3</td>
		<td>1.19</td>
		<td>5.33</td>
		<td>62</td>
		<td>26</td>
		<td>55</td>
		<td>4</td>
		<td>5</td>
		<td>35</td>
	</tr>
</table>

Most of these values were based on objective third party data:

- [trulia.com](http://www.trulia.com/home_prices/California/Berkeley-heat_map/) (prices per square foot, crime)
- [greatschools.org](http://www.greatschools.org/california/berkeley/Berkeley-Unified/) (schools)
- [maps.google.com](http://map.google.com) (transit times) *Note: this is to the SOMA area in SF*
- [walkscore.com](http://www.walkscore.com/CA/Oakland/Montclair) (walk-ability)
- [gis.abag.ca.gov](http://gis.abag.ca.gov/website/liquefactionsusceptibility/index.html) (earthquake)

Weather I could not find excellent data for. I was interested in lots of sun and moderate temperature. In the end, I basically made up these numbers based on anecdotal evidence.

Some of the numbers are on different scales, so I normalized them around 1.0 and applied a crude heuristic to spread the scores out more evenly from 1-100.

```bash
=4*(-18+100*AVERAGE(B$33*(1-B2/1000), C$33*(C2/10), AVERAGE(D$33*(1-D2), E$33*(1-E2/10)), AVERAGE(F$33*(1-F2/60), G$33*(1-G2/60)), H$33*(H2/100), I$33*(I2/10),J$33*(1-J2/10)))
```

## Picking a neighborhood

Of course particular neighborhoods inside a town can be as different from each other as particular towns in the same area. We started going to all the open houses in Alameda, which turned out to only be three to five new listings a week.

Even at that low volume, it's easy to get a sense of the different neighborhoods. The small scale makes it possible to keep track of school districts, landmarks or even individual streets. It turned out that all the houses we liked best were in the same 10 block radius, so we started focusing exclusively on that neighborhood.

Once we decided on a particular neighborhood, we choose a local broker to work with. Knowing how competitive the housing market was, we selected a broker that specialized in winning bids. But it still took months of seeing houses to actually make our first bid.

## Picking a house

As we had conversations at the open houses about what we did and did not like, we wrote those items down. Then we decided how important each item was to us individually. We also estimated the cost to "correct" a given factor. Many items, like distance to a movie theater, cannot be fixed for any cost.

<table style="font-size: 70%;">
	<thead>
		<th>Feature</th>
		<th>Cost</th>
		<th>Chase</th>
		<th>Noel</th>
		<th>Score</th>
	</thead>
	<tr>
		<td>Outdoor space for grill</td>
		<td>10</td>
		<td>10</td>
		<td>8</td>
		<td>9.3</td>
	</tr>
	<tr>
		<td>Outdoor sunny space for garden boxes</td>
		<td>10</td>
		<td>10</td>
		<td>8</td>
		<td>9.3</td>
	</tr>
	<tr>
		<td>Walking distance to grocery store</td>
		<td>10</td>
		<td>8</td>
		<td>10</td>
		<td>9.3</td>
	</tr>
	<tr>
		<td>Lots of kitchen counter space</td>
		<td>9</td>
		<td>8</td>
		<td>9</td>
		<td>8.7</td>
	</tr>
	<tr>
		<td>Open floor plan</td>
		<td>10</td>
		<td>8</td>
		<td>8</td>
		<td>8.7</td>
	</tr>
	<tr>
		<td>Tree lined shady streets</td>
		<td>10</td>
		<td>7</td>
		<td>8</td>
		<td>8.3</td>
	</tr>
	<tr>
		<td>Tall ceilings (especially in stairway)</td>
		<td>10</td>
		<td>7</td>
		<td>8</td>
		<td>8.3</td>
	</tr>
	<tr>
		<td>Dining room can seat 12</td>
		<td>9</td>
		<td>8</td>
		<td>8</td>
		<td>8.3</td>
	</tr>
	<tr>
		<td>Gas range</td>
		<td>4</td>
		<td>10</td>
		<td>8</td>
		<td>7.3</td>
	</tr>
	<tr>
		<td>Tub you can fit in</td>
		<td>9</td>
		<td>7</td>
		<td>6</td>
		<td>7.3</td>
	</tr>
	<tr>
		<td>Walking distance to movie theater</td>
		<td>10</td>
		<td>6</td>
		<td>5</td>
		<td>7.0</td>
	</tr>
	<tr>
		<td>Hardwood floors</td>
		<td>7</td>
		<td>5</td>
		<td>8</td>
		<td>6.7</td>
	</tr>
	<tr>
		<td>Walk-in closet</td>
		<td>5</td>
		<td>6</td>
		<td>8</td>
		<td>6.3</td>
	</tr>
	<tr>
		<td>Deck</td>
		<td>5</td>
		<td>6</td>
		<td>8</td>
		<td>6.3</td>
	</tr>
	<tr>
		<td>Dual vanity</td>
		<td>9</td>
		<td>4</td>
		<td>6</td>
		<td>6.3</td>
	</tr>
	<tr>
		<td>Central AC</td>
		<td>7</td>
		<td>5</td>
		<td>5</td>
		<td>5.7</td>
	</tr>
	<tr>
		<td>Shed/basement space for a work bench</td>
		<td>10</td>
		<td>5</td>
		<td>1</td>
		<td>5.3</td>
	</tr>
	<tr>
		<td>Walking distance to off-leash field</td>
		<td>10</td>
		<td>3</td>
		<td>3</td>
		<td>5.3</td>
	</tr>
	<tr>
		<td>Walking distance to basketball court</td>
		<td>10</td>
		<td>2</td>
		<td>1</td>
		<td>4.3</td>
	</tr>
	<tr>
		<td>Built-in bookcase</td>
		<td>8</td>
		<td>2</td>
		<td>2</td>
		<td>4.0</td>
	</tr>
</table>

This really helped us realistically evaluate homes that had high curb appeal, but which did not ultimately meed all our needs.

## Picking an offer price

Having looked at around 100 homes, and paying attention to asking price and closing price on those, we knew that houses tended to go far over asking price. It turned out that this was a particular problem in Alameda; sellers were routinely listing their house one or two hundred thousand dollars bellow what they actually wanted for it.

After picking our own three to five open houses a weekend for more than six months, we finally found our house through our broker. It was not actually listed on MLS; the seller wanted to do a private sale. Not opening the house to public bids is great from a buyers perspective, but it did mean that we needed to feel confident in our bid price.

There was no substitute for paying close attention to closing prices over an extended period. That gave us a decent idea what the house would likely list for, and what it would likely close for on the open market. Luckily, we were able to quickly pick a price that both parties were happy with.
---
title: My favorite coding interview question
tags: interview
---

Every software engineering interview I have ever participated in has involved a coding exercise. For one position, I
would expect three two five separate coding tests. I've [written previously](/blog/2012/08/31/how-to-prepare-for-a-software-engineer-interview.html)
about why every company asks these questions, and the best way to handle these as a candidate. But what makes a good coding question?

There is very little data out there about effective interviewing. What data does exists seems to suggest interviews are
only good for filtering out candidates that do not meet the minimum bar. According to one popular
[Google internal study](http://www.forbes.com/sites/quora/2013/06/28/is-there-a-link-between-job-interview-performance-and-job-performance/),
there is no correlation between interview results and on the job performance.

With this in mind, the coding question I ask is simple. I have found that you can hardly ask a question that's too
simple; a substantial number of candidates will have no problem hanging themselves with the smallest amount of rope
that you can give them.

## The Question

>Write a function that takes two parameters, a string and an integer. The function will return another string that
>is similar to the input string, but with certain characters removed. It's going to remove characters from consecutive
>runs of the same character, where the length of the run is greater than the input parameter.

    Ex: "aaab", 2 => "aab"
    Ex: "aabb", 1 => "ab"
    Ex: "aabbaa", 1 => "aba"

*Note: I'm evaluating your answer on the simplicity of your code. The goal is for it to be readable; someone new
should be able to walk into this room afterwards and instantly understand what your function is doing.*

## The Evaluation

I explicitly state the part about what I'm looking for with the candidate. I tell them to use the language they are
most comfortable with. Here is what I consider an ideal solution, in Python:

```python
def remove_extra_consecutive(input_str, max_consecutive_chars):
    output, prev_char, current_char_seen = '', None, 0
    for current_char in input_str:
        if current_char == prev_char:
            current_char_seen += 1
        else:
            current_char_seen = 0
            prev_char = current_char
        if current_char_seen < max_consecutive_chars:
            output += current_char
    return output
```

While the candidate is writing code, I focus on the following.

- Thought process. How do they get to a solution? Did they talk about their strategy?
- It needs to work. If it doesn't, I will keep pushing them until it does.
- Are the function and the variables well named?
- Is the code as simple as possible, but not more simple?
- Is the candidate struggling to remember standard library functions? Help them out.

## Other Solutions

The most common variant answer I see is conflating the clauses that count the character and append to the output. Typically this results in a bug where the last character in a run is omitted.

```python
def remove_extra_consecutive(input_str, max_consecutive_chars):
    output, prev_char, current_char_seen = '', None, 1
    for current_char in input_str:
        if current_char == prev_char and current_char_seen < max_consecutive_chars:
            current_char_seen += 1
            output += current_char
        else:
            if current_char != prev_char:
                current_char_seen = 1
                output += current_char
        prev_char = current_char
    return output
```

Another variant is using indexes instead of a named variable for the previous character. This can often lead to an index out of bounds bug. It's also common to forget to add the last character.

```python
def remove_extra_consecutive(input_str, max_consecutive_chars):
    output, current_char_seen = '', 0
    for i in range(len(input_str) - 1):
        if input_str[i] == input_str[i+1]:
            current_char_seen += 1
        else:
            current_char_seen = 0
        if current_char_seen < max_consecutive_chars:
            output += input_str[i]
    if current_char_seen <= max_consecutive_chars:
        output += input_str[i+1]
    return output
```

Finally some candidates try to alter the input string itself, or sometimes loop indexes, which can lead of off by one errors.

```python
def remove_extra_consecutive(str, max_consecutive_chars):
    for i in range(len(str)):
        j = i + 1
        while j < len(str):
            if str[i] != str[j]:
                break
            if j - i >= max_consecutive_chars:
                str = str[0:j] + str[j+1:]
            j += 1
    return str
```

## Summary

I like this problem because it has one simple and robust solution, and a number of more complicated and brittle solutions. If a candidate gets through it quickly and correctly, I follow up by asking them about which edge cases they would want to create unit tests for. If it's in a dynamic language, I ask about how to make the function operate on any iterable.
---
title: Sync dotfiles with GitHub
tags: tmux dotfiles vim
---

## What are dotfiles?

The term dotfiles refers to configuration files for various Linux or OSX utilities, primarily command line applications, which reside in your home directory and begin with a period character. Examples are `.vimrc` for Vim and `.bashrc` for Bash.

Some programs have hundreds of options, granting a multitude of permutations on form and function. For example, you can change your color scheme, or automate a common task with a shortcut.

It's common for software engineers to [maintain a set of these dotfiles](http://dotfiles.github.io/) and evolve them over the course of their career. Eventually, they may end up with interfaces that are hyper optimized just for them.

![dotfiles](/blog/images/dotfiles.png)


## Why host them on version control?

Hosting your dotfiles on a site like GitHub has several advantages. Centralization means you can easily sync your configuration between any number of machines that you use regularly. Having them publicly available means that you can easily share them with other developers. Versioning them makes it easy to roll back a recent change.


## How you can use GitHub to sync dotfiles

If you simply maintain a shared folder of configuration files, whether it's on GitHub or Dropbox, you will quickly run into an irksome maintenance problem every time you need to provision a new machine, or update an exiting one. Because these configuration files need to be located in your home directly to take effect, you will end up manually copying or maybe sym-linking them one at a time.

You might think to write a script for this. No need! There are plenty of exiting utilities out there that do just this. As a Python developer, I choose to use a utility on pip called simply [dotfiles](https://github.com/jbernard/dotfiles).

I encourage you to read the documentation, but essentially this tool maintaines a single directory with a master copy of all your configuration options, and syncs them into the correct place on demand. This should be enough to get you started:

```python
pip install dotfiles
dotfiles --add ~/.vimrc
cd ~/Dotfiles
git init
git add vimrc  # notice that the canonical version does not start with a dot
git commit -m "Added vimrc, welcome aboard!"
```

From then on, you can `git push` your changes to your GitHub account. When you want to sync your dotfiles to a new machine, just do a `git clone` and then run `dotfiles --sync`.


## Gotchas

I did not personally like the default `~/Dotfiles` directory. Instead, I used `~/.dotfiles`, which you can specify in your [.dotfilesrc](https://github.com/chase-seibert/dotfiles/blob/master/dotfilesrc).

I also ended up using the configuration options for `ignores` and `packages`. The later symlinks entire configuration directories, like `~/.ssh`.

*Note: do not check your SSH private key into a public repository!*


## Some gems from my [personal dotfiles](https://github.com/chase-seibert/dotfiles/blob/master/vimrc)

My most extensive configuration is my [vimrc](https://github.com/chase-seibert/dotfiles/blob/master/vimrc). Check it out; it's pretty well commented. I especially like the [python-mode](https://github.com/klen/python-mode) integration, for features like jumping into the definitions of other modules, and doing in-editor linting.

I have an [install.sh](https://github.com/chase-seibert/dotfiles/blob/master/install.sh) script, which contains different blocks for various platforms:

```bash
unamestr=`uname`
if [[ $unamestr == 'Linux' ]]; then
    ...
elif [[ $unamestr == 'Darwin' ]]; then
    ...
fi
```

I've also done some work to [enable integration](https://gist.github.com/burke/5960455) with the OSX clipboard for copying and pasting while inside an SSH session.
---
title: Creating a Budget and Sticking to It
tags: finance 401k
---

The hardest part of doing anything is starting. If you're someone who has delayed
getting serious about retirement savings, or budgeting in general, I would encourage
you to simply begin, today. I'll describe my personal budgeting system, not because I
think it's the best system, but because it's a place you can start if you're not sure
where to begin.


## Track Expenses

In order to create a realistic budget, you first need to quantify what you currently
spend your money on. You may already have some idea of what you spend on the basics
like housing, food and utilities. But what about other large, less frequent expenditures
like vacations, insurance and holiday gifts?

I recommend using a tool like [Mint.com](http://www.mint.com), which can automatically
track purchases and categorize them for you. It will correctly guess the expense category
for the vast majority of charges, and you can manually fix any errant transactions on either
their website or mobile app.

Your goal should be to create a pie chart of all your expenses by category. You can start with as
little as 30 days worth of data, but a year would be better.

![mint.com](https://www.mint.com/blog/wp-content/uploads/2012/07/trends_blog_screenshot-copy.png)

Note: Mint works by storing the passwords for your bank accounts. This is obviously a security
concern. If you want, you can replicate this same data manually. But I would caution you to be
realistic about whether you are going to keep up with that consistently. Like many other pieces
of my personal budgeting system, I highly value the capability to put things on auto-pilot.


## Create a Budget

Next, you need to use that information to create a budget. I would start by simply budgeting for
the actual amounts you spend. Don't try to cut down expenses just yet. Here is a fabricated
budget for someone who is single, living with roommates and making $100k/year, which is
a [realistic salary](http://www.quora.com/What-is-the-average-starting-salary-for-a-software-engineer-with-a-BS-degree-in-San-Francisco-Bay-Area)
for a junior software engineer in the bay area.

<table>
    <tr>
        <th>Category</th>
        <th>$/Year</th>
        <th>$/Month</th>
        <th>$/Week</th>
    </tr>
    <tr>
        <td><b>Gross Pay</b></td>
        <td>$100,000</td>
        <td>$8,333</td>
        <td>$1,917</td>
    </tr>
        <td>Federal Tax</td>
        <td>$14,521</td>
        <td>$1,210</td>
        <td>$278</td>
    </tr>
    <tr>
        <td>Social Security</td>
        <td>$6,143</td>
        <td>$512</td>
        <td>$118</td>
    </tr>
    <tr>
        <td>Medicare</td>
        <td>$1,433</td>
        <td>$119</td>
        <td>$27</td>
    </tr>
    <tr>
        <td>CA Income Tax</td>
        <td>$5,848</td>
        <td>$487</td>
        <td>$112</td>
    </tr>
    <tr>
        <td>401k</td>
        <td>$17,500</td>
        <td>$1,458</td>
        <td>$335</td>
    </tr>
    <tr>
        <td>Medical/Benefits</td>
        <td>$696</td>
        <td>$58</td>
        <td>$13</td>
    </tr>
    <tr>
        <td><b>Net Pay</b></td>
        <td>$46,306</td>
        <td>$3,859</td>
        <td>$887</td>
    </tr>
    <tr>
        <td>Housing</td>
        <td>$14,400</td>
        <td>$1,200</td>
        <td>$276</td>
    </tr>
    <tr>
        <td>Food</td>
        <td>$7,200</td>
        <td>$600</td>
        <td>$138</td>
    </tr>
    <tr>
        <td>Utilities</td>
        <td>$3,600</td>
        <td>$300</td>
        <td>$69</td>
    </tr>
    <tr>
        <td>Vacation</td>
        <td>$6,000</td>
        <td>$500</td>
        <td>$115</td>
    </tr>
    <tr>
        <td>Christmas</td>
        <td>$1,200</td>
        <td>$100</td>
        <td>$23</td>
    </tr>
    <tr>
        <td>Insurance</td>
        <td>$1,200</td>
        <td>$100</td>
        <td>$23</td>
    </tr>
    <tr>
        <td>Emergency Savings</td>
        <td>$6,000</td>
        <td>$500</td>
        <td>$115</td>
    </tr>
    <tr>
        <td>Contingency</td>
        <td>$1,200</td>
        <td>$100</td>
        <td>$23</td>
    </tr>
    <tr>
        <td><b>Left over</b></td>
        <td>$5,506</td>
        <td>$459</td>
        <td>$106</td>
    </tr>
</table>

Notice that I have included a weekly dollar amount. More on that in the next section.
I have also maxed out the 401k contribution. Please [read this](http://chase-seibert.github.io/blog/2014/01/01/saving-for-retirement-as-a-software-engineer.html) before deciding to axe that budget item!


## Pay Yourself Weekly

Most people either get paid twice a month, or every other week. I prefer to pay myself
weekly. Basically I deposit my paycheck into a holding account, and I transfer set amounts
every Friday into various specific purpose accounts. I'm never more than a week away from
a paycheck. If spending money is running low, it's easier to hold out for a few days
versus a week or more. This makes it more likely that I will stick to my budget and not
cheat.

In addition, this system evens out irregular pay cycles. If you get paid on the first and
the fifteenth of the month, some months you will have up to four more days on the second
pay cycle. It also makes scheduling transfers easier. Instead of lining up transfers with
you paychecks, they can happen weekly.


## Consider Separate Accounts

I actually have 17 bank accounts. Some are savings accounts, but most are checking accounts.
Why? It allows me to set aside money for a particular line item in my budget. I can see
at a glance how much money I have saved for vacation. I can be sure that my mortgage check
will never bounce; nothing else draws from that account.

Most importantly, I know when I'm out of spending money for the week. My wife and I both have
our own accounts that receive $350 a week. We make most of our purchases that fall under the
"shopping" category out of these accounts. I can drain that account guilt-free on "wants"
over the weekend, knowing that all our actual needs are accounted for in other accounts. This
makes me hyper aware of what I'm spending. In the ten years I have been doing this, I can
count on one hand the number of times I have overdrawn.

To fund these accounts, I have a series of standing scheduled transfers on Fridays that move
the budgeted amounts from the holding account into the various purposed accounts. Keep in mind
there are [52.1775 weeks in a year](https://www.google.com/search?q=weeks%20in%20a%20year&rct=j),
so I use the formula `transfer = yearly budget / 52.1175` to determine the amounts.

Many banks offer free unlimited accounts if you keep a combined balance over some low
threshold like $10,000.


## Save Before You Spend

> The ability to discipline yourself to delay gratification in the short term
> in order to enjoy greater rewards in the long term, is the indispensable
> prerequisite for success. [Brian Tracy](https://www.goodreads.com/quotes/23014-the-ability-to-discipline-yourself-to-delay-gratification-in-the)

Separate accounts are only useful if you are placing money aside *before* you spend it. This
is as apposed to using credit. This has numerous benefits:

- You cannot go into debt on a purchase you have budgeted for.
- You can spread the cost of once a year purchases like insurance and Christmas over the course of the year.
- You can blow a large amount of money on something relatively guilt free - you saved for it!

Perhaps the biggest benefit is that it forces you to wait for medium sized purchases. Maybe the
new MacBook is out, and you are really tempted to drop $1300 for it. Great! It should only take
you a few months to save for it. If you're like me, most of the time the impulse to purchase
will fade over the course of even ten days, to the point where you may no longer think it's a
good use of money.

If not, you can do ahead and purchase it without using credit. Plus, you'll know that it was something
you really wanted. You were willing to make some sacrifice for it. To me, delayed gratification is
the definition of maturity.

What about credit card rewards? You can still make purchases on a credit card and pay off
the card from the correct account. Just make sure you are keeping track of your budget. If
you can't consistently stay under budget using a credit card, and especially if you can't pay
off the balance every week, I would recommend not using a credit card.


## Emergency Fund & Contingencies

One of the great things about sticking to a budget is being able to set aside money for the
unexpected. Even in hot job markets like software engineering, there have been regular downturns
where many people find themselves out of work. One common strategy is save six to twelve months
of expenses (rent, food, utilities) in an emergency savings account, and only touch it in the case
of job loss or catastrophic medical issue.

For the example budget, rent, food and utilities total $2,100 a month. To save `6 * 2100 == $12,600`
would take just 25 weeks, or half a year, at the budgeted emergency savings rate of $500. Afterwards,
you free up that money for other things. Just remember to bump your emergency savings as your expenses
increase!

Having an emergency fund is your number one protection against going into debt unexpectedly. Some people
will also dip into the emergency fund for things like replacing a stolen bike or a busted water heater.
Personally, I keep a separate smaller account for items like that called "contingencies".
---
title: Thoughts on Object Oriented Class Design
tags: computer-science object-oriented
---

> The problem with object-oriented languages is they've got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle. - Joe Armstrong

In my experience, class design is an area where software engineers are prone to over-estimate their own abilities. This also happens with SQL and regular expressions. The concepts seem simple enough that it's easy to fool yourself into thinking that knowing the basics is equivalent to being an expert. The reality is that most people could use a refresher on what makes a good object oriented abstraction.


## Encapsulation - Hiding Implementation Details

> [Encapsulation is] the process of compartmentalizing the elements of an abstraction that constitute its structure and behavior; encapsulation serves to separate the contractual interface of an abstraction and its implementation. - Gary Brooch


At the core, class design is about encapsulation. You want to make it easier to think about your problem space by limiting what you need to keep in your brain at any given time. You want classes that hide unnecessary implementation details from the outside world, are well tested, and are easy to use. That allows you to think about your problem space as the interaction of instance of these objects, versus the low level detail.

For example, say you have a class that represents a zipped file. Internally, it uses a regular file descriptor to access the base file. That's not something you want to expose to callers; it's an implementation detail. Maybe you decide later that you want to be able to have the base file be an S3 URL. If you expose the internal file descriptor in the initial implementation, that may end up limiting your options down the road.

In some languages like Python, it's not possible to hide implementation details by marking methods and members as private. Even there, it's well worth using standard conventions for communicating to other developers what should and should not be accessed.


## Cohesion - Consistency of Abstraction

> Cohesion refers to the degree to which the elements of a module belong together. Thus, it is a measure of how strongly related each piece of functionality expressed by the source code of a software module is.

I think of cohesion as the level of consistency in the abstraction. If you have a class that represent a web request, you could conceivably try to handle HTTP methods, URL parameters, headers in one large class with a hundred methods on it. But you're probably better off breaking that down into separate more focussed classes. That way your URL parameters class has just a few methods on it, and they are all related to a small problem space.

One tool I find helpful for writing cohesive classes is to start by thinking about the class API first. What are you going to call the class? What are the methods you are going to provide? By thinking first about how you would ideally like to use this hypothetical class, you can focus on making the API cohesive before getting bogged down in implementation details. Once you have started implementation, it's very tempting to allow the scope of the class to grow.

Good naming is one of the hardest parts of writing any code. It's especially important for writing cohesive classes; if your class methods are becoming too awkward, it may be because your class is doing to much.


## Loose Coupling - Swappable Components

![loose coupling](http://i.stack.imgur.com/OJFhI.jpg)

If your components are encapsulated and cohesive, it should be easy to compose them together. It should also be easy to swap out a particular component for another version that implements the same interface. The degree to which this is easy determines whether your system is loosely coupled.

You can typically make components less coupled by reducing the number of things needed to construct them. Similarly, you can have your methods take standard primitives and collections versus custom objects.


## Common Pitfalls

> The phrase "object-oriented" means a lot of things. Half are obvious, and the other half are mistakes. - Paul Graham

Here are some common mistakes in class design.

- Large interfaces. Even small changes in the API may require many updates.
- Subclass methods that are empty/pass. This may be a sign that the abstraction needs to be refactored.
- Multiple inheritance. Often hard to reason about. Limit yourself to simple mixins if you must do it.
- Tying the interface to the implementation. Example: persistent object that exposes underlying database functions directly.
- Invalid method sequences. If you can't call `process` until you call `open`, then `process` should do that for you.
---
title: Topics for One on Ones
toc: true
tags: manager reading-list newboss
---

I like to ask other managers what kind of questions they ask during their one on ones. Everyone has their own style, so I often get some interesting responses. Getting good information out of your one on one conversations is something of an art. Given the amount of time over a career that you will spend during one on ones, it's worth a small time investment to improve your skills.


## Managing Humans

> Schedule one-on-ones with direct reports, keep them on the same day and time, and never cancel them. - Lopp, Michael ([Managing Humans](http://www.amazon.com/Managing-Humans-Humorous-Software-Engineering/dp/1430243147))

This book is commonly held up as the work of God when it comes to managing people. Here are my blatant crib notes of the text.

- Start with the question "How are you?".
- Your biggest job is to gauge mood.
- Learn something from every one on one.
- Status is not the point.
- Three things that are working, three things that are not, what are we going to do about it?
- Never cancel them, it sends a bad message.

When you're stuck, Michael suggests some emergency topics.

- Prepared points - i.e. context items from GTD.
- Mini performance review.
- Your current disaster.

## Specific Questions

### How is it going?

This is a variation on "How are you"? This does lead to a status update most times, so you often need to redirect. It's important to lead with something that lets people talk about whatever they want.

### How do you feel about coming to work every day?

More direct question to get at their mood/happiness.

### How do you feel about working on project X?

Give the employee permission to say that they don't like their current project, as well as implicitly tell them that it's something we can change.

### How is it going working with person X?

Is there some asshole that's bothering you? Maybe I can help.

### Who are your best friends at work?

For some people, work friend ships are critical.

### Who are the top people outside the company who you want to work with again?

We should try to hire them.

### What features would you add?

Are you thinking about the long-term future of the software? I care about what you think!

### What could I be doing better?

I try to ask this as often as I can, but often forget.

### How comfortable are you with uncertainty?

i.e. - how much insight into the sausage making can you stomach? Is that
motivational for you, or counter-productive?

## Other ideas

One thing I like to do is change the scenery of a one on one. Taking a walk outside is a great idea. I also like to do a one on one over lunch occasionally. On the other hand, you definitely want to be in a traditional conference room if you need to give hard feedback, or have a tough conversation.

## The Coaching Habit

[Recommended reading](https://www.amazon.com/Coaching-Habit-Less-Change-Forever/dp/0978440749).
The seven questions they recommend work well for one on ones, too:

- What's on your mind?
- And what else?
- What's the real challenge here for you?
- What do you want?
- How can I help?
- If you're saying yes to this, what are you saying no to?
- What was most useful for you?
---
title: Introducing the Software Engineering Fireside Chat Podcast
tags: podcast
---

I'm pleased to announce the first episodes of the [Software Engineering Fireside Chat](https://itunes.apple.com/us/podcast/sofware-engineering-fireside/id978896299) podcast are available for download. Along with my colleagues [Scott Lobdell](https://twitter.com/slobdell3) and [Jordan Eldredge](https://twitter.com/captbaritone), we are going to be talking about the tech startup scene in the Bay Area. Instead of making this a purely technical podcast, we are actually going to cover ancillary topics such as how to survive tech busts, how to transition from QA to engineering, the state of hacking bootcamps, etc.

Here is an over-view of the initial episodes:

## Episode 0 - Introduction

Scott, Jordan and I introduce ourselves and give an overview of our backgrounds. We talk about how we got into coding in the first place, how we ended up working for a startup, and what kind of projects we work on outside of work.

## Episode 1 - Debbie Sterling

We interview [Debbie Sterling](http://www.engineergirl.org/Engineers/Directory/13512.aspx), engineer and founder of [GoldieBlox](http://www.goldieblox.com), a construction toy and book series targeted at young girls. Debbie shares with us how she got started, and some of the challenges along the way.

## Episode 2 - Ben Henry

[Ben Henry](https://twitter.com/bjhenry), a director of engineering at
[Hearsay Social](http://hearsaysocial.com/) talks about tech bust cycles in the Bay Area. Specifically, we discuss how to limit your exposure to crisis like the original dot-com bust in the early 2000's, as well as the financial crisis in 2008.

## What about future episodes?

We are planning to release one episode a week, on Fridays. If you're not an iTunes person, or you just love raw RSS, you can visit our [podcast page](http://fireside.libsyn.com/) on libsyn.

## Call for guests speakers!

If you're interested in being on the show, drop us a line on Twitter!
---
title: Lessons Learned at My Most Recent Job
tags: newboss
---

Today is my first day of "unemployment" in the last 12 years. I'm switching jobs, soon to start at NerdWallet. This time, I decided to take a couple of weeks off in between to reset and reflect. Here are some thoughts from my last job.

## Work (only) on projects with impact

Don't waste time on projects that don't have impact. In any organization, there are many projects that need to be done that may not be "sexy". That doesn't mean they are not valuable. But try to pick projects that are at least of critical importance, if not interesting.

Note: be especially wary of projects that you feel passionate about. Sometimes that can blind you to the (lack of) upside. Double-check with an impartial third party that they will have high impact.

## Stop trying to kill dead snakes

Hopefully you have an organizational culture where big decisions are made with lots of input. While a big decision is in the works, argue your convictions passionately, but hold those convictions loosely. Be open to having your mind changed. Either way, once the decision is made behind closed doors, you must support it in front of the team at large.

Don't waste time going forward second guessing decisions that have already been made. If you just can't stand it, open up the discussion behind closed doors again.

## Get everyone on board

For any big decision, but especially for things that you're personally pushing for, it's absolutely critical to get everyone on board with the decision. This doesn't just mean a few people. Start by having private discussions ahead of time with key individuals. One on ones are great for this. Have them bring it up with **their** one on ones. Make sure the key players are on the same page, and can echo back the reasoning, as well.

The easiest time to get everyone on board is **before** rolling something new out. The second easiest time is during roll out. The hardest time is after the fact.

## Go around difficult people

If you can't go through a key person to get something necessary done, you may have to go around them as a last resort. For the most part, people only have as much power over you as you let them have. Sometimes it can be hard to see that. Don't be afraid to change the power dynamic by simply refusing to respect it.

## Power voids are made to be filled

Any time someone just above you in the organization leaves, it creates a power vacuum that is just waiting to be filled. These are great times to double down, take on more responsibility and work harder. Don't wait for someone to ask you to take those responsibilities over.
---
title: Docker/OSX Quickstart (not grokking docker yet? start here)
tags: docker
---

Docker has only been around since 2013, but it seems like it's all over my Twitter feed and RSS reader. I've gone trough the "Hello world" example in the past, but never felt like I really understood either the value proposition, or exactly how it works. This week, I had some time to sit down and give it more of my attention. What I found was that it was neither as mysterious or as complicated as I anticipated.

## Installing on a Mac

Docker was born on Linux and uses Linux internals like [LXC](https://linuxcontainers.org/) to work its magic. There is a Windows native version in the works (not that anyone cares). But given that software engineering in the Bay Area is dominated by Macs, let's start by looking at how to get this installed and running on OSX.

First off, don't try to install it via `brew`, or any other package manager. Docker is written in Go, which has the advantage of compiling down to dependency-less binaries. Plus, the project is moving so fast that the versions in package managers are out of date. So, suck it up and install it manually by [downloading the binary](https://github.com/boot2docker/osx-installer/releases/tag/v1.5.0).

If you can open a terminal and run `docker --version`, you're good to go. This tutorial is for version 1.5.0.

## Boot2Docker

If you try to run a docker image now, you will get an cryptic error like `docker max dial unix /var/run/docker.sock: no such file or directory`. This is because the Docker daemon process is not running. Actually, it cannot run on a Mac! Instead, you *must* use [boot2docker](http://boot2docker.io/), which is a tiny virtual machine that runs in VirtualBox and has the Docker daemon. Again, use the binary installer (sorry!).

To get up and running, open a terminal the run the following.

```bash
boot2docker init
boot2docker up
eval "$(boot2docker shellinit)"
docker run ubuntu:14.04 /bin/echo 'Hello world'
```

That's your hello world example. Let's breakdown what's happening here. `boot2docker init` creates a new virtual machine in VirtualBox.

![boot2docker](/blog/images/boot2docker.png)

The next step, `boot2docker up` runs the virtual machine. The `eval "$(boot2docker shellinit)"` step is setting some environment variables that tell Docker what container context you are currently in. If you run just `boot2docker shellinit` by itself, you can see the raw output:

```bash
Writing /Users/chase/.boot2docker/certs/boot2docker-vm/ca.pem
Writing /Users/chase/.boot2docker/certs/boot2docker-vm/cert.pem
Writing /Users/chase/.boot2docker/certs/boot2docker-vm/key.pem
export DOCKER_HOST=tcp://192.168.59.104:2376
export DOCKER_CERT_PATH=/Users/chase/.boot2docker/certs/boot2docker-vm
export DOCKER_TLS_VERIFY=1
```

The first three lines are just informational, only the last three lines are printed to stdout.

The last line, `docker run ubuntu:14.04 /bin/echo 'Hello world'` actually instantiates a new Docker container (using Ubuntu 14.04), and runs a single command inside it.

### A Note about Containers

Containers are little sandboxed Linux instances. Images are the serialized file definition that containers are spun up from. The magic of Docker is that the images are completely portable. This concept escaped me at first. I was under the impression that you needed to build an image on your Mac to run it there, and then build a separate image on Amazon EC2 to run the same thing there.

In fact, you can build an image on your Mac, and then essentially `scp` that file up to AWS and run it. In reality, you don't even need to copy it manually, that's what Docker Hub is for.

Also, the Linux distribution used inside your Docker container does NOT have to match the distribution of the host operating system. You can run Ubuntu inside a CentOS host, and visa-versa.

Finally, images have a built-in [layering mechanism](https://docs.docker.com/terms/layer/). Essentially, you can have a base image and then any number of small layers of diffs on top of that. This is a powerful optimization and abstraction, which we will talk about later.

## Example Python Flask App

This is the canonical tutorial for Python folks getting started with Docker, and yet I could not complete is successfully with any of the documentation I found. Here is my own special snowflake version.

First, create a new directory called `flask`. Inside, you are going to create three files.

The first file is called `app.py`, which is just a simple hello world Flask app.

```python
from flask import Flask
import os
app = Flask(__name__)

@app.route('/')
def hello():
    return 'Hello World!'

if __name__ == "__main__":
    app.run(host="0.0.0.0", debug=True)
```

Then, create a `requirements.txt` file to list Flask as a dependency:

```bash
Flask==0.10.1
```

Finally, create your `Dockerfile`:

```bash
FROM python:2.7
ADD . /code
WORKDIR /code
RUN pip install -r requirements.txt
EXPOSE 5000
CMD python app.py
```

Let's take a moment and breakdown this last file. The `FROM` line tells Docker to base this container off of a named image in the public repository called `python`, and to use the named tag of that image (kind of like a version) of `2.7`.

The `ADD` line copies your code from the current directory `.` to `/code` inside the Docker container Linux instance. `WORKDIR` settings the working directory there as well.

`RUN` can be specified multiple times. It tells Docker to run these commands when building the container for the first time. Run steps are actually cached; changing one of them later will only result in that one being run again. This is possible due to the container layering we talked about earlier.

`EXPOSE` tells Docker that the container will be serving port 5000 externally. This is the port we will run the flask app on.

Finally, the `CMD` line specifies the command that will run inside the container as your main daemon process. If you need multiple daemons, look into [docker-compose](https://docs.docker.com/compose/).

### Run it

To run the example, execute the following commands:

```bash
open "http://$(boot2docker ip):5000"
docker build -t flask-example .
docker run -it -p 5000:5000 -v $(pwd):/code:ro flask-example
```

This should have opened a browser tab before spawning flask. That likely came up with a "This webpage is not available" error page, but if you refresh it now, you should see your "Hello World!" text.

What you have done is create a named image called `flask-example` and run it. You can even edit the code on your local file system and it will sync over to Docker (thanks to `-v`) and flask will restart.

## Running the same container on AWS

Now, let's look at how to run that same container on AWS. First, go sign up for [Docker Hub](https://hub.docker.com/). It's free.

Let's assume your Docker Hub username is `foobar`. First, re-build and publish your image:

```bash
docker build -t foobar/flask-example .
docker login
docker push foobar/flask-example
```

Now, create a new EC2 instance. Make sure to use the "Amazon Linux" base image, which will make installing Docker easier. SSH into your instance and run the docker container:

```bash
sudo yum install -y docker; sudo service docker start
sudo docker run -it -p 8000:5000 foobar/flask-example
```

The first line simply installs Docker and starts it. The second line pulls down your image from Docker Hub (note: no need to authenticate!), runs it in an interactive shell, and maps the external port 8000 on the host EC2 instance to port 5000 inside the container.

If you have your security group setup to expose port `8000`, you should be able to open this EC2 public host name on port 8000 in a web browser.

# More Stuff

When I was getting started with this, I made the mistake of reading about and trying to leverage `docker-compose` and `docker-machine` right away. These are official plugins, which ease the configuration of multi-service and multi-machine capabilities in Docker, respectively. I suggest NOT starting in with those until you have the above basics buttoned down. I found that they clouded my understanding of what was happening at first.
---
title: Book Review - Leadership and Self Deception
tags: book-review newboss
---

This is a review of [Leadership and Self-Deception: Getting Out of the Box](http://www.amazon.com/Leadership-Self-Deception-Getting-Out-Box-ebook/dp/B00GUPYRUS) by The Arbinger Institute.

![cover](/blog/images/leadership-and-self-deception.jpg)

## Summary

- You're "in the box" when you're viewing others around you as mere objects and not as people.
- Other people respond primarily to how to see them, not how to act towards them.
- Everyone is in the box towards some people, and outside of the box towards others, all the time.
- You get in the box by not acting the way you know you should.
- When you're inside the box, you *need* to feel justified, and can't focus on results. This is known as self deception.
- This distorts how you see other people.
- Over time, your boxes can become baggage that you bring to new relationships.
- When you're in the box, you invite others to be in the box towards you.
- Only seeing people as people AND stopping resisting them will get you out of the box.
- Things that DON'T get you out of the box: try to change them, cope with them, leave, change your behavior or even communicate better.

## Commentary

The book is written in the form of a narrative of a fictional new employee at a fictional company, going through a training session on leadership. This is a somewhat surprising format for essentially a self-help/business/management book. But it works for me, I find it to make the subject matter easier to retain. It's also an easy read.

The concept of being "in the box", is simultaneously intuitive in the broad strokes and counter-intuitive in the details. The key insight from my perspective is that you get in the box by betraying your own sense of how you should act in a given situation. I have found myself reflecting on this a lot since I read the book. It's also caused me to rethink some of my previous working relationships.

I've also been trying to think of when I have been victim to self deception in the past. By the very nature of the problem, it's hard to know whether or to what extent you're effected. Perhaps it's best to focus on how to flag self deception in the future. Many corporate cultures have a mechanism for attempting to get this kind of feedback. Even so, I can't help but think that it's often not very effective. I will make a point in the future to explicitly ask people I work with how I am currently deceiving myself.
---
title: Reading List (my common references)
tags: reading-list
---

This post is list of articles, blogs and other resources that I commonly reference.

# Books

- [Rocket Surgery Made Easy](http://www.amazon.com/gp/product/B002UXRGNO) - Usability testing
- [The Bogleheads' Guide to Investing](http://www.amazon.com/gp/product/0471730335) - Saving for retirement
- [Leadership and Self-Deception](http://www.amazon.com/gp/product/B00GUPYRUS) - Management
- [Managing Humans](http://www.amazon.com/gp/product/1430243147) - Management
- [Getting Things Done](http://www.amazon.com/gp/product/0142000280) - Organization
- [Code Complete](http://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670) - Coding
- [The Mythical Man-Month](http://www.amazon.com/Mythical-Man-Month-Software-Engineering-Anniversary/dp/0201835959) - Project management
- [Agile Software Development with Scrum](http://www.amazon.com/gp/product/0130676349) - Agile
- [The Coaching Habit](https://www.amazon.com/Coaching-Habit-Less-Change-Forever/dp/0978440749) - Management

# Articles

- [Three Snakes](http://www.celebrazio.net/jimb/15.html)
- [The Twelve Factor App](http://12factor.net/) - Tenets for building microservices
- [Give it Five Minutes](https://signalvnoise.com/posts/3124-give-it-five-minutes) - Knee jerk idea dismissal
- [The Hiring Post](http://sockpuppet.org/blog/2015/03/06/the-hiring-post/) - Software engineer interviews suck
- [SQLAlchemy for Django Devs](http://lucumr.pocoo.org/2011/7/19/sqlachemy-and-you/)
- [15 Fundamental Laws of Software Development](http://www.exceptionnotfound.net/fundamental-laws-of-software-development/)
- [The Scrum Guide](http://www.scrumguides.org/scrum-guide.html)
- [IT'S NOT A PROMOTION - IT'S A CAREER CHANGE](http://fractio.nl/2014/09/19/not-a-promotion-a-career-change/)
---
title: REST API Design
tags: api
---

[REST](http://en.wikipedia.org/wiki/Representational_state_transfer) APIs operate over HTTP, use standard verbs like `GET` and `POST`, expose a common-sense URL structure and return resources in a well-defined format, typically JSON.

In spite of that simple definition, there is a wide degree of latitude when designing a REST API to fuck it up. Don't do that! Follow the guiding principle of making things as easy as possible for the calling developers. Provide the correct level of granularity, reduce the number of calls they need to make, and document the heck out of it.

In terms of granularity, I'm referring to whether the API exposes individual database records directly, with a thin CRUD layer, versus composing those into higher level resources that represent the problem domain the way your users think of it. The CRUD model makes a lot of sense for internal low-level data APIs, whereas the higher level model makese more sense for exposing to the outside world, as well as internal front-end and mobile developers. Remember - your relational data model details almost never aligns with the user's mental model of how your application works.

For the rest of this article, I'm going to be talking primarily about higher level APIs, hereto referred to as an [Intent API](http://www.thoughtworks.com/insights/blog/rest-api-design-resource-modeling).

## Resource Semantics

What's the difference between an Intent API and a CRUD API? The level of granularity. Instead of exposing implementation details of your actual database scheme (which may change), we instead expose higher order concepts of what the user intention is for your actual use cases.

For example, in a CRUD API for a bank, you might expose [Resources](http://restful-api-design.readthedocs.org/en/latest/resources.html) for Accounts, Account Holders and Transactions. You would allow callers to create an Accout record, and you would allow the creation of Transactions. A caller might implement a transfer between two accounts as two transactions; one debit from account A and one deposit to account B. Hopefully you can also make those two operations an atomic transaction somehow.

With an Intent API, you would probably not allow Account or Account Holder creation at all. Those are likely off-line manual tasks that you want performed by actual humans, or at the very least your own internal services, which then use the private CRUD API. You also likely to not want to allow the creation of a Transaction directly. But maybe you expose a Transfer Resource, as well Purchase and ChargeBack Resources.

What does this get you? By mapping to the intention of the user - what they are actually trying to accomplish - you have the opportunity to tailor an API endpoint to just the set of parameters that make sense for that operation. For a Transfer, you need two Account IDs. For Purchase, you need Vendor metadata. For a ChargeBack, you need a previous Transaction ID.

You also have the ability to ensure that operations are atomic and leave the data in a valid state. If a Transfer fails the second part of a two-stage commit, you can roll the first part back. You do not have to rely on the caller to do that properly.

What you have done is remove the burden of implementing business logic that's specific to your system from the callers, and placed it inside your system (where it belongs).

It's true that this is not strictly RESTful; you're exposing verbs as your Resources. You are likely going to expose nouns as well (Account, Transaction, Vendor), but your any call that's not [idempotent](http://en.wikipedia.org/wiki/Idempotence) should probably be a verb.

For an example of this in action, see the [GitHub API](https://developer.github.com/v3/repos/merging/):

```bash
POST /repos/:owner/:repo/merges
{
  "base": "master",
  "head": "cool_feature",
  "commit_message": "Shipped cool_feature!"
}
```

Notice that semantically speaking you're not creating a Commit Resource that actually represents the merging of two branches in git. Most users of git don't even understand what is actually happening to the data model for a merge; don't make your API callers have to understand it, either.

How do you know which intents to model for? I would start by looking at your core user scenarios, and think about what Resources you would want to have for each one, keeping in mind that you want to minimize the number of API calls required to display the user interface. You have to balance that against the cohesion and separation of concerns  of each Resource.


## URL Structure

On to implementation details! What should your URL structure look like? In general, you want to pick either plural or singular nouns and verbs and stick with that. I'm going to be opinionated and say that you should use plural nouns and singular verbs. For example:

- `/accounts` - list all Accounts
- `/accounts/123` - get the Account with ID 123
- `/accounts/123/transactions` - list Transactions associated with this Account
- `/accounts/123/transactions/123` - get a particular Transaction inside an Account
- `/transactions/123` - get the same Transaction outside the context of an Account
- `/transfer` - Create a transfer between two accounts (POST)

*Note:* there is no problem with exposing the same Resource at multiple end points. This is not a DRY model; remember that our guiding principle is making things easy for the developers. Maybe they don't know what Account is associated with a given Transaction.

Finally, you want to think about what content to display at the root endpoint of `/`. I have seen some APIs that use that endpoint as an opportunity to include likes to developer documentation and/or a list of all the endpoints in the system.


## Requests

The biggest decision here is how to take data from the caller. Most REST APIs will support URL parameters for most use cases. If you're doing that, make sure to support form POST encoding as well, it should be no extra work. These work well for simple key/value parameters, and are easy to implement for the caller.

For nested data, you have a choice of supporting JSON request bodies or using some type of prefix scheme in your key/value pairs. For example, you could represent the following JSON body:

```bash
{
    "user": "Chase Seibert",
    "account": {
        "id": 1,
        "name": "foobar"
    }
}
```

As the key/value pairs `?user=Chase Seibert&account__id=1&account__name=foobar`. Personally, I think that's both ugly and hard for the caller to implement in some cases.

Whatever you choose, make sure to inspect and respect the callers content-type.


## Metadata & Responses

For each API response, you want to have a consistent set of metadata that the callers can rely on being there, as well as a consistent overall packet structure. For example, you likely want to have well defined fields for pagination, results and error messages. But you may also want to include less obvious items, like an echo of the parameters that the user passed to you. This can be useful as signal that you have unambiguously received the arguments, parsed them out correctly and that they are valid for this API call.

Pagination is typically done by supporting something like `limit`, `offset` and `sortBy` as URL parameters. Then you include `nextPage` and `previousPage` fields in your response *which are absolute URLs* to those results in the API. *Note:* I'm using `camelCase` here versus `snake_case`. Given that most API consumers these days are either Javascript apps or native mobile apps (objective-c or Java), it might make sense to use their conventions and go with camel case. Just be consistent.

Error messages are great for developer sanity. Of course you want the primary signal of an error to be the proper HTTP status code for that error class.


## Versioning

You probably want to think about an API versioning strategy up front. In its simplest form, this is just a prefix like `/v1` that you prepend to every API endpoint. The idea is to plan ahead for having multiple supported versions in flight at the same time, to give developers a gentle transition for breaking changes to the API.

But how do you architect your code to be able to serve multiple versions? The heavy handed approach is to fork the code for each supported version. This is fairly simple, and has the advantage of being very predictably stable for older versions. A different approach might be to have the same codebase serve multiple versions. In that case, you will likely want to retain multiple versions of a subset of the unit tests, in addition.

A third hybrid approach would be to fork either with source control branches, or with actual running VMs or containers with that code on it. This has the disadvantage of making the older versions difficult to patch, either for hotfixes or infrastructure changes.

The most important thing is to have a plan up front. I would recommend launching with both a `/v1` and a `/v0` API that have some backwards incompatible difference, even if it's just a dummy endpoint that is removed in version 1.


## Authentication

If you're producing a public facing API, you almost certainly want to use OAuth. Don't write your own, find a framework. Even if your API is restricted to internal use, you should think about including at least some kind of caller identifier mechanism. This can come in handy when you go to produce analytics on who is using the API, in addition to being a prerequisite for rate limiting per caller.

Whichever auth mechanism you choose, you will want 100% of the API calls to be over HTTPS, so as not to leak those credentials. Don't even support a HTTP option.


## Documentation & Developers

Almost as important as the semantics of the API is having excellent, comprehensive documentation. Don't rely on automatically generated documentation here. Or, at least add a lof of explanatory detail about why the developer might want to use the API, and what each piece of the request and response mean. It's especially tricky to put yourself in the place of a person who doesn't know the intimate details of the models in your sytem. Run it by a third party for a sanity check.

Along with the text documentation, you will want to supply full, untruncated examples for common requests and responses. Go ahead and make sure they are pretty printed and perhaps even syntax highlighted. I would also recommend that you pretty print the actual API responses from the server.

A great way to expose examples is with interactive consoles. If you provide HTML documentation, you have even make the examples executable and tweakable in-line. The [Django REST Framework](http://www.django-rest-framework.org/) is a great example of this.


## Python Tooling

Here are some common frameworks for writing REST APIs in Python:

- [Django REST Framework](http://www.django-rest-framework.org/)
- [Flask RESTful](https://flask-restful.readthedocs.org/en/0.3.2/)
- [Flask API](http://flask.pocoo.org/docs/0.10/api/)

A good utility for working with REST APIs is [Bunch](https://github.com/dsc/bunch), which lets you easily translate between JSON API responses and Python objects. You can also go the other way, which may be useful for mapping your database objects to JSON.

For versioning, check out [Flask blueprints](http://flask.pocoo.org/docs/0.10/blueprints/#registering-blueprints).

Finally, depending on whether your API is internal or external, you can look into tools for creating API consoles:

- [Apigee](http://apigee.com/about/)
- [HurLit](https://www.hurl.it/)
- [RAML API Console](http://raml.org/projects.html) - [example](https://anypoint.mulesoft.com/apiplatform/popular/#/portals/apis/6308/versions/6302/pages/31760)
---
title: Flask absolute import bug in debug mode
tags: python flask
---

Ran into a vexing issue this week. I was getting errors trying to use absolute imports in a new flask app:

```bash
Traceback (most recent call last):
  File "nw_api/example/run.py", line 5, in <module>
      from nw_api.docgen.base import DocumentationGenerator
      ImportError: No module named nw_api.docgen.base
```

I was able to use relative imports (i.e. `from docgen import base`), but that's generally considered bad practice. Plus, it was just weird. My unit tests were working with either method, which finally lead me to the offending line:

```python
flask_app.run(debug=True)
```

It turned out that setting `debug=False` fixed the problem. Of course, debug mode is really useful, so I needed to figure out exactly what was going on. That turned up this [werkzeug bug](https://github.com/mitsuhiko/werkzeug/issues/461) titled "Reloader, python -m, and sys.path".

Werkzueg, which provides a lot of Flask functionality (including the [web based debugger](http://werkzeug.pocoo.org/docs/0.10/debug/)), is doing something pretty tricky to implement hot-reloading of the source when you change a Python file. It spawns a subprocess to and loads the code again. However, it does not pass all of the arguments of the original python command line, notably missing the `-m` argument, which is how you get python to run a module versus a single file:

```bash
venv/bin/activate; (cd src && python -m nw_api.example.run)
```

That was the run line from my original Makefile. Instead, I had to go with this:

```bash
venv/bin/activate; (cd src && export PYTHONPATH=${PYTHONPATH}:nw_api && python nw_api/example/run.py)
```

Success! What's going on here is that to avoid using `-m`, you need to update your `PYTHONPATH` environment variable to include the (relative in this case) path to your module. Then you can run any single file in that module and it will pick up absolute imports for the rest.
---
title: Python Mock Cookbook
tags: python testing
---

The python [mock](https://pypi.python.org/pypi/mock) library is one of the awesome things about working in Python. No matter what code you're unit testing, it's possible to mock out various pieces with very little test code. That being said, it's sometimes difficult to figure out the exact syntax for your situation. I attribute this to the nature of how you apply the mocks. Sometimes it feel like you're shooting in the dark.

The [official documentation](https://docs.python.org/3/library/unittest.mock.html) is comprehensive, but I find it somewhat hard to locate what you're looking for. I recommend their [examples doc](http://www.voidspace.org.uk/python/mock/examples.html).

This post is a write-up of my own personal usage.

# Big Upfront Caveat

The biggest mistake people make is mocking something out in the wrong place. *You always need to mock the thing where it's imported TO, not where it's imported FROM.* Translation: if you're importing `from foo import bar` into a package `bat.baz`, you need to mock it as `@mock.patch('bat.baz.bar')`. This can be confusing if you think you should be mocking it where it's defined, not where it's used.

# Setup

For all these sections, assume we're in a package called `myapp`. The code you're testing is in a module at `myapp.app` and the definition of the objects that you're mocking is imported there from `myapp.lib`.

Want to see the full code? I have an repository on git with these examples called [python-mocking](https://github.com/chase-seibert/python-mocking).


# Constants

The easiest things to mock out are constants.

```python
@mock.patch('myapp.app.MAX_ITEMS', 7)
def test_constant(self):
    ...
```

# Functions

For functions, you will commonly need to specify a return value, check if they were called, and with what values.

```python
@mock.patch('myapp.app.get_first_name')
def test_function(self, mock_get_first_name):
    mock_get_first_name.return_value = 'Bat'
    ...
    mock_get_first_name.assert_called()
    mock_get_first_name.assert_called_once_with('baz')
```

# Methods

Mocking a method on a class is just like mocking a function, you just reference it through the class name.

```python
@mock.patch('myapp.app.Car.get_make')
def test_method(self, mock_get_make):
    mock_get_make.return_value = 'Ford'
    ...
    mock_get_make.assert_called()
```

# Properties

These are just special methods on a class with the `@property` decorator. Now we're starting to get tricky.

```python
@mock.patch('myapp.app.Car.wheels', new_callable=mock.PropertyMock)
def test_property(self, mock_wheels):
    mock_wheels.return_value = 2
    ...
```

# Entire classes

What if you want to swap out an entire class implementation? No problem! The key is that the `return_value` should be a new instance of the class.

```python
@mock.patch('myapp.app.Car')
def test_class(self, mock_car):

    class NewCar(object):

        def get_make(self):
            return 'Audi'

        @property
        def wheels(self):
            return 6

    mock_car.return_value = NewCar()
    ...
```

# Class Methods

What about a `@classmethod` on a class? It's the same as a method.

```python
@mock.patch('myapp.app.Car.for_make')
def test_classmethod(self, mock_for_make):
    new_car = Car()
    new_car.make = 'Chevy'
    mock_for_make.return_value = new_car
    ...
```

# Static Methods

Static methods are the same as class methods.

```python
@mock.patch('myapp.app.Car.roll_call')
def test_classmethod(self, mock_get_roll_call):
    mock_get_roll_call.return_value = [Car('Ford'), ]
    ...
```

# Decorators & Context Managers

Decorators are a tough one. They are defined at import time, and are thus diffucult to re-define as a mock. Your best bet is to create a function for the body of the decorator, and mock that.

Context managers are more do-able, but tricky.

```python
@mock.patch('myapp.app.open_car')
def test_context_manager(self, mock_open_car):

    def enter_car(car):
        pass

    mock_open_car.return_value.__enter__ = enter_car

    ...
```

# Bonus - Mocking All Tests in a Suite

Say you have a certain mock that you want to apply to all tests in a TestCase class. You have two options. You can apply the patch in the `setUp` and un-apply the patch in `tearDown`, or you can over-ride `run`.

```python
def run(self, result=None):
    with mock.patch('myapp.app.foo') as foo:
        self.foo = foo
        super(MyTestCase, self).run(result)
```

Alternatively, you can mock out something in `setUp`:

```python
def setUp(self):
    patcher = mock.patch('myapp.app.foo')
    self.mock_foo = patcher.start()
    self.addCleanup(patcher.stop)
    super(NWApiTestCase, self).setUp()
```
---
title: How to write effective unit tests
tags: testing
---

Unit tests differs from integration testing primarily in terms of what you're testing for. Where as with integration tests, you testing for whether the entire system behaves as expected when put together, with unit tests, your goal is simply to enable refactoring with confidence. Ideally, when you refactor something and it's broken, at least one unit test fails. But when you refactor something and it's working, unit tests pass.

Integration tests are naturally  *high leverage*; you can typically test a large swath of functionality with not much code. But they also are typically slower to run, test fewer edge cases and tend to not give you a very good idea of what is broken when they fail. To make sure your unit tests are complimentary, you want to make sure they are fast, test many edge cases, and test only one thing so you know what's broken when they fail.


## What not to do

It's common to have a large set of unit tests written that don't actually add much value. If you have many unit tests testing the same thing implicitly, then they will all fail at the same time. For example, you're testing all your Flask views, and many of them have a decorator to test if the user is logged in. If you break that decorator, many tests will fail. Ideally, you want to test the decorator itself in one set of tests, and then have the rest of your tests mock that out.

In the later case, what happens when you go to refactor how login works? Hopefully, you only have to update a handful of tests. Going through the process of mocking things out and only testing one unit at a time will also improve the quality of the code itself. You will see how your components could be designed better for separation of concerns, so that they are testable separately.

Some common anti-patterns:

- Unit tests for one unit of code allow that code to actually call into all it's dependencies
- Unit tests load a large database before they start
- Unit tests are slow, so you hate adding more and running them
- Assert on many things in one test, or assert on an entire nested JSON object, which is the same thing
    - Just think of what will happen if you ever change that JSON schema
- Forget to test branches in your code's logic
- Tests are verbose


## What to do, instead

- Mock other parts of the code base
- Mock database access when you can
    - or, make sure database access in your tests is fast to setup and reset between tests
    - you can leverage database transactions for this
- Devise a mechanism to run just one test, or a small set of tests, on demand
- Every logical branch in your code gets its own unit test
- Tests employ same DRY principles as any other code


## Example: a Flask app

Here is an example Flask app with a fictional ORM layer.

```python
from flask import Flask
from flask.json import jsonify

app = Flask(__name__)

@app.route('/user/<int:user_id'>/)
def get_user(user_id):
    if not request.user:
        return jsonify({'error': 'You are not logged in'})
    try:
        user = User.objects.get(id=user_id)
    except NotFoundError as e:
        return jsonify({'error': e})
    return jsonify(dict(id=user.id, name=user.name, email=user.email))
```

What tests would you want to write for this code, and how would they keep isolated from the rest of your code?

- Test that if you make do a `GET /user/1` that your view is called with a `user_id` of 1.
    - You *don't* need to test `@app.route` per say, but you do need to test that the part you have customized (the URL) is working.
    - Test what happens when you call this route with a non-integer.
    - Just verify that `get_user` is called with the right parameter, do NOT execute the body in this test. If you did, a breakage there would fail this test as well as subsequent tests.
    - For example, what if you misspelled `/user`, or forgot to specify that `user_id` was an `int`.
- Test what gets returned if `request.user` is NOT defined. That's a branch in your code.
    - If you turned that into a decorator, you would just want to verify that the decorator executed, but you could defer the testing of the logic to tests for that component. This is an example of improving your component design via testing.
- Test what happens if the `user_id` is not found.
- Test the return value of a successful call.
    - Again, this could be made better by having a `User.to_json()` method which is tested separately.
    - In that case, you would just assert that the return value is equal to `User.to_json()`, not what the actual JSON is.

For some of these test cases, you will need to actually create a record in the database for that `user_id` first. Your testing framework should give you a convenient place to do that. Again, the best way to do that is to write code to create just the record you need for this set of tests, versus running your tests against a full database backup. The later can be quite hard to maintain, and is generally slower.
---
title: Customizing Celery with Task Arguments
tags: celery
---

[Celery](http://www.celeryproject.org/) is an awesome distributed asynchronous task system for Python. It's great out of the box, but a couple of times I have needed to customize it. Specifically, I want to be able to define behavior based on a new `apply_sync` arguments. Also, it would be nice to be able to pass state to the worker tasks.

First, you can subclass the main `Celery` class to define a custom `Task` class.

```python
import socket

from celery import Celery, Task
from kombu.exceptions import InconsistencyError


class MyCelery(Celery):
    """ Subclass of a Celery application class that uses a custom Task type """
    task_cls = 'myapp.mymodule:MyTask'

```

In your `Task` class, you can override `apply_async` (which is also called from `delay`), as well as `__call__`, which wraps around the actual task body.

```python


class MyTask(Task):

    abstract = True

    def apply_async(self, args=None, kwargs=None, task_id=None, producer=None,
                    link=None, link_error=None, **options):
        """ invoked either directly or via .delay() to fork a task from the main process """

        # parse any custom task options from the .delay() or .apply_async() calls
        safe = options.pop('safe', False)  # safely trap errors talking to celery broker

        if 'headers' not in options or options['headers'] is None:
            options['headers'] = {}
        options['headers'].update({
            'safe': safe,
        })

        try:
            return super(MyTask, self).apply_async(
                args, kwargs, task_id, producer, link, link_error, **options)
        except (InconsistencyError, socket.error) as e:
            # InconsistencyError == cannot find the celery queue
            # socket.error == cannot talk to the queue server at all
            if not safe:
                raise

    def __call__(self, *args, **kwargs):
        """ execute the task body on the remote worker """
        safe = self.get_header('safe')
        try:
            return super(NWTask, self).__call__(*args, **kwargs)
        except Exception:
            if not safe:
                raise

    def get_header(self, key, default=None):
        return (self.request.headers or {}).get(key, default)
```

In this example, I'm introducing an optional `safe` argument to `apply_async`, which traps and ignores specific exceptions trying to fork the task. It also piggy backs on the celery task headers to pass itself to the worker process, where it ignores any exception thrown by the task itself.
---
title: Getting Started with Sphinx docs
tags: python
---

Everyone knows that you should write documentation for your code. Writing documentation actually is not that hard. Like any type of writing, the hardest part is beginning. So how do you begin writing documentation for your Python projects?

# Enter Sphinx

[Sphinx](http://sphinx-doc.org/) is the de-facto standard in the Python community for generating documentation for your projects. It's simple to create HTML/PDF files with code samples, tables of contents, and built in search. Many popular projects host their Sphinx docs on [readthedocs.org](https://readthedocs.org/). A couple of good examples are [celery](http://celery.readthedocs.org/en/latest/getting-started/first-steps-with-celery.html#application) and [flask](http://flask.readthedocs.org/en/latest/quickstart/).

One of the key feature of Sphinx is that it allows you to generate as much of the documentation as possible from comments in your Python code. It can also automatically pull the signatures of your modules, classes, functions and methods. Together, these features allow you to keep most of your documentation up to date automatically. Of course, you will also want to write higher level pages on specific topics, and you can easily define those in either [Markdown](http://daringfireball.net/projects/markdown/) or [reStructuredText](http://docutils.sourceforge.net/rst.html), both of which render as rich text when viewed directly in your GitHub repository.


# Quickstart

First, you need to install Sphinx. Then you run their quickstart command, which prompts you for configuration options. You're pretty safe use the defaults in most cases. **The only sphinx-quickstart options I typically customize are enabling autodoc and using "docs" as the project root.**

Here are the basic console commands. You would run these from inside your project root.

```bash
sudo pip install sphinx
sphinx-quickstart
cd docs
make html
open _build/html/index.html
```

This will be sufficient to get a basic HTML document. Typically my next steps will be to integrated with my project README, and configure autodoc to find my code.

# Update your README

I usually have my READMEs defined in Markdown, but for the purposes of Sphinx I think it's worth it to switch to reStructuredText, simply so that you can include the README as the first page of your docs. reStructuredText is pretty simple, here is a quick example of a `README.rst` to get you started.

```bash
===========================================
 IPython: Productive Interactive Computing
===========================================

Overview
========

Welcome to IPython.  Our full documentation is available on `our website
<http://ipython.org/documentation.html>`_; if you downloaded a built source
distribution the ``docs/source`` directory contains the plaintext version of
these manuals.  If you have Sphinx installed, you can build them by typing
``cd docs; make html`` for local browsing.

Instant running
===============

You can run IPython from this directory without even installing it system-wide
by typing at the terminal::

   python -m IPython
```

This example illustrates how to format titles, subtitles, links, inline code and code blocks.

# Include the README.rst in your docs

Edit your `docs/index.rst` file to include the README:

```bash
.. include:: ../README.rst
```

If that's all that's in your index file, and you run `make html` again, you should see your README contents.

# Break out into more than one file

Next, we will start breaking out our documentation into multiples files. Simply create a new reStructuredText file along side `index.rst`. You can call it anything, for example `example.rst`. You can then include this file in your index like so:

```bash
.. include:: ../README.rst

Read More
---------

.. toctree::
   :maxdepth: 2

   example
```

This tells Sphinx to render your README, followed by a subtitle of "Read More", followed by a list of other documents, one of which is your external `example.rst` file.


# Start using autodoc

In that `example.rst`, you could put the following.

```bash
Some Examples
=============

Here are some examples to get you started.

.. automodule:: src.examples
    :members:
```

This will look at your code in `src/examples.py` for classes, functions and methods. Each one will be listed in this section of the docs, along with any docstrings that where present. See the following example. All of these sections in the text are optional.

```python
def public_fn_with_sphinxy_docstring(name, state=None):
    """This function does something.

    write as much as you want here

    here is a code sample:

    >>> from example import public_fn_with_sphinxy_docstring
    >>> public_fn_with_sphinxy_docstring(
    ...     'foobar',
    ...     'pending')
    0

    :param name: The name to use.
    :type name: str.
    :param state: Current state to be in.
    :type state: bool.
    :returns:  int -- the return code.
    :raises: AttributeError, KeyError
    """
    return 0
```

# Automating Doc Generation

If you use Jenkins for your continuous integration system, you can use the [HTML Publisher Plugin](https://wiki.jenkins-ci.org/display/JENKINS/HTML+Publisher+Plugin) to automatically build the documentation every time you merge. It will also host the HTML for you, right in Jenkins.

# References

- [Sphinx Basics](https://pythonhosted.org/an_example_pypi_project/sphinx.html) - Good listing of basic reStructuredText syntax for Spinx
---
title: Per-OS virtual environments in Python
tags: python
---

A common setup for web development is to have a virtual machine on your Mac running all your code. Typically this involves a shared folder containing the code, so that you can edit the file locally on your Mac, but it's also available to the guest OS to execute. At least with Vagrant and either VirutalBox or VMWare, the performance of the shared folder can quickly become a nuisance.

If you're running a long test suite, you may be sacrificing as much as 40% of the time to the virtualization in my admittedly informal testing. Also vexing is hot reloading code changes in your web app, where you suffer from lag between when a file is changed and when the event bubbles up in the guest OS. I've written about some [mitigation strategies](http://chase-seibert.github.io/blog/2014/03/09/vagrant-cachefilesd.html) before. The upshot is that you may be waiting several seconds longer than necessary before you can refresh your app the see the changes. Those seconds add up when you perform this cycle hundreds of times a day. Git hook performance can also be slower than normal.

What's preventing you from running some or all of these operations on your local Mac? Mainly that all your dependencies only exist inside the virtual machine. With Python code bases, this often translates into the fact that your virtualenv directory has architecture (Linux) specific binary packages inside it.

If you happen to use a `Makefile` to generate your virtualenv and run common commands, you can easily handle this there by creating different virtualenv directories for each environment.

```bash
VENV_DIR=venv
ifeq ($(shell uname),Darwin)
    # allows you to have two venv dirs, one for local osx development (faster)
    VENV_DIR=venv-osx
endif

test:
    @. $(VENV_DIR)/bin/activate; nosetests
```

Just remember to add the new virtualenv directories to your `.gitignore` file.
---
title: Optimizing Flask client tests
tags: python flask
---

When writing Python integration tests, it's useful to put slow code such as database access in a 
[setUpClass](https://docs.python.org/2/library/unittest.html#setupclass-and-teardownclass) method, 
so that they are only executed once for the entire `unittest.TestCase`. Recently, when writing
integration tests for a Flask API, I wanted to make an API call once for the `TestCase`, but have
many tests that assert on various parts of the JSON response. This was a little awkward because 
the Flask [test client](http://flask.pocoo.org/docs/0.10/testing/) was only being instantiated with 
an instance of `TestCase`.

I ended up caching API responses in a global variable in my custom base `TestCase` subclass.

```python
from functools import partial
from unittest import TestCase


_cached_api_responses = {}


class MyTestCase(TestCase):

    def set_cached_json_data(self, cache_key, test_callable):
        """ We want to separate out tests for various keys in the json response 
            of an API call, but we only want to make an API once for performance
            reasons. Solution is to cache this between test calls, which is made
            more difficult due to test classes being re-instantiated between
            individual tests. Cache in a global. """
        global _cached_api_responses
        response_json = _cached_api_responses.get(cache_key)
        if not response_json:
            response = test_callable()
            response_json = response.json
            _cached_api_responses[cache_key] = response_json
        return response_json
        
        
class SpecificTestCase(MyTestCase):

    @classmethod
    def setUpClass(cls):
        # do a bunch of database record creation
        cls.db_object = ...
        
    def setUp(self):
        # cache a flask API response 
        test_callable = partial(self.get, '/my-url')
        self.response_json = self.set_cached_json_data('my-url', test_callable)
    
    def test_foo(self):
        foo = self.response_json['foo']
        self.assertEquals(len(foo), 1)
            
    def test_bar(self):
        bar = self.response_json['foo']['bar']
        self.assertEquals(len(bar), 10)
```

The use of partial is just to make it easier to pass any test client call into the caching function.

The fact that it's caching JSON and not a collection of SQLAlchemy database objects is important,
if you tried that, you would find that SQLAlchemy would throw exceptions about the objects no longer
being tied to a session in your tests. 
---
title: Flask-Admin Edit One To Many Fields from the List View
tags: python flask
---

I got to play with [Flask-Admin](https://flask-admin.readthedocs.org/en/latest/) for the first time
this week. Compared with Django admin, it's very extensible, though the default UI is pretty ugly.
In any case, I ran into one particular challenge that took a good deal longer to figure out than
it should have. Specifically, making a one to many relationship editable from the list view.

on the `ModelView`. 
You can easily expose editable fields from the list view using `column_editable_list`. See
[the documentation](https://flask-admin.readthedocs.org/en/latest/api/mod_model/#flask_admin.model.BaseModelView.column_editable_list)
for more.

```python
class MyModelView(BaseModelView):
    column_editable_list = ('question', 'details', 'status')
```    

This ends up looking like this:

![column_editable_list](/blog/images/column_editable_list.png)

This works for most `db.Model` column types I tried. For `String`, `Text` and `Integer`, you get a 
simple `input` HTML element. For basic many to one relationships, you get a `select` HTML element.
But when I tried to make a one to many relationship editable, I got a stack trace:

```bash
Traceback (most recent call last):
  File "/srv/nerdwallet/myproject/venv/lib/python2.7/site-packages/flask/app.py", line 1836, in __call__
    return self.wsgi_app(environ, start_response)
  ...
  File "/srv/nerdwallet/myproject/venv/lib/python2.7/site-packages/wtforms/fields/core.py", line 149, in __call__
    return self.meta.render_field(self, kwargs)
  File "/srv/nerdwallet/myproject/venv/lib/python2.7/site-packages/wtforms/meta.py", line 53, in render_field
    return field.widget(field, **render_kw)
  File "/srv/nerdwallet/myproject/venv/lib/python2.7/site-packages/flask_admin/model/widgets.py", line 93, in __call__
    kwargs = self.get_kwargs(subfield, kwargs)
  File "/srv/nerdwallet/myproject/venv/lib/python2.7/site-packages/flask_admin/model/widgets.py", line 148, in get_kwargs
    raise Exception('Unsupported field type: %s' % (type(subfield),))
Exception: Unsupported field type: <class 'flask_admin.contrib.sqla.fields.QuerySelectMultipleField'>
```

The model column definition in question was a simple relationship with a join table:

```python
class MyModel(AAAModel):
    ...
    # uselist=True means you can have more than one child MyChildModel per MyModel
    children = relationship('MyChildModel', secondary='model_children_join', uselist=True)
```

Looking into the [Flask-Admin code](https://github.com/flask-admin/flask-admin/blob/master/flask_admin/model/widgets.py#L100), 
they actually support a number of other fields types such as `Boolean`, `DateTime`, `Float`, etc. But
no select multiple. Here is the `Flask-Admin` code block that I was looking at:

```python
class XEditableWidget(object):
    """
        WTForms widget that provides in-line editing for the list view.

        Determines how to display the x-editable/ajax form based on the
        field inside of the FieldList (StringField, IntegerField, etc).
    """
    def __call__(self, field, **kwargs):
        kwargs.setdefault('data-value', kwargs.pop('value', ''))

        kwargs.setdefault('data-role', 'x-editable')
        kwargs.setdefault('data-url', './ajax/update/')

        kwargs.setdefault('id', field.id)
        kwargs.setdefault('name', field.name)
        kwargs.setdefault('href', '#')

        if not kwargs.get('pk'):
            raise Exception('pk required')
        kwargs['data-pk'] = str(kwargs.pop("pk"))

        kwargs['data-csrf'] = kwargs.pop("csrf", "")

        # subfield is the first entry (subfield) from FieldList (field)
        subfield = field.entries[0]

        kwargs = self.get_kwargs(subfield, kwargs)

        return HTMLString(
            '<a %s>%s</a>' % (html_params(**kwargs),
                              escape(kwargs['data-value']))
        )

    def get_kwargs(self, subfield, kwargs):
        """
            Return extra kwargs based on the subfield type.
        """
        if subfield.type == 'StringField':
            kwargs['data-type'] = 'text'
        elif subfield.type == 'TextAreaField':
            kwargs['data-type'] = 'textarea'
            kwargs['data-rows'] = '5'
        elif subfield.type == 'BooleanField':
            kwargs['data-type'] = 'select'
            # data-source = dropdown options
            kwargs['data-source'] = {'': 'False', '1': 'True'}
            kwargs['data-role'] = 'x-editable-boolean'
        elif subfield.type == 'Select2Field':
            kwargs['data-type'] = 'select'
            kwargs['data-source'] = dict(subfield.choices)
        elif subfield.type == 'DateField':
            kwargs['data-type'] = 'combodate'
            kwargs['data-format'] = 'YYYY-MM-DD'
            kwargs['data-template'] = 'YYYY-MM-DD'
        elif subfield.type == 'DateTimeField':
            kwargs['data-type'] = 'combodate'
            kwargs['data-format'] = 'YYYY-MM-DD HH:mm:ss'
            kwargs['data-template'] = 'YYYY-MM-DD  HH:mm:ss'
            # x-editable-combodate uses 1 minute increments
            kwargs['data-role'] = 'x-editable-combodate'
        elif subfield.type == 'TimeField':
            kwargs['data-type'] = 'combodate'
            kwargs['data-format'] = 'HH:mm:ss'
            kwargs['data-template'] = 'HH:mm:ss'
            kwargs['data-role'] = 'x-editable-combodate'
        elif subfield.type == 'IntegerField':
            kwargs['data-type'] = 'number'
        elif subfield.type in ['FloatField', 'DecimalField']:
            kwargs['data-type'] = 'number'
            kwargs['data-step'] = 'any'
        elif subfield.type in ['QuerySelectField', 'ModelSelectField']:
            kwargs['data-type'] = 'select'

            choices = {}
            for choice in subfield:
                try:
                    choices[str(choice._value())] = str(choice.label.text)
                except TypeError:
                    choices[str(choice._value())] = ""
            kwargs['data-source'] = choices
        else:
            raise Exception('Unsupported field type: %s' % (type(subfield),))

        # for Select2, QuerySelectField, and ModelSelectField
        if getattr(subfield, 'allow_blank', False):
            kwargs['data-source']['__None'] = ""

        return kwargs
```

Basically, it's generating some HTML from a `kwargs` dict based on the field type. These are 
[WTForms](https://wtforms.readthedocs.org/en/latest/) types from the 
[SQLAlchemy WTForm extension](https://wtforms.readthedocs.org/en/latest/ext.html?highlight=queryselectfield#module-wtforms.ext.sqlalchemy). 
But the `data-type` types are actually from a Javascript library called [x-editable](https://vitalets.github.io/x-editable/docs.html), which is
doing the actual UI and Ajax call for the update. 

It turns out that `x-editable` doesn't support a `select multiple` element, but they do have a [checklist](https://vitalets.github.io/x-editable/docs.html#checklist)
type, which is just as good (at least for a small number of choices). Hacking this into the 
`ModelView` was pretty simple.

```python
from flask.ext.admin.model.widgets import XEditableWidget


class CustomWidget(XEditableWidget):

    def get_kwargs(self, subfield, kwargs):
        if subfield.type == 'QuerySelectMultipleField':
            kwargs['data-type'] = 'checklist'
            kwargs['data-placement'] = 'left'
            # copied from flask_admin/model/widgets.py
            choices = {}
            for choice in subfield:
                try:
                    choices[str(choice._value())] = str(choice.label.text)
                except TypeError:
                    choices[str(choice._value())] = ""
            kwargs['data-source'] = choices
        else:
            super(CustomWidget, self).get_kwargs(subfield, kwargs)
        return kwargs


class CustomFieldList(ListEditableFieldList):
    widget = CustomWidget()
    
class MyModelView(BaseModelView):
    column_editable_list = ('question', 'details', 'slug', 'status', 'children')

    def get_list_form(self):
        return self.scaffold_list_form(CustomFieldList)
```

---
title: Getting Engineering Involved Early in a Project
tags: process newboss product-management
---

> Agile requirements are ideally visual and should be barely sufficient, i.e. the absolute
minimum required to enable development and testing to proceed with reasonable efficiency.
                    - [Agile Principle #4](http://www.allaboutagile.com/agile-principle-4-agile-requirements-are-barely-sufficient/#sthash.wIvPTKXl.dpuf)

Some engineering teams don't want to think about a feature or a product until a specification has
been written. Their ideal world would be getting handed 100 pages of specification along with high
fidelity mockups and then starting to implement. I'm not sure where this instinct comes from; it's
never been my experience of how engineering can be successful, at least in start ups.

The first thing you have to realize as an engineer is that the spec is always wrong. If you can't
write code without bugs, you can't expect a specification without bugs. In fact, because engineers
as pretty great at finding bugs, they can add a lot of value debugging specs. Just like coding,
the earlier you find the bugs, the more work you're saving yourself over time.

If you're a PM, and your engineers are saying that they don't want to be involved until specs are
written, what they *may* really be saying is that they don't want to commit to building it yet.
Make sure you're arguing about the same thing.


# My Ideal Process

For me, the ideal process is having one technical lead, one designer and one product manager sit
down over lunch before anything is set in stone. Maybe there is an email chain or a few bullets on
a powerpoint deck at this point, but certainly nothing approaching a specification that could not
also fit on a napkin.

There is a standing joke about agile teams really performing a series a medium sized waterfalls.
This is a subset that I have seen in the wild - product managers doing what is essentially a
waterfall specification and mockup exercise and only involving engineering after the fact to
estimate.

If you've spent any time in a kitchen, you may already know how critical it is to season food
*before* you cook it. Salting a piece of meat before it goes on the heat (ideally many hours before)
produces a totally different result from cooking the meat and then adding salt. This is why the
short-lived proposal on regulating just that in NYC was so laughable; it would have produced vastly
inferior products.


# How Involving Engineers Early Adds Value

Engineers are experts at complexity. They can identify complexity where no one else has, and then
quickly compartmentalize it and think about the big picture. That's essentially how they break
down problems, whether it's on code or when evaluating a spec. They are also great at finding edge
cases, i.e. holes in the spec with undefined behavior.

The goal should be to have both the design and the implementation match the user's mental model.
What does that mean? The user likely has some expectation of what's happening when they use a piece
of software. That expectation should be echoed in the design, so that their options at any given
point match their expectations. That's when software becomes easy to use. But, long term, it's also
critical that the actual implementation match the same model. Too often, a mismatch here could have
been easily avoided by getting feedback early in the process from an engineer who can already see
that there will be weird implementation implications.

Ultimately, that translates into product quality.


# Startup Culture

Most startup engineering veterans can navigate this by instinct. Early on, the company may only
*have* engineers. There is no choice; they are involved in creating specs because there is no one
else to do it. The crimes of naive engineers on innocent designs and products are the topic of
another post, but at least they are forced to be involved at the right phase - the beginning.

Engineers want to be engaged. Go light on specs, but go heavy on iteration.
---
title: Death to Meetings (and other alternatives)
tags: process reading-list meetings manager newboss
toc: true
---

> What percentage of your time at work do spend in meetings? If you’re a middle manager, it's likely about 35% of your time, and if you’re in upper management, it can be a whopping 50%. What's worse is how unproductive these meetings usually are. - [themuse.com](https://www.themuse.com/advice/how-much-time-do-we-spend-in-meetings-hint-its-scary)

Meetings are a fact of life in corporate America, whether you're working at a large company, or a startup. Is that 35% number correct? Seems pretty close to my personal results. I've been using [Toggl]() for a while to track time, and over the last 30 days, I have spent more like 40% of my time in meetings.

![toggl_meetings](/blog/images/toggl_meetings.png)

*The red block is meetings, green is coding, and blue is writing.*

What's worse, it's actually not that much lower for individual contributors. Do business owners really want their employees spending this much time in meetings? Is that the most productive arrangement? What's wrong with meetings, anyway?

## Bad Meetings

> Meetings are indispensable when you don't want to do anything. - [John Kenneth Galbraith](http://www.brainyquote.com/quotes/keywords/meetings.html#z5clr8zclSbxAafk.99)

Some meetings are very useful. The problem is that every meeting is very expensive. Say you get 10 people in a room for an hour. If they each make $150k/year (hey, this is the Bay Area), you just spent $750 (plus taxes and benefits). What if there are 100 such meetings a day across the company? What did you get for that money?

Have an all hands meeting for 250 people? That comes to $18,750. For that amount of money, you could book the [Indigo Girls](http://priceonomics.com/how-much-does-it-cost-to-book-your-favorite-band/) to present your powerpoint slides for you. You could give each employee a little over [2 gallons](https://www.thrillist.com/drink/nation/best-coconut-water-brand-amy-brian-coconut-juice-from-thailand-ranks-top) of coconut water. **You could hire another six engineers.**

The worst case scenario is that meetings are being called primarily for the benefit of the meeting organizer. That person obviously feels that it's a good use of time. What about everyone else? If you're not careful, you may have attendees who only need to engage for 5-10 minutes in a one hour slot, and are basically sitting idle the rest of the time.

For a software engineer, that looks a lot like synchronous blocking on I/O. You're killing throughput. Wouldn't a system of asynchronous callbacks be a lot more efficient?

## Alternative to Meetings

> If you had to identify, in one word, the reason why the human race has not achieved, and never will achieve, its full potential, that word would be 'meetings.' - [Dave Barry](http://www.brainyquote.com/quotes/keywords/meetings.html#z5clr8zclSbxAafk.99)

We have tools at our disposal to solve this problem - but we need a shift in approach.

### Slack

[Slack](https://slack.com/) is awesome. You should get it. If you're on anything else, you're missing out. How can you use it to replace some meetings? Just create a channel or private group instead of a meeting. Invite the attendees. Put the agenda in the chat. Link to some shared docs that people need to reference. Start a discussion.

One interesting thing that happens is that the conversation becomes asynchronous. It no longer strictly matters *when* people chime in. You can hold a conversation over the course of a day, or a week. This is a *big win* for everyone involved; let them participate when it makes the most sense for them.

Will people sometimes be slow to engage? Yep. That's when it's on the "meeting" organizer to ping people with an @mention. Meeting owners *need* to be driving the conversation - whether it's over Slack or in a traditional sit down.

Bonus points: your meeting is taking its own notes! Next time you have the meeting; the last meeting's notes are right there.

*PS: If you work at Slack and you're reading this - you guys should totally implement a "kill this meeting" plugin for Google calendar that deletes the meeting and sets up a temporary Slack channel with all the attendees.*

### Email

Email is another good option. Less collaborative than chat, but maybe better for longer prose. Of course, email is only slightly less maligned than meetings themselves. But hey, at least it's asynchronous.

### GoToMeeting

GoToMeeting is another good option, especially for screen sharing. Slack really needs to get on this, but for now it's probably your best option for some types of meetings.

### Recurring "Meetings"

One sticky wicket is the recurring meeting. If you have the same meeting weekly, do you still need a calendar item to remind people to join the chat, or start the email chain?

I guess you could, but that seems like the wrong tool for the job. Most recurring meetings are run by managers. Managers have a ton of things to keep track of. I guarantee they have a personal reminder system. They should use that to remind themselves to kick off the meeting on the right day.

Better yet, maybe you don't actually *need* the meeting this week. This is a subtle benefit - recurring meetings default to *not happening*, versus defaulting to happening unless someone cancels them. Inertia is a funny thing. Managers *should* be consciously thinking about whether a meeting is really required, every time.

## Good Meetings

> Meetings are at the heart of an effective organization, and each meeting is an opportunity to clarify issues, set new directions, sharpen focus, create alignment, and move objectives forward. - [Paul Axtell](http://www.amazon.com/Meetings-Matter-Strategies-Remarkable-Conversations/dp/0943097142)

Some meetings are not really meetings. One on ones are not meetings. Those should absolutely be on the calendar and happen every week.

What are some examples of good meetings?

- Demo + celebration of recently shipped projects
- Brainstorming ideas for a new product
- Strategy sessions

*Note: if you're going to have a brainstorming meeting, make sure people know how to brainstorm. Talking about potential implementation and challanges is not brainstorming.*

## Meeting Hygiene

No matter what the meeting is for, practice basic meeting hygiene.

- Have a clear meeting owner
- Have a clear agenda written down ahead of time
- Start the meeting on time
- The owner drives the discussion - stay on track
- End the meeting with a +/-/delta poll
- End the meeting on time, if not early
- Send out meeting notes via email to the whole team

## Culture

> Management's job is to convey leadership's message in a compelling and inspiring way. Not just in meetings, but also by example. - [Jeffrey Gitomer](http://www.brainyquote.com/quotes/keywords/meetings.html#z5clr8zclSbxAafk.99)

Again, most meetings are called by managers. It's up to those managers to set a culture of respect for people's time. Some parting ideas:

- Give everyone explicit permission to skip meetings at their discretion. No judgement when they do!
- Meeting notes are a great way to make sure people are not attending meetings just so they can know what's going on.
- What if a meeting owner had to put $1 in a jar every time they schedule a meeting? What if it was $1/person?
- Are you [tracking metrics](https://github.com/chase-seibert/gcal-report) on hours spent in meetings? Probably not. Why not?
---
title: The Role of Specifications in Agile
tags: process
---

It's commonly said that everyone does Agile differently. In my experience, it's also common to do 
basically whatever you want and call it Agile. It can be useful to occasionally reset and examine 
what canonical Agile recommends. For software specifications, it's pretty simple. Do just enough, 
no more.


# Why do we need specifications at all?

> A software requirements specification (SRS) is a description of a software system to be 
developed, laying out functional and non-functional requirements, and may include a set of use 
cases that describe interactions the users will have with the software.
    - [Wikipedia](https://en.wikipedia.org/wiki/Software_requirements_specification)


Historically, specifications have been used to communicate to the customer (be it an internal or 
external customer) what will be built. Agile is built on the principle that this is actually not in
the best interest of the customer. Why? 

- Low effort specifications will have all parties making critical assumptions.
- High effort specifications take a lot of time - and will necessarily be wrong in big ways.
- Scope can and should change as we learn more about the problem.

In Agile, we build in short sprints. Documentation need only be created for the next sprint, or 
perhaps one additional sprint. This is known as 
    - ["Just in time Documentation"](http://www.agileforall.com/2009/02/new-to-agile-remember-one-thing-just-enough-just-in-time/)
    
    
# Self-directed Agile Teams

> Agile artefacts such as technical spikes and development iterations mean that high-level 
requirements can be considered sufficient at project initiation. 
    - [Ryan Hewitt](http://www.batimes.com/articles/applying-agile-principles-to-requirement-analysis.html)

For agile teams, specifications exist so that the team knows what they need to build.
Because we are committing only to one sprint at a time, we have no need to project long-term dates
that would necessitate a full specification. What documentation does the team need to get started? 
A highly detailed specification for just that first sprint.

![agile](http://www.userstories.com/system/product_image/file/980/Feedback_Flow-original.png?1392328012)

An agile team is composed of a product manager, a designer and one or more developers. This team 
must be empowered to design and implement their vision of a solution. A specification is definitely
NOT for feedback or sign-off prior to building. 
    
    
# How does the team know to build the right thing?

> Agile requirements, on the other hand, depend on a shared understanding of the customer that is 
shared between the product owner, designer, and the development team. That shared understanding and 
empathy for the target customer unlocks hidden bandwidth for product owners. They can focus on 
higher-level requirements and leave implementation details to the development team, who is fully 
equipped to do so – again, because of that shared understanding.
    - [Dan Radigan, Senior Agile Evangelist, Atlassian](https://www.atlassian.com/agile/requirements/)

User stories are the form that specifications take. Each user story is created in advance and 
placed in a backlog, but only the small set of the very next stories are flesh out in detail. Then,
the level of detail is very high. Designs are included at this stage, and so are detailed 
descriptions of fine grained behavior like validation, individual errors messages, etc.
 
Though the PM owns the user story, the team itself generates the detail through a processes called
grooming. User empathy is critical - while PM and design naturally represent the customer in the 
design process, the entire team needs to understand the customer motivation and pain points. 

In the end, the team may very well not build the right thing. This is where feedback comes in - at
the end of a sprint, once working software is produced and shown to the customer.
    
    
# When to get feedback

> Working software over comprehensive documentation
    - [The Agile Manifesto](http://agilemanifesto.org/)

In Agile, feedback is given based on working software, not specifications. The team commits to 
delivering and demoing working software every sprint. These demos are where feedback is generated. 
I've often been surprised when [customers don't really know what they want until they see it.](http://haacked.com/archive/2005/08/17/misunderstanding-agile-design.aspx/)
---
title: Case Study in Documentation Usability - Django vs. Flask/SQLAlchemy
tags: documentation
---

When I'm evaluating new open source projects, there are a few things I look for. I look for how many stars they have on GitHub. I look at their issues in GitHub. If you are not on GitHub, I'm basically not interested. Finally, I look at their documentation. But how do you know if the documentation is any good? What makes good documentation, anyway?


## Documentation Usability

Documentation can be exhaustive and still not ultimately useful. Documentation usability is the idea that documentation ultimately needs to help users solve their problems. Given a task to be done, how quickly and correctly can the documentation enable the user to complete their task?

The first thing you need to realize is that search is the interface for your documentation. You might get users to read a high level overview of the project when they are first starting out, but with rare exception no one is reading the entire documentation front to back for anything remotely complicated. Instead, they will search when they have a question. Usually using Google, not some crappy search functionality inside your documentation.

Your first challenge then is finding out what terminology your users are using. Importantly, you cannot assume that they will use your invented vocabulary. Your project is part of the entire ecosystem of tools they already use. Your best bet is to use [existing terminology](http://idratherbewriting.com/2009/04/28/documentation-usability-a-few-things-ive-learned-from-watching-users/).

Also, users should know at all times [where they are](http://techwhirl.com/tips-and-tricks-10-heuristics-documentation-usability/) in the documentation relative to the whole, and relative to where they were previously. You want them to be able to quickly find related topics.


## Case Study - Django versus Flask/SQLAlchemy/Alembic

[Django](https://www.djangoproject.com/) is a framework renowned for their excellent documentation. It's a swiss army knife project that includes lots of functionality. A common alternative is a collection of projects, headlined by [Flask](http://flask.pocoo.org/), [SQLAlchemy](http://www.sqlalchemy.org/) and [Alembic](https://alembic.readthedocs.org/en/latest/). The documentation for the second set of projects is better than most. It's pretty exhaustive. But it's not very usable.

From the Django [Guide to Documentation](https://docs.djangoproject.com/en/1.8/internals/contributing/writing-documentation/):

> We place a high importance on consistency and readability of documentation. After all, Django was created in a journalism environment! So we treat our documentation like we treat our code: we aim to improve it as often as possible.

This dedication to continuous improvement of the documentation is evident in some common use cases.


### Task: SQL IN clause queries

One common task in both ecosystems is accessing a database. Further, there is a common (if not core) use case where you want to query a database by a list of record IDs. If you Google for `Django query in list`, the first hit is the [Django documentation](https://docs.djangoproject.com/en/1.8/ref/models/querysets/#in), and there is a sidebar with the "in" clause called out:

![django in](/blog/images/django_in.png)

If you search for `SQLAlchemy query in list`, you get five Stackoverflow answers before you see their official documentation. The first hit does not contain a correct answer. The [actual documentation hit](https://pythonhosted.org/Flask-SQLAlchemy/api.html) takes you a nebulously titled page, "API". The table of contents does not contain anything at the granularity of individual topics on that page. Searching the page for "in" is of course fruitless. There is [specific documentation for this feature](http://docs.sqlalchemy.org/en/rel_0_7/core/expression_api.html#sqlalchemy.sql.operators.ColumnOperators.in_), but it does not come back in the first few pages of Google results. Even if you find it, it does not include a specific code example.


### Task: Modify records when they are saved

If you Google for `Django on save`, the first hit is Stackoverflow. The second hit a [specific documentation](https://docs.djangoproject.com/en/1.8/ref/models/instances/#what-happens-when-you-save) for what happens when a record is saved. That links directly to how to over-ride a method called `post_save`.

If you Google for `SQLAlchemy on save`, you see three identical results for three difference versions of SQLAlchemy. In each case, you get a page for ORM events, which you basically need to read the entirety of to understand. There are nebulous sidebar links for "Mapper Events", "Session Events" and "Query Events", but those are not terms that make immediate sense to me, without reading a bunch of other documentation.


## Task: Redirecting HTTP to HTTPS

If you Google `Django redirect HTTPS`, the third result is a [bug report](https://code.djangoproject.com/ticket/12043) where someone went back and added a link to the canonical feature for this, `SECURE_PROXY_SSL_HEADER`. Great example of paying attention to what users are searching for.

If you Google for `Flask redirect HTTPS`, you also get a bug report as the first hit. There is some discussion, but not solution, at least for the case where something like Flask-admin is doing the redirect. But there actually is a [documented solution](http://werkzeug.pocoo.org/docs/0.10/routing/), though it does not show up in the Google results.


# Take aways

- Exhaustive documentation is not necessarily useful documentation
- Take time to write in your user's vocabulary
- Find out what they are actually searching for
- Update old documentation to point to new solutions
---
title: Comparison of Chat Message Platforms
---

If you're writing a chat app, the first big question to consider is whether you are going to write the actual chat backend. Why would you want to? You would get complete control over the functionality, reliability and interface. Why would you *not* want to? Well, persistent socket systems that scale to thousands or hundreds of thousands of users are no small undertaking. Plus, a bunch of good platforms already exist. It probably comes down do whether chat is part if your core business. If you're Snapchat, you probably want to write your own chat platform. If chat is just a feature you use to deliver your actual value, it probably does not make sense to spend the engineering time on it.

What platforms exist that could be used to write a chat app, and how do they stack up? Here is a rundown as of late 2015. My criteria is specific to what I'm trying to build:

- Excellent documentation is a must.
- iOS, Node and Python SDKs preferred.
- iOS notifications.
- Web hooks (so we can use stateless web servers).
- Chats with more than two participants.
- Need to be able to extend the messages with custom metadata.
- End-to-end message encryption would be ideal.
- Proven ability to handle a large number of sessions.

I evaluated eight platforms, and seriously considered three options before making a final decision.


# Also Rans

These platforms *could* be used to create a chat app, but all had various glaring issues.

- [Telegram](https://telegram.org/) - No SDKs, just a REST APIs and a bunch of iOS code you can refactor to meet your needs. Hey guys, I don't want to write HTTP request code manually for three platforms!
- [Intercom.io](https://www.intercom.io/) - Their primary focus is providing a call center chat app themselves, not enabling you to build your own app. As far as I could tell, chats were always one to one.
- [Pusher](https://pusher.com/) - They do much more than chat, not focused on that. No iOS notification support.
- [Firebase](https://www.firebase.com/) - They do *much* more than chat, you can build your whole app and deploy it inside their platform. No native chat semantics, you would need to implement everything yourself.
- [Iron.io](http://www.iron.io/) - It's a message queue. No iOS SDK. API is REST, not socket based.


# #3: Quickblox

[Quickblox](https://quickblox.com/) focuse on chat, and on the developer experience (they don't have a chat app themselves). They have been around for a while (2013), and they're based on XMPP, which is an open standard. They have an excellent reference iOS application.

No Python SDK, and no web hooks. But the killer was the difficulty of our prototype work. Calling their REST API from Python was super painful. The auth request has a timestamp, nonce and HMAC signature, which is super brittle. If the order of the parameters is not alphabetical, or there is any problem with how you setup the HMAC, you just get a generic signature error with no trouble-shooting information. Then you have a series of required custom HTTP headers.

All of these problems are solvable by writing your own Python SDK. But the iOS and Node.js SDKs were also hard to get working. Their Node library, admittedly in beta, was broken. Overall, their documentation was poor - it was hard to get stuff working. Take a look at how verbose the following code is:


```python
import random
import time
import hashlib
import hmac

import requests


APP_ID = 'XXX'
AUTH_KEY = 'XXX'
AUTH_SECRET = 'XXX'
USER_ID = 'XXX'
PASSWORD = 'XXX'
DIALOG_ID = 'XXX'


def create_session():
    nonce = str(random.randint(1, 10000))
    timestamp = str(int(time.time()))
    signature_raw_body = (
        'application_id=' + APP_ID +
        '&auth_key=' + AUTH_KEY +
        '&nonce=' + nonce +
        '&timestamp=' + timestamp +
        '&user[login]=admin' +
        '&user[password]=password')
    signature = hmac.new(AUTH_SECRET, signature_raw_body, hashlib.sha1).hexdigest()
    response = requests.post(
        'https://api.quickblox.com/session.json',
        headers={
            'Content-Type': 'application/json',
            'QuickBlox-REST-API-Version': '0.1.0',
        },
        json={
            'application_id': APP_ID,
            'auth_key': AUTH_KEY,
            'timestamp': timestamp,
            'nonce': nonce,
            'signature': signature,
            'user': {
                'login': 'admin',
                'password': 'password',
                }
        })
    json_data = response.json()
    return json_data['session']['token']


def get_messages(qb_token):
    return requests.get(
        'https://api.quickblox.com/chat/Message.json?chat_dialog_id=' + DIALOG_ID,
        headers={
            'QB-Token': qb_token,
        }
    )


if __name__ == '__main__':
    qb_token = create_session()
    messages = get_messages(qb_token)
    print messages, messages.content
```


# #2: PubNub

[PubNub](https://www.pubnub.com/) is not focused exclusively on chat, but chat is a first class topic in their documentation. Truly excellent SDKs for virtually every language you could care about. Sometimes there are multiple SDKs for the same language that are specialized for particular frameworks - think Django, Twisted and Tornado. They have a lot of mindshare in the developer community; I constantly hear about other engineers using PubNub for hackathon projects. They pop up organically in my Twitter feed a lot.

Prototyping could not have been simpler. See the bellow code. The only downside is that they do not have true web hook support. Their web hooks are limited to when conversations start and stop, versus firing on every chat message.

Why are web hooks a big deal? They probably are not for many use cases. But one thing we want to be able to do is have a Python bot that listens to all conversations and performs various actions in real-time. As far as I can tell, the PubNub model for this is to use something like Twisted, which is an asynchronous event-based networking engine in Python. You leave a socket open to PubNub all the time and get notified of new messages over that active connection.

That performs really well, which is why they do it. But it drastically increases the complexity with regards to production high availability, versus a vanilla web server. With a web server, if you loose a node (and you will, think EC2 instance going down), it's no problem. You simply have multiple nodes behind a load balancer. Because they are all stateless, recovery is automatic as new requests keep flowing to the remaining nodes.

With an active socket daemon, you have to solve the high availability problem yourself. If you create a master/hot backup architecture, you need to realize that the primary node is down and have the secondary server establish a connection and *resume any in progress conversations*. That's a separate critical code path that will execute only very rarely - a recipe for bugs. If you go active/active, then you have all the same problems of resuming a portion of the connections when a node fails, plus you have to figure out connection sharding.

None of these are insurmountable problems. But they are significant added complexity, which is what I'm trying to avoid by using an existing platform. PubNub has a super simple SDK, but then they force you to solve a bunch of architectural complexity yourself.

Here is our prototype code. Notice the paradigm; event loops. This code blocks until you kill the process.


```python
from pubnub import Pubnub


def callback(message, channel):
    print(message)


def error(message):
    print("ERROR : " + str(message))


def connect(message):
    print("CONNECTED")
    print pubnub.publish(channel='test', message='Hello from the PubNub Python SDK')


def reconnect(message):
    print("RECONNECTED")


def disconnect(message):
    print("DISCONNECTED")


pubnub = Pubnub(publish_key='XXX', subscribe_key='XXX', secret_key='XXX')

pubnub.subscribe(
    channels='dit',
    callback=callback,
    error=callback,
    connect=connect,
    reconnect=reconnect,
    disconnect=disconnect,
)
```


# #1: Layer

[Layer](https://layer.com/) is a new player. They focus solely on chat. They have good iOS and Node SDKs (but not Python). Their REST API semantics are dedicated to chat. Conversations and messages are first class concepts. Their documentation is good, if not as exhaustive as PubNub. Importantly, they have web hooks for each message (currently in private beta).

One downside is that they do not support end-to-end encryption per-se. Of course, you can manually encrypt your message bodies yourself. Being newer, they also do not have the track record that PubNub has.

Even though they do not have a Python SDK, our prototype code could not have been more Pythonic. This makes me confident we will be able to write clean code, and use a simple web server architecture.

Finally, there is significant upside to being a chat specific platform. The SDKs support concepts like read/unread, typing indicators, etc. You can do that yourself via PubNub, but you have to write more code. PubNub is schemaless - you are writing all the schema plus serialization yourself, separately for each language. Code is the enemy!


```python
import requests


APP_ID = 'XXX'
PLATFORM_API_TOKEN = 'XXX'
HOST = 'https://api.layer.com'
BASE_URL = HOST + '/apps/%s' % APP_ID
HEADERS = {
    'Accept': 'application/vnd.layer+json; version=1.0',
    'Authorization': 'Bearer %s' % PLATFORM_API_TOKEN,
    'Content-Type': 'application/json',
}


def _request(method, relative_url, data=None):
    data = data or {}
    callable_ = getattr(requests, method)
    return callable_(
        BASE_URL + relative_url,
        headers=HEADERS,
        json=data,
    )


def get_conversation():
    return _request('post', '/conversations', data={
        'participants': ['John', 'Jane'],
        'distinct': True,
    })


def get_messages(conversation_id):
    return _request('get', '/conversations/%s/messages' % conversation_id)


def post_message(conversation_id, text):
    return _request('post', '/conversations/%s/messages' % conversation_id, data={
        'sender': {
            'name': 'Chase',
        },
        'parts': [
            {
                'body': text,
                'mime_type': 'text/plain',
            }
        ]
    })


conversation = get_conversation()
conversation_id = conversation.json().get('id')
conversation_id = conversation_id.replace('layer:///conversations/', '')
print conversation_id

messages = get_messages(conversation_id)
print messages, messages.content

post_response = post_message(conversation_id, 'this is a test')
print post_response, post_response.content
```

# Conclusion

I hope this helps you if you need to make a similar decision. Hit me up on Twitter and let me know how it goes!
---
title: Interoperable AES256 encryption between CryptoJS, PyCrypto and CryptoSwift
---

Even though AES256 is a standard, there are enough choices left to implementing libraries to make
cross platform encrypting and decrypting tricky. In particular, getting Javascript, Python and
Swift code that could all encrypt to the same ciphertext using the same plaintext and keys, and
then successfully decrypt back to the plaintext proved to be a multiple day adventure for three
engineers.

Thanks to [marcoslin](https://gist.github.com/marcoslin/8026990) for getting us started!

Also, thanks to my co-workers Chris Boyle and Yair Loeza, who assure me that they do not have
Twitter accounts worth linking to ;)

# CryptoJS

This library makes some implementation decisions that required diving into the source code to
even find out about.

- In the canonical usage `Crypto.AES.encrypt(plaintext, key, options)`, the second parameter is not actually
the AES key. It's the "passphrase", which is used to randomly generate `key`, `iv` AND `salt` values.
- However, if you pass a byte array instead of a string, it WILL use that value as the `key` directly.
- We also found it expedient to set an `iv` value explicitly via the third `options` parameter.
- The default AES mode and padding scheme are also defaulted differently than other libraries, but
can easily be over-ridden in the constructor. We chose zero byte padding knowing that it's simple and
that we would need to implement pad/unpad ourselves on the other platforms.
- Finally, we chose a serialization method of hex over the wire, to reduce the change that a character
encoding issue make our ciphertext invalid in transit.


```javascript
var Crypto = require('cryptojs');
Crypto = Crypto.Crypto;

var KEY = 'This is a key123';
var IV = 'This is an IV456';
var MODE = new Crypto.mode.CFB(Crypto.pad.ZeroPadding);

var plaintext = 'The answer is no';
var input_bytes = Crypto.charenc.UTF8.stringToBytes(plaintext);
var key = Crypto.charenc.UTF8.stringToBytes(KEY);
var options = {iv: Crypto.charenc.UTF8.stringToBytes(IV), asBytes: true, mode: MODE};
var encrypted = Crypto.AES.encrypt(input_bytes, key, options);
var encrypted_hex = Crypto.util.bytesToHex(encrypted);
console.log(encrypted_hex); // this is the value you send over the wire

output_bytes = Crypto.util.hexToBytes(encrypted_hex);
output_plaintext_bytes = Crypto.AES.decrypt(output_bytes, key, options);
output_plaintext = Crypto.charenc.UTF8.bytesToString(output_plaintext_bytes);
console.log(output_plaintext); // result: 'The answer is no'
```


# PyCrypto

We actually started with this implementation, but ended up having to tweak it more to be compatible
with what we were doing in CryptoJS. Nothing weird here, but we did need to set the mode correctly,
and we needed to implement the padding ourselves. PyCrypto does not require that plaintext be a
multiple of BLOCK_SIZE the way PyCrypto does, but we needed to ensure that it could encrypt and
decrypt to the same outputs as PyCrypto.


```python
import binascii
from Crypto.Cipher import AES


KEY = 'This is a key123'
IV = 'This is an IV456'
MODE = AES.MODE_CFB
BLOCK_SIZE = 16
SEGMENT_SIZE = 128


def encrypt(key, iv, plaintext):
    aes = AES.new(key, MODE, iv, segment_size=SEGMENT_SIZE)
    plaintext = _pad_string(plaintext)
    encrypted_text = aes.encrypt(plaintext)
    return binascii.b2a_hex(encrypted_text).rstrip()


def decrypt(key, iv, encrypted_text):
    aes = AES.new(key, MODE, iv, segment_size=SEGMENT_SIZE)
    encrypted_text_bytes = binascii.a2b_hex(encrypted_text)
    decrypted_text = aes.decrypt(encrypted_text_bytes)
    decrypted_text = _unpad_string(decrypted_text)
    return decrypted_text


def _pad_string(value):
    length = len(value)
    pad_size = BLOCK_SIZE - (length % BLOCK_SIZE)
    return value.ljust(length + pad_size, '\x00')


def _unpad_string(value):
    while value[-1] == '\x00':
        value = value[:-1]
    return value


if __name__ == '__main__':
    input_plaintext = 'The answer is no'
    encrypted_text = encrypt(KEY, IV, input_plaintext)
    decrypted_text = decrypt(KEY, IV, encrypted_text)
    assert decrypted_text == input_plaintext
```


# CryptoSwift

Only weird thing here is that we needed to implement our own conversion of a hex to NSData.


```swift
class AESHelper {


    var key: String
    var iv: String
    let BLOCK_SIZE = 16

    init (key: String, iv: String) {
        self.key = key
        self.iv = iv
    }

    func encrypt (stringToEncrypt: String) -> String {
        let messageData = stringToEncrypt.dataUsingEncoding(NSUTF8StringEncoding)
        let byteArray = pad(messageData!.arrayOfBytes())
        let encryptedBytes = try! AES(key: self.key, iv: self.iv, blockMode: .CFB).encrypt(byteArray, padding: .None)
        return encryptedBytes.toHexString()
    }

    func decrypt (var message: String) -> String {
        let messageData = message.dataFromHexadecimalString()
        let byteArray = messageData?.arrayOfBytes()
        let decryptedBytes: [UInt8] = try! AES(key: self.key, iv: self.iv, blockMode: .CFB).decrypt(byteArray!, padding: .None)
        let unpaddedBytes = unpad(decryptedBytes)
        var unencryptedString = NSString(bytes: unpaddedBytes, length: unpaddedBytes.count, encoding: NSUTF8StringEncoding)
        return String(unencryptedString)
    }

    private func pad(var value: [UInt8]) -> [UInt8] {
        let length: Int = value.count
        let padSize = BLOCK_SIZE - (length % BLOCK_SIZE)
        let padArray = [UInt8](count: padSize, repeatedValue: 0)
        value.appendContentsOf(padArray)
        return value
    }

    private func unpad(var value: [UInt8]) -> [UInt8] {
        for var index = value.count - 1; index >= 0; --index {
            if value[index] == 0 {
                value.removeAtIndex(index)
            } else  {
                break
            }
        }
        return value
    }

}

extension String {

    /// http://stackoverflow.com/questions/26501276/converting-hex-string-to-nsdata-in-swift
    ///
    /// Create NSData from hexadecimal string representation
    ///
    /// This takes a hexadecimal representation and creates a NSData object. Note, if the string has any spaces, those are removed. Also if the string started with a '<' or ended with a '>', those are removed, too. This does no validation of the string to ensure it's a valid hexadecimal string
    ///
    /// The use of `strtoul` inspired by Martin R at http://stackoverflow.com/a/26284562/1271826
    ///
    /// - returns: NSData represented by this hexadecimal string. Returns nil if string contains characters outside the 0-9 and a-f range.

    func dataFromHexadecimalString() -> NSData? {
        let trimmedString = self.stringByTrimmingCharactersInSet(NSCharacterSet(charactersInString: "<> ")).stringByReplacingOccurrencesOfString(" ", withString: "")

        // make sure the cleaned up string consists solely of hex digits, and that we have even number of them

        let regex = try! NSRegularExpression(pattern: "^[0-9a-f]*$", options: .CaseInsensitive)

        let found = regex.firstMatchInString(trimmedString, options: [], range: NSMakeRange(0, trimmedString.characters.count))
        if found == nil || found?.range.location == NSNotFound || trimmedString.characters.count % 2 != 0 {
            return nil
        }

        // everything ok, so now let's build NSData

        let data = NSMutableData(capacity: trimmedString.characters.count / 2)

        for var index = trimmedString.startIndex; index < trimmedString.endIndex; index = index.successor().successor() {
            let byteString = trimmedString.substringWithRange(Range<String.Index>(start: index, end: index.successor().successor()))
            let num = UInt8(byteString.withCString { strtoul($0, nil, 16) })
            data?.appendBytes([num] as [UInt8], length: 1)
        }

        return data
    }
```


Happy encrypting!
---
title: QA 101 - How to Write a Bug Report
tags: reading-list qa product-management
---

The purpose of a bug report is to enable an engineer to fix the bug.
In companies with no dedicated testing/QA resources, bug reports get generated by many different
people. Product managers may encounter a bug when validating a new release. Customer support may
be talking to a user who has encountered a bug. Bugs may get discovered by other engineers.

Career QA engineers develop the art of intuiting what is causing a bug, and hone the craft of writing
the most actionable possible bug reports. The rest of us have to muddle though.


# Your Friendly Neighborhood Engineer

Fixing a bug is typically only a small portion of bug squashing time. If you spent 100 units of time
on a bug, 90 units will be spent trying to make the bug happen in your development environment, 9
units will be spent finding where the problem is in the code, and 1 unit will be spent fixing it.
This is of course a generalization; in a small number of cases once you find the code that's
responsible for the bug, you realize that it will be a huge amount of work to fix.

Engineers are inundated by bugs. Any software project of any size will eventually end up with
hundreds, maybe thousands of known bugs. The ones that get fixed tend to be reproducible and
prioritized.


# Reproducible

Being able to make a bug happen on demand, and writing down the steps to make it happen, is called
having a reproducible. It's the difference between saying "a user said something happened to them",
or "something happened to me" and "I can make this happen" or "I can show you". Understand that for
the engineer, the most important thing is being able to make it happen, ideally on their own
environment.

The quality of a bug report from worst to best goes:

1. It's broken
2. This particular thing is broken
3. This particular thing is broken, but only when I do this.
3. This particular thing is broken, but only when I do this. I expect this to happen instead.
5. This particular thing is broken, but only when I do this. I expect this to happen instead. It also happens on staging.

The zen of bug reporting is whittling the reproducible case down to the simplest set of steps that
still trigger the issue, while not omitting anything critical. Maybe this bug does not happen for
all users. Maybe it's just new users. Maybe it's just new users with a certain status. If you get
that specific, it's very likely that the engineer will know what the problem is off the top of their
head. It's also possible that you can come up with a work-around on your own!

What the engineer really needs is to make the same thing happen in their development environment.
The challenge is that data is different in production and staging. But if you can reproduce the
same bug in both production and staging, which also have different data, they you can probably
explain how to reproduce it in development.

Not every bug can be reproduced. Or rather, *very occasionally*, the step of steps to reproduce is
so esoteric that we cannot reasonably divine them. For example, this bug only happens on Tuesdays
to users in Chile, when there are at least 100 other users in the system. But I can't stress enough
that this is NOT the typical case. 99% of the time, you should be able to reproduce a bug. After all,
if you cannot reproduce a bug, how will we know whether we have fixed it?


# Information to Include

- Facts (make sure they are right)
- Speculation (this is OK, but label it as not a fact)
- What you have already tried that did not work
- Error messages
- User names, record IDs
- Screen shots (not Word documents, please!)
- Stack traces


# Pro-tip

If you're testing a web-app, you should know how to view the development console. This can often
reveal stack traces or at least errors messages when stuff goes wrong. This is bug report gold!

In Chrome:

1. Go to View -> Developer -> Developer Tools
2. Open the "Console" tab
3. Reproduce the bug. You may see a red error like this:

![console](http://mcgivery.com/wp-content/uploads/2015/07/error.png)
---
title: Abstracting away session handling in Flask/SQLAlchemy
---

Having used the Django ORM for years, I appreciate the power of the SQLAlchemy ORM. Many complicated SQL queries that are not possible to express in a single Django QuerySet are possible in SQLAlchemy. But one thing I do not appreciate is the requirement to explicitly handle sessions all the time. It's another power feature; sometimes you need to manage your own sessions. But for web apps, most of the time the session should be tied to the request life cycle; the session should only be committed if the request returns a 200 status code, any errors should roll back the session. Also, passing the session around everywhere in your stack is just a pain.

In my apps, I have fallen into a interesting pattern. I use the [declarative base](http://docs.sqlalchemy.org/en/rel_0_8/orm/extensions/declarative.html) method of defining my Models. This allows me to not explicitly invoke a session for read queries. For writes, I actually implement methods on my base declarative object.

```python
from sqlalchemy import create_engine
from sqlalchemy.exc import DatabaseError
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import scoped_session, sessionmaker


engine = create_engine('sqlite://')
session = scoped_session(sessionmaker(autocommit=False, bind=engine))


class MyBase(object):

    def save(self):
        session.add(self)
        self._flush()
        return self

    def update(self, **kwargs):
        for attr, value in kwargs.items():
            setattr(self, attr, value)
        return self.save()

    def delete(self):
        session.delete(self)
        self._flush()

    def _flush(self):
        try:
            session.flush()
        except DatabaseError:
            session.rollback()
            raise


MyModel = declarative_base(cls=MyBase)
MyModel.query = session.query_property()
```

Then, all your Models inherit from `MyModel` and you can do things like `UserModel(name='Foo Bar').save()`, or `user = UserModel.query.get(1234); user.update(name='Bat'); user.delete()`. You never have to pass sessions around. Notice that I'm only *flushing* the session, however.


# Commits

When do things get committed? I do that in my Flask middleware:

```python
from sqlalchemy.exc import DatabaseError

from models import session  # this is the same session object defined above


@app.after_request
def session_commit(response):
    if response.status_code >= 400:
        return
    try:
        session.commit()
    except DatabaseError:
        session.rollback()
        raise
    # session.remove() is called for you by flask-sqlalchemy
```

If you go down this path, you will need to remember to commit sessions for non-Flask frameworks. For example, in command line scripts I just import the session and commit it at the end. For celery tasks, I do something like:

```python
class MyTask(Task):

    def __call__(self, *args, **kwargs):
        try:
            return super(MyTask, self).__call__(*args, **kwargs)
        finally:
            session.commit()
```

# Tests

A fun side-effect of this pattern is that your integration tests do not necessarily have to commit anything. Flushing is enough to be able to query for the records in the same transaction, which is typically all you need in tests.

That being said, I do still make sure to tear down sessions and re-create my schema for each test:

```python
from models import engine, session, MyModel


class MyTest(TestCase):

    def setUp(self):
        session.close()
        session.remove()
        MyModel.metadata.drop_all(engine)
        MyModel.metadata.create_all(engine)
        super(MyTest, self).setUp()
```

This mainly is to isolate tests from each other if they are invalidating the session with a DatabaseError, or if they are actually changing schema somehow.
---
title: Designing for the Mental Model
tags: product-management
---

Software is typically more complicated under the hood than it looks on the surface. A good product presents a simple and consistent user interface. Ideally, the implementation is also simple and consistent with the story the UI is telling. But that's often not possible. If the implementation is complex, if and when that complexity leaks through to the user experience, it will violate the user's expectations and provide a bad experience.


# Mental Models

This user expectation is what I will call their mental model of your product. A good example of a simple mental model, and one that is never violated, is iOS.

![ios home](/blog/images/ios_home.png)

Under the hood, there is a more or less traditional operating system with a file system, a multi-tasking process scheduler, even a [command line](http://www.iphonefaq.org/archives/971616). But you *never* see these things. Do you even know what a file path on iOS looks like? You don't, and that's because the user interface model is inviolate - it never breaks down.

Even when things are not working, the user model holds. You never crash to a command line, or see an error dialog with a file path in it. A task running in the background never crashes and throws an alert on top of your current app.

These behaviors enforce the mental model that there IS nothing underlying the iOS home screen. In some sense, that is the lowest level the user is aware of.

Apple has a history of inviolate mental models. When I started on Macs, the GUI was already the only thing that existed. There was no "there" underneath.

![system 6](/blog/images/system6.jpg)

Applications were icons; there was nothing else. You installed and uninstalled them by moving the icon around, or to the trash. There was no other way to make an action happen than to navigate to a menu item; that's where the platonic form of that action "lived".


# Mental Model Breakdowns

When the user's mental model is violated, it's a poor user experience. This often happens with new products especially.

In recent versions of OS X, the Notes app has become a cloud synced system. Apple kind of sucks at cloud stuff; their v1 products especially rarely work smoothly. Yet, they did not go out of their way to prevent the inevitable errors from breaking the user's mental model.

When you create a note on your Mac, it's supposed to show up on your phone. What if it does not show up? You don't know why. Maybe the internet is slow. Most likely it's an obscure iCloud permissions issues; more and more, "sign out of iCloud and sign back in" is becoming the "did you try rebooting?" of the Mac experience.

In any case, there is no feedback that something went wrong. No status message, no errors, not even a counter of items still being synced. Nothing. When "just works" doesn't actually work, you have no recourse.

I'm sure some designer somewhere thought this was a great design. Why burden the user with what's happening behind the scenes? I can just here them saying that this is a bug on the backend, and once that's fixed their perfect design will once again result in a perfect product. Newsflash - to the user, your design sucks. They are not making the distinction between interface design and implementation. To them, the design is what failed.

The same issues plagued the new photos app on release. You drag photos into a folder. Nothing happens. Did it get my pictures? Not sure. You drag them again. Nothing. Five minutes later, iCloud syncs and you have two copies of your images in the folder. Why isn't there a status bar or a count of how many items are pending - SOME user feedback when the user makes this fundamental action? Probably because the status bar would be "ugly". More and more, Apple's ideal UI is an empty white box.


# Now, You Try

If you're working on products, especially early products, plan to not violate the user's mental model. Plan for things not working perfectly all the time. When errors do happen, you have two choices:

- Make the internals visible
- Generic/unhelpful error/status messages

Assuming perfect implementation is a bad design - or at least the user would say it's bad design. Which is a better user experience?
---
title: What makes a compelling product?
tags: product-management
---

Words matter. When we use the term "product", what do we mean? Are we talking about the same thing? In the context of tech startups, we often mean product with a capital "P" - as in [Minimum Viable Product](https://www.quora.com/What-is-a-minimum-viable-product). While there is a lot of discussion about what constitutes the "minimum" part of MVP, the actual definition of product is often implicit, not explicit.

If we narrow our focus to just software products, how do you know whether what you're looking at constitutes a product? I'm not 100% sure, but I know it when I see it. I can tell you some things that are NOT required to be a product:

- Specific technologies (mobile apps, web)
- Money changing hands (ex: Google Search, Facebook)
- A business plan/monetization plan (this may be required to be successful, though)

What is certainly IS required for a product is a user. A product solves a problem for a user. A "user" may also be a company (i.e. B2B products), but it cannot be YOUR company. You are solving a problem for someone else, not yourself.


# A Compelling Product Solves a User Problem

"The first step is figuring out the problem that needs to be solved and then developing a minimum viable product (MVP) to begin the process of learning as quickly as possible."

A good product has one central value thesis, one primary user problem that it solves. Certainly you can solve many related problems, eventually. But users will always associate your brand with one core problem you are solving. Users should be able to articulate the problem you are solving. If they cannot, your product thesis may not be as strong as you think.

Users should be actively coming to you to solve this pain point. IMO, a web based product is not something that a user "ends up at" by mistake. In order to claim a spot in a user's mental space, your product must also be focused and have sufficient value.

Examples:

- StackOverflow: you end up there, you don't start there. Is the product the user?


# A Compelling Product is Deep

A product can solve a simple, shallow problem. But it may not be a defensible or successful product. If the problem you're solving is easy to solve in other ways, it's going to be difficult to maintain a spot as the primary product users think of for this use case.

Note: this does not mean that your product needs to be complicated. The best products solve a hard problem in a simple way - that does not mean that it's easy.


# A Compelling Product is Cohesive

This is an extension of focusing on one problem. A product may be complex and have many different features, but these features need to be cohesive. If there are many unrelated features, you have more than one product. It should be obvious to a user how all the pieces relate to each other.


# A Compelling Product Keeps Them Coming Back

A web site is something that users consume. A web application is a website that people engage with - they are primarily there to do things, not read things. A web product is something that users come back to often to solve a particular problem.

Re-engagement is one of the most important metrics for a product. What percentage of users are returning on a regular basis? Lack of re-engagement means that either users are not getting value (not able to solve their problem) or the problem you are solving is not that important to them (they decide they don't need to solve it).

There is a corner case where the problem you need to solve only happens very occasionally. That's OK, but only if it's intrinsic to your problem. Example: TurboTax.


# A Compelling Product has a Virtuous Cycle

A virtuous cycle is a feedback loop in your product that makes it more valuable over time. This can have concrete benefits for the user, but it's primarily an indicator of product success for the business.

Examples:

- Google Search results get better as users tell Google which results are the best by clicking on them. Better results means more users, which means more advertising dollars.
- Facebook user growth means there is more content on Facebook for people to look at, which attracts more user signups.
- Netflix user growth means they can negotiate for more content, which drives further user growth.

# A Compelling Product Takes Time

Products to not happen over night. The spirit of MVP is to ship quickly, get user feedback and then iterate. Iteration will take months, if not years. If you can create a good product in less time, what is stopping a competitor from creating the same product?

The challenge of user growth is often underestimated. You need a lot of users to validate your product hypothesis, but attracting those initial users will be very difficult. This is even more true on mobile, where the [cost of user acquisition is much higher](http://venturebeat.com/2015/04/30/it-costs-more-than-3-to-acquire-a-mobile-user-now-fiksu-finds/).
---
title: Using Points vs Hours for Estimates
tags: manager
toc: true
---

One of the key innovations of Agile is that estimates should NOT be in hours,
but in points. But it doesn't just intuitively make sense. As a result, many
teams resist using points. This is one of the reasons why for teams new to
Agile, I always recommend that you try doing vanilla-by-the-book Agile first,
before you change anything. I think that if you do, the value of points will be
obvious.

If you're on a team that has always done Agile, but with hours, do you find
yourselves chronically under-estimating stories and not completing them? If so,
maybe I can convince you to give points a shot.


## Points are Relative, Hours are Absolute

One of the primary insights of Agile is that humans suck at estimating in
absolute terms, but are much better estimating in relative terms.

> Unfortunately, [reams of research](http://pm.stackexchange.com/questions/11675/is-there-any-published-research-about-story-points-vs-time-estimation) shows that humans are inherently horrible at
estimating in time. It turns out when we estimate jobs by how long they will
take us to complete we have an error rate of 400%. [Scrum, Inc](https://www.scruminc.com/points-vs-hours/)

Instead of trying to estimate how long a task with take, simply try to estimate
whether it is larger or smaller than another story. Don't think, "I should be
able to do that in three hours". Compare the story to something you have already
completed. Is it bigger than that 3 point story you did last sprint? Almost
twice as big? Would you say it's another 3, or is it more like 5 or 8?

Fundamentally, we are not estimating how long a task will take, but how large
it is relative to another task. In [terms of an analogy](https://www.mountaingoatsoftware.com/blog/the-main-benefit-of-story-points),
we want to know how long the trail is, not how long it would take you to run it.


## Points Enable Collaborative Estimates

An absolute estimate is how long it would take a particular engineer to complete
the task. If the task needs to be re-assigned to another engineer, it would need
to be re-estimated. Maybe this task involves regular expressions, or JavaScript
or schema design. The skill set between your team members is not uniform; they
will all have strengths and weaknesses.

Points are not pegged to an individual's implementation speed. Maybe we can all
agree that a story is three points, even though it would take the most senior
member on the team an hour, and other people would need five hours. That's fine!
At least we can all collaborate on a common estimate for the story. [This is
likely to make the estimates more accurate](https://www.mountaingoatsoftware.com/blog/dont-equate-story-points-to-hours);
the wisdom of the group may well identify both short-cuts and gotchas in the
details.

One particular related trap is having one person, usually the team lead, estimate
all the stories. Are you sure you are estimating how long it will take the team
to complete the story, and not how long it would take YOU? Big difference.
Besides, the team will be much more committed to estimates that they came up
with themselves.


## Points Communicate Uncertainty

In particular, Agile recommends that you stick to the Fibonacci numbers for
point estimates. Why? Because it accurately communicates the inherent
uncertainty of estimates. Estimates are wrong; everyone on the team should
internalize that. Given that they are going to be mostly wrong, we cannot also
make them very exact. Deliberating on whether a story will take 3 or 4 hours is
pointless, you are well inside the error bars. Deciding whether it's a one
point or a two point story forces you to accept an inherent margin of error.

Points may also allow for more nuance in the estimates. If forced to pick a
number of hours, there will be a tendency to be optimistic. But points should
reflect not only how large a task is, but also how much risk or uncertainty
there is. It's common to say that a story should only be 3 points of work, but
we're calling it an 8 because we're not sure about some of the details.


## Points Make the Team Accountable for Stories, not Estimates

A point is NOT equal to any number of hours - it's an average. Eventually, you
will get a sense that a point may take you anywhere from say 1 to 4 hours of
work. Note: that's just an example, points are always team specific; it's a
mistake to try to compare the velocity of one team to another using points.
But importantly, you do not expect those hours spent to converge; you will always
have a broad range of hours that a single point story took to complete.

This makes it clear to the team that they are not being held accountable for
100% or less overruns or under runs in their individual stories. If you use
hours, there is a great tendency to say "you said this would take one hour, but
it really took three". That doesn't matter! You should not even measure it.
What matters is, did you complete all of the stories in the sprint?


## References

- [Is there any published research about story points vs time estimation?](http://pm.stackexchange.com/questions/11675/is-there-any-published-research-about-story-points-vs-time-estimation)
- [Sprint Planning: Story Points Versus Hours](https://www.infoq.com/news/2009/09/story-points-versus-hours)
- [Scrum + Engineering Practices: Experiences of Three Microsoft Teams](https://collaboration.csc.ncsu.edu/laurie/Papers/ESEM11_SCRUM_Experience_CameraReady.pdf)
---
title: SQLAlchemy - Storing Application Version Strings
---

One example of when you might need to store application version numbers in
your database is when you're tracking which users have which versions of a
mobile app installed. In that case, you may want to preserve the ability to
easily sort the version numbers, so you can answer questions like "Which users
have a version greater than or equal to 9.3.2?"

In that case, you have two options. You can store the version number in three
different fields, such as major, minor and revision number. Or, you can pad
the values such that regular string sorting would work. Why? Because otherwise,
the sorting will be incorrect. For example, the string `10.0.0` is less than
the string `9.0.0` because the first character `9` is greater than `1`. This
would be the behavior you would get with a simple
`SELECT * FROM users ORDER BY version DESC;`

For example, the version `9.3.2` might be stored as `0009.0003.0002`. Here is
a drop-in implementation in SQLAlchemy which allows you to only deal with the
"nice" native format in Python, while storing the padded version in the
database.

```python
from sqlalchemy import types


def to_version_string(value):
    if not value:
        return None
    parts = value.split(VERSION_DELIM)
    if len(parts) > VERSION_NUMBER_PIECES:
        raise ValueError('Version string has too many parts')
    while len(parts) < VERSION_NUMBER_PIECES:
        parts.append('0')
    for part in parts:
        try:
            assert str(int(part)) == part
        except (ValueError, AssertionError):
            raise ValueError('Version part is not an integer')
    return VERSION_DELIM.join([part.zfill(4) for part in parts])


def from_version_string(value):
    if not value:
        return None
    parts = value.split(VERSION_DELIM)
    return VERSION_DELIM.join([str(int(part)) for part in parts])


class VersionString(types.TypeDecorator):
    """ stores a string like '0.9.4' as '0000.0009.0004' so it can be sorted """

    impl = types.String

    def process_bind_param(self, value, dialect):
        return to_version_string(value)

    def process_result_value(self, value, dialect):
        return from_version_string(value)
```

And some quick tests:

```python
from unittest import TestCase
from models import to_version_string, from_version_string


class VersionStringTests(TestCase):

    def test_to_version_string(self):
        self.assertEquals(to_version_string('9.3.2'), '0009.0003.0002')
        self.assertEquals(to_version_string('9.3'), '0009.0003.0000')
        self.assertEquals(to_version_string('9'), '0009.0000.0000')
        self.assertEquals(to_version_string(''), None)
        self.assertEquals(to_version_string(None), None)

    def test_to_version_string_bad(self):
        for value in [
            '9.3.2.1',
            '9.3.a',
            '9.03.2',
            'foobar',
        ]:
            with self.assertRaises(ValueError):
                to_version_string(value)

    def test_from_version_string(self):
        self.assertEquals(from_version_string('0009.0003.0002'), '9.3.2')
        self.assertEquals(from_version_string('0000.0000.0000'), '0.0.0')
        self.assertEquals(from_version_string(None), None)

```
---
title: Management 101
tags: manager newboss
toc: true
---

Coming off some annual management training, I realize that I need to write
some of these learnings down, in order to actually internalize them. This
is mostly a reference for me.

## Getting work done through others

Most of time, engineers in particular are easy to delegate to. Often they
know more than you do about their technical area, and they are intrinsically
motivated to produce. But if one of those assumptions is not true, it's your
job to set them up for success.

![will-skill](https://danspira.files.wordpress.com/2010/11/skill-will-coaching-details.gif)

If your engineer has all the technical proficiency to pull off their project, as
well as the wisdom of experience to do it well, then they are set up for success
in the "skill" dimension. If this is not the case, they may need explicit
coaching, usually in the form of 1:1 pairing, more vigorous code review, and
more real-time feedback on their solutions.

If the engineer is excited to actually do the work because they find it fun,
are motivated by the impact it will have, or are actively working towards a
career development goal, then they probably are set up for success on the "will"
dimension. If not, see *Motivation and career conversations*.

If they are firing on all cylinders in terms of skill and will, make sure you're
not micromanaging their work - give them space to execute.


## Giving feedback

Feedback should ideally be real-time, but private. If you're in a meeting and
you have some feedback for a team member, try to give it to them in person,
privately, right after the meeting. In general, praise can be public, but
constructive criticism should be in private. Never criticize in front of others.

One on ones are a great catch-all for any feedback that you did not give
in real-time. See [my previous blog post](http://chase-seibert.github.io/blog/2015/02/10/one-on-ones.html)
for more.

Some people actually do not like positive feedback. It can make them
uncomfortable. That's fine, but you should still find a way to give it. One
strategy is to call out that you know they don't like getting positive feedback,
but that you want to make sure they know what their strengths are. Another
tactic is to ask them to identify what they are doing well - and call out when
you agree with them.

Similarly, some people will prefer totally direct and frank feedback, even if
it's constructive. That's down to knowing the person well.

Whatever the scenario, you want to seek to understand the other person's point
of view, not just make sure you cover your own talking points. They should
talk more than you do. Also, keep notes on what topics you cover - if only
to remind yourself later.


## Motivation and Career

Motivation can either be intrinsic or extrinsic. Engineers will be intrinsically
motivated if they find the work fun or if they know that it will be high impact.
Extrinsic motivation could be compensation, or working towards a career goal.
But it's important to realize that extrinsic motivation is fleeting - it's not
sustainable if engineers are motivated primarily by monetary rewards or
promotions. Those are great, and we should maximize those, but they also need
to be intrinsically motivated.

How do you know what motivates someone? You can ask them, but often people will
not be that self-aware. One good tactic is to ask them "What was the highlight
of that last quarter/project for you"? Take notes on their responses over time,
some patterns will become apparent. Retrospectives are also a great source
of data here.

In terms of career conversations, you want to focus on helping the person
maximize their strengths. You want them to be aware of their weaknesses, but
we typically do not need to turn those into strengths - we just need to get
them to an acceptable level where they are not interfering with the work.

Some opportunities for growth are new responsibilities, special projects,
setting aside time for explicit learning, and having them teach others.


## Hard conversations

An inevitable part of being a manager is giving negative feedback. Maybe an
engineer is really not performing up to the bar. You want to address those
early - not wait until a performance review. Performance reviews should never
be surprises.

You want to begin by asking open ended questions. Truly give them space to
identify the problem, give their perspective and offer solutions.

Often you will need to highlight the problem. Put it in the context of the
impact that it's having. If you have feedback from other people on the team,
one way to phrase it is "when you do this, it can be perceived like this".
Again, provide the space for the engineer themselves to come up with a solution.

If it's not happening, reset and try to at least come to consensus that there is
a problem. Then be straightforward in saying that this is something we need to
fix. Either way, make sure you have clear next steps and commitment, and follow
up during one on ones.


## Triggers

You need to be aware of when people are having primordial, lizard brain
subconscious reactions during meeting and conversations. I definitely notice
this in myself. Try to identify what topics tend to elicit emotional reactions
in yourself. The safest course of action when you realize that it's happening
is to table the conversation for later. Alternatively, you can call out that this
is triggering you. That may defuse the situation.
---
title: PyGithub Quickstart Examples
---

[PyGithub](https://github.com/PyGithub/PyGithub) is the most popular GitHub API
SDK for Python. Their [documentation](http://pygithub.readthedocs.io/en/stable/introduction.html)
is very light on examples. They [seem to think](https://github.com/PyGithub/PyGithub/issues/321)
this is fine. Prime candidate for the new
[Stack Overflow Documentation](http://stackoverflow.com/tour/documentation) site!

In the meantime, I thought I would share my example code. These took me an hour
of playing with the REPL to figure out.

## Working with Pull Requests

The main challenge here was realizing that I needed to scope everything to
my private org, not my user.

```python
git = PyGithub('GITHUB_TOKEN')
org = git.get_organization('OrgName')
repo = org.get_repo('repo-name')
pr = repo.get_pull(1)
print 'PR author: %s' % pr.user.login
comments = pr.get_issue_comments()
for comment in comments:
  print 'Comment: ', comment.created_at, comment.user.login, comment.body
pr.create_issue_comment('Comment from GITHUB_TOKEN user') # aka git.get_user()
```

## Getting the Contents of a File

From the default branch, not a particular pull request.

```python
git = PyGithub('GITHUB_TOKEN')
org = git.get_organization('OrgName')
repo = org.get_repo('repo-name')
file_contents = repo.get_file_contents('path/to/file')
```

## Listing the Members of a GitHub Team

There is no method to get a team by name, so you need to get them all and
then pick out the one you want.

```python
git = PyGithub('GITHUB_TOKEN')
org = git.get_organization('OrgName')
teams = org.get_teams()
team = [t for t in teams if t.name == 'TeamName'][0]  # assumes a match
print [m.login for m in team.get_members()]
```
---
title: Cross Functional Scrum Teams
tags: scrum newboss
toc: true
---

> Teams are also cross-functional; Team members must have all of the skills
necessary to create an increment of work. - [Scrum Guide](http://www.scrumguides.org/docs/scrumguide/v1/scrum-guide-us.pdf)

I've been doing scrum for maybe eight years. When that first team
enthusiastically adopted sprints, points and grooming, our stack was Java
backends and ColdFusion on the front-end, with a smattering of jQuery. Perhaps
due to chance, or perhaps because the stack was so simple, all three engineers
on the team were fully fluent with all the technologies.

In the last few years, the complexity of the front-end has grown tremendously,
and it's become common practice for engineers to specialize in front-end or
back-end. Recently, my team has also added a mobile specialty. I've largely
encouraged this specialization because that seems to be the way the industry
is going, but recently I've been rethinking it.

Instead, I'm going to try getting back to my teams being full cross-functional.
That means everyone regularly working in all parts of the stack. I'm not sure
this is better for all teams, but I think it's a good idea for a small 3-5
person product team that has to do with in many parts of the stack. Why?

## Better for Team

One tenet of scrum is group estimation and grooming. The more people that are up
to speed on the technologies and the codebase, the more likely you are to think
of all the potential gotchas related to a story. There will be fewer occasions
where a story becomes vastly more work than anticipated.

Ideally the entire team is taking ownership of ALL the stories in a sprint. I've
noticed that with specialized teams, we tend to assign a story to an individual
even before sprint planning - in some cases there is really only one person
who can do a job. If the sprint is almost done, and only a mobile story remains,
a specialized team is likely to bring in a new unrelated story to work on, rather
than swarming on the story the team actually committed to.

Additionally, cross-functional teams can also be useful for sprints where there
happens to be a lot of high priority work in one area. It allows the team to
actually deliver the most important stories, and not compromise on a mix of
importance and specialized bandwidth.

Finally, on small teams, you can remove single points of failure by being
cross-functional. On a specialized team, if your only mobile engineer goes on
vacation or quits, it's much more disruptive.

## Better for Individuals

Cross-training can be good for individual engineers, too. During an engineering
career, you will find yourself needing to pick up new languages and technologies
on a regular basis. Failure to do so can make it difficult to stay relevant (read: employed), or capitalize on job opportunities. The choice of what to specialize
in can be tricky. What if you spend a bunch of time learning Angular, but then
it's replaced by ReactJS in the zeitgeist just as you're looking for a new job?
To some extent, cross-training lets you have more irons in the fire at once. You
can place more bets in parallel.

There is also a more subtle danger to having engineers work predominantly in
one part of the stack; they may get burned out. Especially when moving from one
specialized role to another, there is a chance that the engineer will wake up
one day and feel that the move was a mistake. Even if the engineer knows at an
intellectual level their manager is more than happy to have them change
specialization again, it might be easier mentally to simply look for another job.
This could even be an unconscious process.

## Mixed Bag for the Product

It's a less obvious win at the product level. A team of specialists will
produce more raw output than cross-functional team. This is most obviously true
during the ramp-up phase when one or more engineers is learning a technology for
the first time. Even more so when multiple engineers are doing it all at once,
such as if you try to transition from specialization to cross-training. But it's
even true to some extent at steady state; keeping up with 3x as many technologies
is that much more overhead.

But it does allow you to solve a given problem at the right place in the stack.
This can result in great complexity savings, which ultimately translates into
velocity down the line. All too often, I see engineers solve a problem in a
complicated way in the wrong part of the stack, simple because that's the tool
they have for solving it. This could probably be solved somewhat by a great
tech lead who's doing super detailed grooming - but that person would themselves
need to be cross-functional.

In the end, the biggest product wins are better code reviews and thus product quality,
and better grooming/estimates.

## In Practice

In the real world, you are always going to have engineers who are better at
some parts of the stack. Those people should definitely be first in line for
code reviews and architecture design white boarding for those areas. They are
also great candidates for code pairing, especially when training up another
cross-functional engineer.

To some extent, cross-training is inevitable on small teams. With three
engineers, having fully separate back-end, front-end and mobile roles is going
to introduce a lot of schedule trashing even for simple vacations. If an engineer
leaves, you may be dead in the water for quite some time. On that level, having
at least some redundancy in a given area is a business imperative.

Some engineers are simply not going to want to do this. A common
refrain is that it's not the most efficient setup. Another reason may be that
they simply will not enjoy working in a specific part of the stack. That may be
true, or maybe they just haven't given it a try yet. For these engineers, perhaps
are more realistic goal is to have them be "T-shaped" with their skills: deep in
one area but with some baseline skill in the entire stack.
---
title: Tiger Team Survival Guide
tags: manager
toc: true
---


> A tiger team is a diversified group of experts brought together for a single project, need, or event. They are usually assigned to investigate, solve, build, or recommend possible solutions to unique situations or problems. They are almost always populated with mature experts who know what's at stake, what needs to be done, and how to work well with others. Their strengths are diversity of knowledge, a single focus or purpose, cross-functional communications, decision-making sovereignty, and organizational agility. Once their venture is completed they cease to be a team and usually go back to their previous assignments.


## Scope, Time and Money

As an engineering manager, you can typically have some input on the scope, time and resources allocated to a project. This is a variation of the ["pick two problem"](https://en.wikipedia.org/wiki/Project_management_triangle#.22Pick_any_two.22). When the project scope and timeline are largely dictated by business needs for a high priority project, resourcing may be the most flexible factor.

If possible, identify a shortlist of veteran engineers with specific skill sets that can make the project successful. Talk to their managers and get buy-in. Work to alleviate any concerns the individuals may have about what you're asking from them, and whether their existing projects will fall on the floor.

Some specific resources to think about:

- Senior back-end or front-end talent
- Make sure you look at planned vacation time before asking for individuals.
- Devops
- Project management/co-ordination/communication support
- Outside experts/contractors - can you find someone who is a professional trainer in an area you need help in on short notice?
- Are you planning on making a deep technical contribution yourself, or focusing on organizing/leading?
- Make sure you have 100% alignment on the KPI for the project.

## The War Room Model

A natural instinct on a short schedule is to find a private space where the team can isolate themselves, have persistent wall space for collaborating, and cancel all other obligations within reason.

Try deputizing a specific person from workplace ops and have them handle logistics:

- Find a room big enough for the team, with room to spare. Should have ample whiteboard space and a monitor.
  - Nearby breakout spaces and or extra standing desks is ideal.
- Make sure people have badge access if needed.
- Increase lunch order size if you're moving offices.
- Plan on canceling all your own meetings except direct report 1:1s, 1:1 with your boss and critical status meetings.

Potential IT asks:

- Extra monitors, keyboards, mice and power strips in the war room.
- Any new hardware you might need (test devices).

Other stuff:

- Ask recruiting to cancel all interviews for the team
- Ask for support when engineers go to clear their calendars
- Block off everyone calendars with individual all day appointments
- Using one all day appointment that spans days or weeks is likely to be ignored by people booking conflicts

Communication tools:

- Create a Slack group
- Create an email distribution list
- Create a JIRA or Trello board
- Create a Google Docs folder
- Start compiling a list of important team links and put them on a Trello card, pinned to the Slack channel topic

## Reducing Risk

- What dependencies do you have on other teams?
  - For engineering dependencies, try to get a dedicated domain expert on the tiger team.
  - Are the APIs you need to use publicly accessible? Do they require some specific authentication, scopes or do they return results that are too highly tailored to another use case?
- Starting thinking about what you need to do for security sign off.
- Starting thinking about what you need to do for compliance sign off.
- Have design start with high level mocks - not pixel perfect. Get the flow nailed down ASAP.
- What kind of existing code will you be able to reuse?
- Does the re-use require another team to refactor something?
- What is your analytics/data integration plan?

## Sample Kick-off Day Agenda

- *start at 10am sharp*
- Intros (5 min)
- Business context and goals
- Initial mocks share out
- Need a large format print out what we can put on the wall and write on
- Define milestones
- What is everyone hoping to get out of this experience?
  - Write it down as a list on a wall
- Working agreement
  - Whitelist of outside meetings we will attend
  - Brainstorm how we can move fast - create a speed list
  - Create technical debt list
  - Create a big decision list
- Process - define how we want to work
  - Working hours
  - Daily stand up
  - Pairing
  - No code reviews?
  - Multiple merges a day
  - Daily demo
- Pair assignments and component ownership
- *lunch at 12pm*
- Code reuse strategy
- Break outs
  - Front-end architecture, how to break things into components?
  - Back-end architecture, high level system capabilities needed
- Code layout and separation of concerns
  - Write it down, eventually copy to README)
- Hacking
- Demo
- *done at 6pm*

## Make it fun

Don't make it a death march. Plan to work reasonable hours and have some fun activities mixed in.

- Booze; plan to start drinking early and often
- Video games
- Tiger team branded laptop stickers, T-shirts
- Donuts/muffins/coffee
- Massage Station
- Sonos
- Go to a related meet up as a group
- Get a photographer to document your awesomeness for posterity
- Special lunch outings (food trucks, local spots)
- Get out of the office and play basketball, or go to a movie as a team
---
title: Deploying React Native apps with Buddy Build
tags: app react-native
---

[Buddy Build](https://buddybuild.com/) is a great tool for getting your app out
to physical devices, whether that is during development, or in production. Their
tool is fairly agnostic to frameworks. For example, their documentation is nearly
devoid of mentions of React Native, but it's actually quite simple to get a
React Native app building.

In fact, nearly all the challenges we faced involved specific requirements, for
example having multiple builds with different configurations. In the end, most
of our learnings were about React Native itself. But maybe this guide will be
useful to other people who are trying to not only figure out how Buddy Build
works, but how React Native builds work.


# Private NPM Repositories

If you have a private NPM repository, you can upload the `.npmrc` file
using the "Secure Files" feature in Buddy Build. We ended up doing the same
for gradle.properties. For the later, you need a [custom post-clone script](http://docs.buddybuild.com/docs/custom-prebuild-and-postbuild-steps)
to copy the file into the correct location.

```bash
#!/usr/bin/env bash

cp ${BUDDYBUILD_SECURE_FILES}/.npmrc ${BUDDYBUILD_WORKSPACE}
mkdir -p ~/.gradle
cp ${BUDDYBUILD_SECURE_FILES}/gradle.properties ~/.gradle
```

# Multiple configurations

Maybe you have a use case where you want to have multiple builds that you actually
distribute to phones for testing. In our case, we wanted a staging build and a
production build. Both needed to be fully self-contained, with the Javascript
packaged into the app. But the URLs for the APIs they were hitting, as well as
some of the API keys, needed to be different.

We ended up using [react-native-config](https://github.com/luggit/react-native-config)
to create `.env.staging` and `.env.production` config files. In order to be able
to re-use some exiting Node code, we also mapped these into the
`process.env` space.

```javascript
import Config from 'react-native-config';

const env = Config;

// this is for backwards compatibility with existing Node pattern
export default function injectEnv() {
  Object.assign(process.env, env);
}
```

# Debug Builds

For iOS, React Native has two "variants", `debug` and `release`. For Android,
React Native will have gradle builds with the same names. In both cases, the
primary difference is whether the Javascript code is packaged and bundled with
the build, or loaded from a live packager running outside the app. Initially, we
tried to separate configuration by debug/release, and were confused when debug
builds deployed via Buddy Build crashed immediately. It turns out they were
trying and failing to connect to a packager.

The solution is to have *multiple release builds*.

# Multiple iOS Builds

On iOS, making a second release build is just duplicating your Release scheme in XCode.
**Note: the new schemes must be marked as "shared" in XCode.** You also want to
make sure that in the scheme definition, all the targets for Build/Run/Archive etc
are set to "Release", not "Debug". This is what React Native uses to determine
whether to package the Javascript code, or not.

You can follow the [react-native-config iOS setup](https://github.com/luggit/react-native-config#ios-1)
to get it to pick up a different `.env` file per scheme. We ended up using the
recommended `Build -> Pre-action` step, which worked fine. You could probably
also get our Android branch solution to work.

# Multiple Android Builds

On Android, creating a second release build was more problematic. It should be
possible to create additional gradle variants, but after a lot of experimentation,
we could not stop those builds from crashing on physical devices. At a low level,
we were never sure how React Native was choosing to bundle the package, or not.
We assumed it was based on the name of the build, but we're not sure.

What we ended up doing is changing our build script to push multiple release
branches up to GitHub, one for `staging` and one for `production`. We then
configured the Buddy Build project for Android to run a custom pre-build script to
copy in the correct `.env` file based on git branch.

```bash
#!/usr/bin/env bash

echo "BUDDYBUILD_BRANCH: $BUDDYBUILD_BRANCH"
if [ "$BUDDYBUILD_BRANCH" = "production" ]
then
   echo "Using .env.production"
   cp ${BUDDYBUILD_WORKSPACE}/.env.production ${BUDDYBUILD_WORKSPACE}/.env
else
   echo "Using .env.staging"
   cp ${BUDDYBUILD_WORKSPACE}/.env.staging ${BUDDYBUILD_WORKSPACE}/.env
fi

cat ${BUDDYBUILD_WORKSPACE}/.env
```

# Buddy Build SDKs

The SDKs, which enables user feedback in the app, was fantastically useful.
Highly recommended!
---
title: React Native Multiple Release Variants
tags:
---

By default, React Native gives you a `debug` variant and a `release` variant.
In debug mode, the app connects to a running packager service, and enables various
`__DEV__` checks that could slow down performance. In release mode, the JavaScript,
CSS and JSX are bundled into the app itself, and optimizations are enabled.

You may have a use case for multiple `release` variants. For example, you want
a staging and production build of you app, and you want to deploy them both
to physical devices for testing.

In that case, you would add the following to the `buildTypes` section of your
`android/app/build.gradle` file:

```bash
buildTypes {
    debug {
        applicationIdSuffix ".debug"
    }
    release {
        minifyEnabled enableProguardInReleaseBuilds
        proguardFiles getDefaultProguardFile("proguard-android.txt"), "proguard-rules.pro"
        signingConfig signingConfigs.release
    }
    releaseStaging {
        initWith(buildTypes.release)
        applicationIdSuffix ".staging"
    }
}
```

Note that the naming convention for `releaseStaging` is actually significant. We
originally tried just `staging`, and ended up getting the following error when
deployed to a physical device:

```bash
java.lang.RuntimeException com.facebook.react.devsupport.JSException
Could not get BatchedBridge, make sure your bundle is packaged correctly
```

It turned out to be that the build had not bundled assets. You can actually correct
that by using the `project.ext.react` directive in `android/app/build.gradle`, as
noted in the in-line comments of that file. However, though that gave us a working build,
performance was suddenly horrible. We eventually figured out that the build was in
`__DEV__` mode.

Looking at the React Native source, we found this reference in
[eeact.gradle](https://github.com/facebook/react-native/blob/e083f9a139b3f8c5552528f8f8018529ef3193b9/react.gradle#L79):

```javascript
def devEnabled = !targetName.toLowerCase().contains("release")
if (Os.isFamily(Os.FAMILY_WINDOWS)) {
    commandLine("cmd", "/c", *nodeExecutableAndArgs, "node_modules/react-native/local-cli/cli.js", "bundle", "--platform", "android", "--dev", "${devEnabled}",
            "--reset-cache", "--entry-file", entryFile, "--bundle-output", jsBundleFile, "--assets-dest", resourcesDir, *extraPackagerArgs)
} else {
    commandLine(*nodeExecutableAndArgs, "node_modules/react-native/local-cli/cli.js", "bundle", "--platform", "android", "--dev", "${devEnabled}",
            "--reset-cache", "--entry-file", entryFile, "--bundle-output", jsBundleFile, "--assets-dest", resourcesDir, *extraPackagerArgs)
}
```

In other words, you must have the token "release" in the build variant name for regular
release build behavior to apply.
---
title: Appium + React Native Quickstart
tags:
---

[Appium](http://appium.io/) is a great framework for automated functional testing
of mobile apps. But it's very general purpose, meaning that there is relatively
little documentation for specific mobile app architectures. It's also fairly
complicated. Being "Selenium for mobile", and leveraging a lot of existing
Selenium architecture, it requires a lot of context to get up and running.
This is a guide for getting starting testing a React Native app on Appium,
without assuming too much prior knowledge.

# Moving Pieces

At a high level, you write code in any number of supported languages that
comprise your tests. You can use your existing test framework, i.e. py.test for
Python, Mocha for JavaScript, whatever you want. In your test bootstrapping, you
are going to instantiate a Appium "web driver"[1] client, and configure it. The
driver will connect to a running `appium` service, which you will have installed
and kicked off in the background. Down the line, you can get a vendor like
[SauceLabs](https://saucelabs.com/) or [AWS Device Farm](https://aws.amazon.com/device-farm/)
to run that in the cloud if you want.

In your tests, you use the driver to tell the appium service to launch your app
in a simulator. Eventually, you can also tell it to run on a physical device. You
give it a path to your app `.app` file for iOS, or the `.apk` for Android, and it
can either re-use an emulator that you've already launched on your desktop, or
start one.

Once the app loads, you issue commands to the driver to perform certain actions
inside the app. Common examples are clicking on buttons or links, entering data
into forms and checking for content on the screen. Those checks are essentially
the asserts in your tests. If at any point your tests cannot locate the item
that it's expecting on the screen, you will get an exception and that test will
fail.

[1] The term "web driver" is a historical hold over from Selenium.

## Potentially Confusing Parts & Gotchas

I've written some Selenium test suites in the past, but it's been a while. I
remember having some of the same initial confusions with Appium.

- The `appium` service needs to be launched and run in the background; your
tests do not kick that off for you.
- You can start your emulator manually and have Appium connect to that instance.
It happens automatically, for Android anyway.

# Installing Stuff

Appium's [getting started](http://appium.io/getting-started.html) doc has a
surprising amount of confusing cruft on it, but you should reference it for the
latest instructions.

As of 2017-01-16, this is a TL;DR version to get you started with everything
you need for both iOS and Android.

```bash
brew install libimobiledevice --HEAD
brew install carthage
brew install node
npm install -g appium
npm install wd
npm install -g ios-deploy
gem install xcpretty  # optional
appium
```

That should get you a running `appium` process. Just leave it open in a tab.

Example output:

```bash
[Appium] Welcome to Appium v1.6.3
[Appium] Appium REST http interface listener started on 0.0.0.0:4723
```

# Running a Dummy Test

*Note: Your React Native app build should be in production mode. Otherwise,
selecting elements on the page will be super crazy slow. You can validate that
you're in the right mode by killing your packager; the production builds should
have all the JS assets packaged into the app.*

Before getting into the full blown details of how to write a full test, I found
that I first just wanted to make sure I could write some code that launched my
app correctly in the emulator. I suggest starting with Android if possible; iOS
apps under XCode 8 and later required a [bunch of additional moving pieces](https://github.com/appium/appium-xcuitest-driver).

Appium has a bunch of [sample client code](https://github.com/appium/sample-code/tree/master/sample-code/examples)
for various languages. I found it to be too much; it's hard to tell what's abstracted
where, and if you don't know the test runner they have chosen, there may be
additional learning curve. I'm going give you a very simple example in Python.

**Note: we're assuming that your app has a link with specific text in it on the
first screen, and clicking that links loads a second page with more specific
text on it.**

First, you need to install the Python client:

```bash
virtualenv venv
. venv/bin/activate
pip install Appium-Python-Client
```

Then, create a file called `test.py`:

```python
import os
import time
from appium import webdriver

driver = webdriver.Remote(
    command_executor='http://127.0.0.1:4723/wd/hub',
    desired_capabilities={
        'app': os.path.expanduser('~/Downloads/app-release.apk'),
        'platformName': 'Android',
        'deviceName': 'Nexus 6P API 25',
    })

# wait for app to load
time.sleep(10)

# find the link with the text "Click here" and click on it
link = driver.find_element_by_xpath('//*[@text="Click Here"]')
link.click()

# wait for the next screen to load
time.sleep(10)

# make sure the correct "Success" result is on the page
driver.find_element_by_xpath('//*[@text="Success"]')

# important; you will not be able to launch again if this does not happen
driver.quit()
```

This is terrible code, but it should launch your app, in any case. From there
you can start to play around with it. Try inserting a `import pdb; pdb.set_trace`
before the first `sleep` to get an interactive repl where you can try querying
for selectors.

# Writing Tests

You might notice that we're using a bunch of sleeps. That's a common anti-pattern
in both Selenium and Appium; you don't want to rely on indeterministic page loading
timing. We're also using xpath selectors. I don't know about you, but I never want
to write another one of those. Let's explore some options.

## Useful Patterns

My goal here is to make individual tests as simple to define as possible, and to
remove boilerplate, waits and finicky selectors from them.

### A Base Test Class

It's common to have a base class for all your functional tests that does the basic
starting the simulator and loading of the app. But it's also useful as a place to
put re-usable methods for making writing tests easier.

Replace the contents of your `test.py`:

```python
import os
import unittest
from appium import webdriver


class AppiumTest(unittest.TestCase):

    def setUp(self):
        self.driver = webdriver.Remote(
            command_executor='http://127.0.0.1:4723/wd/hub',
            desired_capabilities={
              'app': os.path.abspath('~/Downloads/app-release.apk'),
              'platformName': 'Android',
              'deviceName': 'Nexus 6P API 25',
        })

    def tearDown(self):
        self.driver.quit()


class ExampleTests(AppiumTest):

    def test_login(self):
      assert True        
```        

If you're using `py.test` (avaiable with `pip install pytest`), then you can run
your example test with the command `py.test test.py`.

## Selectors

The hardest part of writing tests is figuring out how to locate the elements on
the screen that you need to interact with, or validate. Whether you're dealing with
iOS or Android, Appium exposes an XML DOM that represents what's currently on
the screen. You can actually see the full DOM at any time by evaluating
`driver.page_source`. It get's pretty complicated quickly. Here is vastly
simplified example:

```xml
<?xml version="1.0" ?>
<hierarchy rotation="0">
	<android.widget.FrameLayout bounds="[0,0][1440,2392]" checkable="false" checked="false" class="android.widget.FrameLayout" clickable="false" content-desc="" enabled="true" focusable="false" focused="false" index="0" instance="0" long-clickable="false" package="com.myapp" password="false" resource-id="" scrollable="false" selected="false" text="">
		<android.widget.LinearLayout bounds="[0,0][1440,2392]" checkable="false" checked="false" class="android.widget.LinearLayout" clickable="false" content-desc="" enabled="true" focusable="false" focused="false" index="0" instance="0" long-clickable="false" package="com.myapp" password="false" resource-id="" scrollable="false" selected="false" text="">
  		<android.view.ViewGroup bounds="[0,224][1440,1655]" checkable="false" checked="false" class="android.view.ViewGroup" clickable="false" content-desc="" enabled="true" focusable="true" focused="false" index="0" instance="6" long-clickable="false" package="com.myapp" password="false" resource-id="" scrollable="false" selected="false" text="">
  			<android.widget.TextView bounds="[53,224][1388,298]" checkable="false" checked="false" class="android.widget.TextView" clickable="false" content-desc="" enabled="true" focusable="false" focused="false" index="0" instance="0" long-clickable="false" package="com.myapp" password="false" resource-id="" scrollable="false" selected="false" text="Sign in"/>
  		</android.view.ViewGroup>
		</android.widget.LinearLayout>
		<android.view.View bounds="[0,2392][1440,2560]" checkable="false" checked="false" class="android.view.View" clickable="false" content-desc="" enabled="true" focusable="false" focused="false" index="1" instance="0" long-clickable="false" package="com.myapp" password="false" resource-id="android:id/navigationBarBackground" scrollable="false" selected="false" text=""/>
	</android.widget.FrameLayout>
</hierarchy>
```

### XPath

See the `TextView` element with the text `Sign in`? If you want to locate that
element, you can select it using XPath:

```python
link = driver.find_element_by_xpath('//*[@text="Sign in"]')
```

This selector looks for any DOM element (`*`) that has a `text` attribute of
"Sign in". What if there is more than one on the page? You can use
`find_elements_by_xpath` (notice the plural "s" on elements) to get the full
list, and these pick the right one by index.

XPath is somewhat finicky. As your DOM changes, hardcoded text values and
especially indexes for multiple matches will change often. This will break
your tests.


### Accessibility Labels

A better option is to put IDs on elements and use a selector for that specific ID.
Typically, options here are test IDs, test tags and accessibility labels. Until
React Native [implements test IDs](https://github.com/facebook/react-native/pull/9942),
or Appium [supports test tags](https://discuss.appium.io/t/react-native-ui-element-access-via-testid/7845/5),
the only option here is accessibility labels.

In your React Native code, you can add the `accessibilityLabel` to any `View`
element. *Note: these are ONLY for View elements, you will likely end up wrapping
other elements in a View often.*

```xml
<View accessibilityLabel="Sign in here">
  <Link
    onPress={this.selectOnboardingType.bind(this, 'signin')}
    style={styles.link}
    id="signin"
  >
    {I18n.t('signInTitle')}
  </Link>
</View>
```

**Note: Accessibility labels ARE user visible; they are used for disabled users.
So you need to make sure they are human readable and make sense.**

Then, in your test, you can use the

```python
link_view = driver.find_elements_by_accessibility_id("Sign in here")
```

You can typically interact with the `View` object directly for clicking and
sending user input. If that doesn't work, you can use `link_view.find_element_by_xpath`
method to query for it's children.

See [Appium documentation](http://appium.io/slate/en/0.18.x/?javascript#ios-only) for
other selector strategies. [More](http://appium.wikia.com/wiki/Finding_Elements).


## Waits

I've found that a useful pattern for avoiding sleeps is to instead wait for an
expected element to show up on the screen. You define a total time you're willing
to wait, such as 10 seconds, and a short time interval that you want to poll the
screen on, say 200 milliseconds.

This is exactly what Selenium's `WebDriverWait` module does. But I'm going to
show you a basic implementation.


## Putting it all together

Ideally, I want my actual test cases to be very concise. Here is an example:

```python
def test_login(self):
    self.wait_until('Please enter your login', partial=True)
    self.get('#Email').send_keys('foo@example.com\n')
    self.get('#Password').send_keys('Password1')
    self.driver.hide_keyboard()
    self.get('Sign in', index=1).click()
    self.wait_until('Welcome')
    # self.dump_page()  <-- write the DOM to a file
    # self.repl()  <-- get an interactive shell
```

Here is an example test base class with those helpers:

```python
class AppiumTest(unittest.TestCase):

    def setUp(self):
        self.driver = webdriver.Remote(
            command_executor='http://127.0.0.1:4723/wd/hub',
            desired_capabilities=OPTIONS)

    def tearDown(self):
        self.driver.quit()

    def repl(self):
        import pdb; pdb.set_trace()

      def dump_page(self):
          with open('appium_page.xml', 'w') as f:
              raw = self.driver.page_source
              if not raw:
                  return
              source = xml.dom.minidom.parseString(raw.encode('utf8'))
              f.write(source.toprettyxml())

    def _get(self, text, index=None, partial=False):
        ''' get RIGHT NOW, fail if it's not there '''
        selector = options['xpath_selector']
        if text.startswith('#'):
            elements = self.driver.find_elements_by_accessibility_id(text[1:])
        elif partial:
            elements = self.driver.find_elements_by_xpath('//*[contains(@%s, "%s")]' % (selector, text))
        else:
            elements = self.driver.find_elements_by_xpath('//*[@%s="%s"]' % (selector, text))
        if not elements:
            raise Exception()
        if index:
            return elements[index]
        if index is None and len(elements) > 1:
            raise IndexError('More that one element found for %r' % text)
        return elements[0]

    def get(self, text, *args, **kwargs):
        ''' try to get for X seconds; paper over loading waits/sleeps '''
        timeout_seconds = kwargs.get('timeout_seconds', 10)
        start = time.time()
        while time.time() - start < timeout_seconds:
            try:
                return self._get(text, *args, **kwargs)
            except IndexError:
                raise
            except:
                pass
            self.wait(.2)
        raise Exception('Could not find text %r after %r seconds' % (
            text, timeout_seconds))

    def wait_until(self, *args, **kwargs):
        # only care if there is at least one match
        return self.get(*args, index=0, **kwargs)
```
---
title: Long Term Software Estimates
tags: manager newboss
toc: true
---

> False scheduling to match a patron's desired date is more common in our discipline than elsewhere in engineering because it is VERY DIFFICULT to make a vigorous, plausible, job-risking defense of an estimate that is derived by no quantitative method, supported by little data, and brought about by the hunches of developers.
- Mythical Man Month

Estimating a timeline for a project that will take many months and dozens of
engineers to complete has a very high failure rate - defined as the rate of
projects that take longer than the estimate to complete.

## The Myth of Software Project Planning

The biggest myth of software engineering is that we can estimate with any
accuracy. Junior engineers often learn this the hard way; coming up with estimates
that are best case scenarios, and working extra hours when the inevitable complications
arise. Seasoned engineers learn to pad their estimates (by a lot) to account
for the risk of going over. They learn that estimates are not reliable.

So why do we keep relying on estimates that the estimators themselves know are
unreliable? Because there is a legitimate business need to know. But we should
not confuse the need for certainty with the ability to be certain.

## Estimates Change as Scope Changes

![Dilbert](https://s-media-cache-ak0.pinimg.com/736x/7f/cc/b0/7fccb03a99c29ca90290709cf08afc7c.jpg)

There is often a great amount of accepted uncertainly around what will be built.
A successful business realizes that they need to keep requirements loosely defined
so that they can respond to new data as it comes up. Of course, if the scope
of a project is not high confidence, the estimate cannot be high confidence, either.

## Agile - Enter the Ordered Backlog

> No battle plan survives contact with the enemy.
- [Helmuth von Moltke](http://www.lexician.com/lexblog/2010/11/no-battle-plan-survives-contact-with-the-enemy/)

There are two main problems that need to be solved with making project estimates:

1. Humans are really bad at estimating.
2. We know that the scope of the project will change.

I'm not someone who believes that process can solve everything, but
[Agile](http://chase-seibert.github.io/blog/2013/07/19/agile-motivations-and-objections.html)
 does address these problems pretty well. The practice of using [relative pointing](http://chase-seibert.github.io/blog/2016/05/13/agile-points-vs-hours.html)
for estimates helps greatly with accounting for uncertainty and human error. Using
an ordered backlog chops uncertainty into bite sized chunks and using a well
established velocity keeps the error bars on longer term estimates manageable.

The most important thing is to ship it early and iterate.
---
title: Getting Started with AWS Device Farm and Python Appium
tags:
---

[AWS Device Farm](https://aws.amazon.com/device-farm) is a service for running
mobile app integration tests against a suite of physical devices. The
Amazon [documentation](http://boto3.readthedocs.io/en/latest/reference/services/devicefarm.html)
is exhaustive, and they support many different API clients.
However, the documentation does not have a quick start example for thing most
people are going to want to do first: run a test. Here is some working code
using the Python `boto3` client.


# Setup

First, you will want to create a Python virtual environment for this and install
some dependencies. Feel free to omit the versions numbers, but I've tested the
script against these versions.

```bash
virtualenv venv
. venv/bin/activate
pip install boto3==1.4.4 requests==2.13.0
```


# Pre-requisites

This script assumes you have already built an APK file for your mobile
app, and that you have bundled your Appium Python tests in a zip file as
per the [Amazon Appium Python documentation](http://docs.aws.amazon.com/devicefarm/latest/developerguide/test-types-android-appium-python.html).

Need to write your first tests? I have a [Appium Python Quickstart](http://chase-seibert.github.io/blog/2017/01/06/appium-react-native-quickstart.html),
too.

*Note: for the bundled Appium tests, make sure to do the wheel step on a Linux
 x86_64 vm.*

Also, you need to be authenticated to AWS. The easiest method is to generate an
API key and secret for your user in the AWS web console, and run `aws configure`
via the AWS [Command line interface](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html).


# Running the Test Suite

The script bellow will do the following:

1. Find the ID your AWS device farm project by name.
2. Find the ID of the pool of devices you want to use to test by name.
3. Upload a local Android APK file to AWS.
4. Upload a local zip file of your tests, requirements.txt and wheelhouse.
5. Start a test run.
6. Poll until the test run is completed.

```python
#!/usr/bin/env python
import logging
import pprint
import time

import boto3
import requests


REGION = 'us-west-2'
PROJECT_NAME = 'My Mobile App'
DEVICE_POOL_NAME = 'Top Devices'  # this is a default pool
RUN_TIMEOUT_SECONDS = 60 * 20
WEB_URL_TEMPLATE = 'https://us-west-2.console.aws.amazon.com/devicefarm/home#/projects/%s/runs/%s'


device_farm = boto3.client('devicefarm', region_name=REGION)
s3 = boto3.client('s3', region_name=REGION)
logger = logging.getLogger(__name__)


def get_project_arn(name):
    for project in device_farm.list_projects()['projects']:
        if project['name'] == name:
            return project['arn']
    raise KeyError('Could not find project %r' % name)


def get_device_pool(project_arn, name):
    for device_pool in device_farm.list_device_pools(arn=project_arn)['devicePools']:
        if device_pool['name'] == name:
            return device_pool['arn']
    raise KeyError('Could not find device pool %r' % name)


def _upload_presigned_url(url, file_path):
    with open(file_path) as fp:
        data = fp.read()
        result = requests.put(url, data=data, headers={'content-type': 'application/octet-stream'})
        assert result.status_code == 200


def create_upload(project_arn, upload_type, name, file_path):
    # name needs to be a file name like app-releaseProduction.apk, not "Android App"
    logger.info('Uploading %s %r' % (upload_type, file_path))
    result = device_farm.create_upload(
        projectArn=project_arn,
        name=name,
        type=upload_type,
        contentType='application/octet-stream',
    )
    upload = result['upload']
    _upload_presigned_url(upload['url'], file_path)
    return upload['arn']


def schedule_run(project_arn, name, device_pool_arn, app_arn, test_package_arn):
    logger.info('Scheduling test run %r' % name)
    result = device_farm.schedule_run(
        projectArn=project_arn,
        appArn=app_arn,
        devicePoolArn=device_pool_arn,
        name=name,
        test={
            'type': 'APPIUM_PYTHON',
            'testPackageArn': test_package_arn,
        }
    )
    run = result['run']
    return run['arn']


def _poll_until(method, arn, get_status_callable, success_statuses, timeout_seconds=10):
    check_every_seconds = 10 if timeout_seconds == RUN_TIMEOUT_SECONDS else 1
    start = time.time()
    while True:
        result = method(arn=arn)
        current_status = get_status_callable(result)
        if current_status in success_statuses:
            return result
        logger.info('Waiting for %r status %r to be in %r' % (arn, current_status, success_statuses))
        now = time.time()
        if now - start > timeout_seconds:
            raise StopIteration('Time out waiting for %r to be done' % arn)
        time.sleep(check_every_seconds)


def wait_for_upload(arn):
    return _poll_until(
        device_farm.get_upload,
        arn,
        get_status_callable=lambda x: x['upload']['status'],
        success_statuses=('SUCCEEDED', ),
    )


def wait_for_run(test_package_arn):
    result = _poll_until(
        device_farm.get_run,
        test_package_arn,
        get_status_callable=lambda x: x['run']['status'],
        success_statuses=('COMPLETED', ),
        timeout_seconds=RUN_TIMEOUT_SECONDS,
    )
    final_run = result['run']
    logger.info('Final run counts: %(counters)s' % final_run)
    return final_run['result'] == 'PASSED'


def get_run_web_url(project_arn, test_run_arn):
    # project_arn = arn:aws:devicefarm:us-west-2:foo:project:NEW-ARN-HERE
    # test_run_arn = arn:aws:devicefarm:us-west-2:foo:run:project-arn/NEW-ARN-HERE
    project_arn_id = project_arn.split(':')[6]
    test_run_arid = test_run_arn.split('/')[1]
    return WEB_URL_TEMPLATE % (
        project_arn_id,
        test_run_arid,
    )


if __name__ == '__main__':
    logging.basicConfig(format='%(message)s')
    logger.setLevel(logging.INFO)
    project_arn = get_project_arn(PROJECT_NAME)
    logger.info('Project: %r' % project_arn)
    device_pool_arn = get_device_pool(project_arn, DEVICE_POOL_NAME)
    logger.info('Device pool: %r' % device_pool_arn)
    app_arn = create_upload(
        project_arn,
        'ANDROID_APP',
        'my-app.apk',
        '/tmp/my-app.apk',
    )
    wait_for_upload(app_arn)
    logger.info('App: %s' % app_arn)
    test_package_arn = create_upload(
        project_arn,
        'APPIUM_PYTHON_TEST_PACKAGE',
        'test_bundle.zip',
        '/tmp/test_bundle.zip',
    )
    wait_for_upload(test_package_arn)
    logger.info('Test package: %s' % test_package_arn)
    test_run_arn = schedule_run(
        project_arn,
        name='Test Run 1',
        device_pool_arn=device_pool_arn,
        app_arn=app_arn,
        test_package_arn=test_package_arn,
    )
    logger.info('Scheduled test run %r' % test_run_arn)
    logger.info('View scheduled run at %s' % get_run_web_url(project_arn, test_run_arn))
    success = wait_for_run(test_run_arn)
    logger.info('Success')
```

The only tricky parts here are using the pre-signed URLs from the `create_upload`
step to make a subsequent `PUT` call with the file contents, and setting the
`ContentType` correctly for that to work. Also, we need to poll for both uploads
to be processed before we kick off the test run.
---
title: Best Practices for Meetings
tags: meetings manager newboss
toc: true
---

Every team meeting needs a primary owner. If you are the owner of the meeting,
it's your responsibility to make sure that this is a great use of team time.

*This only applies to team meetings with 3+ people, not 1:1s.*

## Before the Meeting

### Make attendance optional

Not everyone needs to go to every meeting. If you're using Google Calendar,
when you invite a team, expand the group into individuals and click on the
person icon to the left of a name to mark that attendee optional.

![optional attendees](/blog/images/meetings_optional.png)

### Write an agenda

Any meeting with more than a couple of people should have a written agenda on
the calendar item itself. Have some standard meeting agendas that
the team agreed on. Part of the agenda is how much time you want to allocate
to each topic. The entire allotted time for agenda items should be at least
5 minutes less than the block of time for the meeting. The written agenda
should be on the meeting at least the day before.

*It's perfectly reasonable to decline attending a meeting if you've asked for
an agenda, and there is still not a written agenda.*

### Share out pre-Reading

As part of the agenda, share out and reading materials ahead of time.
You should not spend a lot of time during a meeting reading slides; the meeting
should be for getting feedback and discussion of the content.

### Setup remote access

If you use Zoom.us for remote meeting access, meeting owners are responsible
for setting up Zoom for all meetings. You never know when someone is going to
be remote that day. You can use the [Zoom Chrome plugin](https://chrome.google.com/webstore/detail/zoom-scheduler/kgjfgplpablkjnlkjmjdecgdpfankdle?hl=en)
to quickly create Zoom info. This will allow attendees to dial in before the
meeting even starts.

![zoom plugin](/blog/images/meetings_zoom.png)

## During the Meeting

### Get there early

Meeting owners should get to the room a few minutes ahead of time to make
sure that Zoom and screen sharing it setup and working. You should not be
making attendees wait around for this to happen. This will sometimes
necessitate that meeting owners leave a previous meeting 5 minutes early. If
you own a meeting, that's part of your responsibility.

### Start on time

Start the meeting on time. Don't wait for stragglers, get right into it. If
people show up late, it's their responsibility to try to catch up.

### Take notes

Either the meeting owner or someone they delegate to should be taking notes
for the meeting. These can be on a wiki, a Google doc, or directly in an email.

### Timebox discussions

It's the meeting owner's responsibility to keep the meeting on track. If the
amount of time you have allocated for a specific discussion is going over, you
may have to cut it off.

- Keep discussions on topic
- Feel free to put items "in the parking lot" to discuss later/offline
- Move on to the next agenda item when time is up

### Follow the agenda

Don't get side tracked by discussion that was not on the agenda.
Alternatively, you can edit the agenda in real time if you believe the new
discussion is more valuable than an existing agenda item. But be conscious
about which agenda items are being removed, and call that out in real time to the team.

### Ask for feedback

Before the meeting ends, take 2 minutes to do a +/-/Delta exercise with the
group.

- Write +, - and Delta on the white board as categories
- Ask the group what they liked about the meeting and put it under the + column
- Ask the group what they did not like about the meeting and put it under the - column
- Ask the group what they would change about the meeting and put it under the Delta column

### End on time

End meetings promptly. Give attendees 5 minutes to get to their next meeting.

## After the Meeting

### Send action items

Send both the meeting notes and the action items out to the attendees as an
email. Action items have a specific person assigned to them, and detail the
next action to take to make progress on that issue.
---
title: Minimizing Impact of Interruptions on Engineers
tags: meetings manager
toc: true
---

Any software engineer knows the feeling of being "in the flow", or
"in the zone". It's when you get a large block of uninterrupted time to just
code. These periods are rare, super productive and morale boosting.

## Importance of Flow

> During single-minded work time, people are ideally in a state that psychologists call flow. It is a condition of deep, nearly meditative involvement, a gentle sense of euphoria when one is largely unaware of the passage of time. For anyone involved in engineering, design, development, writing or similar tasks, flow is a must. These are high-momentum tasks that only go well when you’re in flow. Unfortunately, it can’t be turned on like a switch, it takes a slow descent into the subject, 15 minutes of more of concentration before the state is locked in. Each time you’re interrupted, you require an additional immersion period to get back into flow. [Peopleware](https://www.amazon.com/Peopleware-Productive-Projects-Teams-Second/dp/0932633439)

Flow is critical to creative endeavors like coding. Our goal as engineers and
managers should be to maximize this across our teams. Because interruptions can
disrupt flow, they must be minimized. Beyond that, what should our goal be
for flow time?


## Maximize Large Blocks of Flow Time

The current state of the industry is that a programmer is likely to get just
one two hour session of flow per day [[Programmer Interrupted](http://blog.ninlabs.com/2013/01/programmer-interrupted/)]. That
should be our absolute floor. [Paul Graham](http://www.paulgraham.com/makersschedule.html) advocates for half day blocks (4 hours). For context, [air traffic controllers
are required to take a break every two hours](https://www.quora.com/What-is-a-typical-shift-length-for-an-air-traffic-controller).

In the absence of any research on the maximum duration a software engineer can
be in the flow, let's say that our goal should be to maximize the number of
two to four hour blocks of flow per week.

For a 40 hour week, we could theoretically schedule 10 four hour flow sessions
or 20 two hour flow sessions. That's assuming 100% capacity. Based on experience,
Scrum best practice is to assume that engineers get about 5 "effective hours"
 or "net work hours" a day. See: [Determining How Many Task Hours an Agile Team Can Accomplish](https://www.leadingagile.com/2011/05/determining-how-many-task-hours-an-agile-team-can-accomplish/).

That aligns with surveys which show that [50% of engineering time](http://www.infoworld.com/article/2613762/application-development/software-engineers-spend-lots-of-time-not-building-software.html) is taken by non-coding tasks.

![img](http://images.techhive.com/images/idge/imported/article/ifw/2013/04/08/elasticcloud_swengineer_hours-100422926-orig.png)

*Note: taken together, this means that an engineer is on average getting about
20 hours of coding time, but only 10 of those hours are "in the flow".*


## Strategies to Minimize Interruptions

These are mostly common knowledge.

- Put on headphones
- Don't answer email/Slack
- Hold "office hours" to batch up Interruptions
- Work from home
- Go sit somewhere by yourself in the office

Notice that these are entirely on the individual engineer to manage. What
can we do as managers to help?


## Strategies to Maximize Flow Blocks

As managers, the most effective way to maximize the number of blocks of flow
our teams get is to clear the calendar. In this context, what types of calendar
items constitute an interruption? Basically, anything. Regular meetings, 1:1s,
interviews are all obvious interruptions.

Here are some common top down strategies to minimize interruptions. Again, our
baseline is the industry average 2 flow hours a day, or 10 flow hours a week,
and our goal is 20 flow hours per week.

Also, remember we are assuming a **maximum of five effective hours per day.**

### No Meeting Days

The most popular strategy is to have an entire day per week with no meetings,
where everyone can be heads down. You might combine that with giving people
permission to not answer email/Slack, or work from home.

The great advantage of this tactic is that it's easy to communication and
understand. It's also relatively easy to get buy-in to delay a meeting by at
most one day.

In the best case scenario, this translates to a 1 four hour block
per week and 4 two hour blocks a week, for a total of 12 flow hours.

### Consolidating Meetings to One or Two days

If you hold all meetings on one or two days a week, and leave the entire rest
of the calendar free, you would have 3 or 4 four hour blocks per week. Assume
that the one or two days you actually have meetings are complete write-offs.
That results in a total of 12 to 15 flow hours.

### No Meeting Mornings/Afternoons

You could block off 3-4 hours every day for interruption free working
time. This could take the form of never scheduling meetings before lunch,
or between 1pm and 5pm, etc. In practice, it would be important to align
this with times people are actually in the office, if you allow flexible work
hours.

This would result in 5 three or four hour blocks a week. Any non-free time
can be assumed to involve interruptions. Let's estimate 15 to 20 flow hours.

## Conclusion

Given that engineers are going to get at most five effective hours a day, the
ideal set up from a flow perspective is to get a 3-4 hour block of time blocked off
every single day with no interruptions. Interestingly, this is 80% as effective
as never having any interruptions at all - the equivalent of a 100% meeting
free week. It's also the same 20 hours of coding that engineers are typically
getting, but a 100% increase in the number of coding hours "in the flow".
---
title: React Native Six Months In
tags:
---

NerdWallet released our first cross platform mobile app at the start of 2017,
about six months ago. We choose to use React Native, and we are still using
99% of the same code base for the iOS and Android apps. What have we learned
so far?

# Initial reasoning

We initially chose React Native because it was the only way to hit our goals. We
didn't have any native mobile engineers, and we had been asked to deliver an
iOS and Android app from scratch in just five weeks. We also happened to have
very solid React experience on the team. Additionally, the app overlapped very
nicely with some features we already had on the web, and we thought we could
get some code re-use there.

# Code Reuse Deep Dive

In our case, we started with an intuition that 50% of the mobile app codebase
could actually be NPM modules re-used from our web stack. Plus, we got the
obvious savings of 100% code re-use between the iOS and Android app code.

After 6 months, we can look at our actual code re-use to get a better sense of
where we actually ended up. Our current app has 26,000 lines of JavaScript. The
NPM modules that we include from our web app stack total 40,000 lines of
JavaScript. In practice, the maintenance of that shared code is virtually 100%
offloaded to other teams.

![react](https://docs.google.com/drawings/d/1-pqAgRwz9gh66290u3yJQ5Jfevn3hBEqzBFtORpQ9nE/pub?w=960&h=720)

Here is what it might look like if we had two separate native app code bases.

![native](https://docs.google.com/drawings/d/1QWus2w-VxQRpIbR_y51DKukeGXIgYKGObWY5eJuQ8Pk/pub?w=480&h=360)

If you think of an app codebase as 2 unit of library code and 1 unit UX code,
our app is about 1.2 units of code that we actually maintain. Writing two
native apps would be about 6 units. The most interesting learning for me was
just how much logic in an app is not related directly to the UI, as well as
how easy it is to re-purpose NPM modules targeted at web.

*Note: if you're interested in reading more about leveraging NPM modules in
React Native, see [@parshap's slides](https://t.co/tM8CSoZvlk).*

# Limitations

We have run into a number of areas where we are making compromises on the user
experience. It's not totally clear yet which of these are legitimate things that
React Native is not good at, and which are due to us as a team not having
focused on them.

## Performance

We have seen persistent issues with app performance, particularly around the
time to switch between tabs in the navigation. That issue seems to be due to a
bug in our navigation library, `react-native-router-flux`. But it's important
to call out that performance is largely dependent on getting React basics right.
Performance has been good enough that we have not dedicated time to look
into it further. In general, we still don't think there is anything that most
apps need to do for which React Native will not perform well, except maybe
custom animations.

## Animations

Basic stock animations like screen transitions are available and perform well
out of the box. For custom animations such as fading out a header as the user
scrolls down on a list view, you need to write the animation yourself. We
initially tried the naive solution where we control style properties from
JavaScript. That did not perform well. But it was relatively easy to use the
actual bridged animation primitives, at least for simple animations. We still
don't have a good sense of how difficult complex custom animations will be.

## Fonts

Initially we did not use the correct default fonts for each platform. It turned
out to be relatively simple to use either San Francisco or Roboto based on the
platform. Some font variants that are normally available are not included. This
ended up being something that probably took more time to get right than a native
app would, but if you pay attention to it the first time it should be smooth.

# Biggest risks

The largest risk with React Native is still the immaturity of the platform.
Releases come every two weeks with potentially breaking changes. But the larger
risk is more existential; even though it's still building in popularity, there
is a chance that React Native fades away in the coming years as the framework
ecosystem [continues to evolve](https://trends.google.com/trends/explore?q=react%20native,phonegap,cordova,Xamarin).

![trends](https://dl.dropboxusercontent.com/spa/sffu0th1cc1sg9q/ellfigls.png)

The second largest risk I see is a little more insidious. As we have ramped up
the team with engineers who have prior mobile experience, they have begun to
point out areas where we are not following best UX practices. Some of these are
intentional. It's very tempting once you have a cross platform app to make UX
decisions to optimize for code re-use, not the best user experience. Even more
troubling is the times when the engineers themselves don't realize that a
particular design is not following best practice UX; typically due to many of
them not having mobile experience.

# Question marks

In my mind, the biggest open question is around how these mobile native
platforms continue to evolve. Both Android and iOS have their developer
conferences this time of year. Neither are expected to announce anything ground
breaking in terms of how we build apps. This is a natural trend as the platforms
mature; there is less interesting stuff to do. This happened to the PC operating
system platforms over the last 30 years. 10 years into mobile, are we starting
to see the same? This could be the time when putting an abstraction layer on top
starts to make more sense, and actually becomes a winning strategy.
---
title: Trading off Value, Quality and Time
tags: manager newboss product-management
toc: true
---

The traditional [Iron Triangle](https://en.wikipedia.org/wiki/Project_management_triangle)
tries to explain in graphical form how software projects need to make hard
tradeoffs between scope, schedule and resources.

![img](/blog/images/ironTriangle.jpg)

This is alternatively referred to as the Time-Cost-Quality Triangle,
Triple Constraints, the Triangle of Balance, or the Iron Triangle.

## Many Triangles

There are many variants. A common variant is phrase "Fast, Cheap or Good. Pick two."

![img](/blog/images/fast-cheap-good.png)

A more nuanced version allows illustrates the nature of the tradeoffs that
you're making, an allows for a middle option where all three are in balance, but
you're not really optimizing any of them.

![img](/blog/images/Project_Management_Triangle.jpg)

Software engineers is all about managing tradeoffs. The highest level
tradeoff is during planning and prioritization in the form of trading off
value delivered, the quality level of that value and the time to deliver it.

## The Agile Triangle

>"If you're a team that practices waterfall development or new to agile development, the important thing to remember is the difference between what is fixed and what is estimated. Unlike waterfall development, agile projects have a fixed schedule and resources while the scope varies."
[https://www.atlassian.com/agile/agile-iron-triangle](https://www.atlassian.com/agile/agile-iron-triangle)

Agile methodology in particular uses fixed time periods and fixed resources. This
is typical of a start up environment, where you need to ship quickly and do not
have the resources to hire many engineers. You're also typically building an
MVP type product, trading off scope versus time. Scope can be further broken
down into feature breadth and depth, and the quality of the whole experience.

![img](/blog/images/agiletriangle.jpg)


## My Agile MVP Triangle

To simplify, my version of the Agile MVP triangle looks like this.

![img](/blog/images/Value-Quality-Time.png)

You can pick exactly one spot on this triangle for a given project. If you
choose a spot close to the value point of the triangle, you are explicitly
giving up some focus on a quality, polished experience. You're also choosing to
push out time to deliver somewhat to get more value in.

This is joking talked about as "Fast, Cheap or Good. Pick two. *No, not that one*".
Alternatively describes as "Nine women cannot make a baby in one month"
([Mythical Man Month](https://en.wikipedia.org/wiki/The_Mythical_Man-Month)),
meaning that even if you did add resources in the form of extra engineers,
you can quickly get to a point of diminishing returns where adding people
doesn't actually speed up the delivery.

What do these points mean, exactly?

### Value

This is basically scope. It could also be labeled "Features", both in terms of
breadth of different features, and the depth/scope of an individual feature. I
thought about calling this "User Value", but quality could also be considered to
deliver user value. Likewise, scope could include quality/polish work. In the
end, this isn't a perfect term, but I basically mean user value excluding
quality/polish.

### Quality

Some examples of quality are high fidelity graphic style, UX optimizations
based on feedback and performance tuning. This is often subjective, and can
take a virtually unlimited amount of time as you polish on the far end of the
diminishing returns curve. The trick is getting to 80% of max quality with 20%
of the effort.

### Time

This is simply the time to ship - to put the software in front of real users.
---
title: Estimating Epic Stories in Three Steps
tags: manager product-management
toc: true
---

In Agile, an Epic story is a potentially large placeholder story in the backlog.
Typically they will be broken down into manageable stories during grooming
before they are worked on. But if you need to estimate an Epic before breaking
it down, how might you do that?


## Use Story Points

In [Defense of Large Numbers](https://www.mountaingoatsoftware.com/blog/in-defense-of-large-numbers),
Mike Cohn talks about how removing the larger values from your estimation
toolkit is like deciding to strike "millions" and "billions" from our vocabulary
just because our bank balances are only in the thousands.


### #1 Show Historical Data

Pull up a list of recently completed Epics, along with their final point value.
For example, if you recently finished a "User Registration" Epic, tally up all
the points from all the stories in that Epic.

You might end up with something like this:

- Password Reset, 15 points
- User Registration, 35 points
- Admin Interface, 70 points

### #2 Discussion of Mocks

The next step is to view the mocks as a team. Yes, this means to need mocks.
Go over every screen as a team.

- Write down a high level set of features.
- Write down how many pages/screens are involved.
- Write down any complexity you see.
- Think about any dependencies you may have.

For Epic estimation, I like to cheat a little and ask the team to agree whether
this story is the same size as another Epic on the board, or whether it's
between two of the epics. That will ground planning poker.

### #3 Relative Estimation

You should be ready to play planning poker. Ask the team if they are ready to
estimate. Tell them that we are going to continue to use the Fibonacci sequence.
As a refresher, here are the larger Fibonacci numbers:

`13, 21, 34, 55, 89, 144`

Assign a number of fingers to each number. For example:

`13 (1), 21 (2), 34 (3), 55 (4), 89 (5), 144 (6)`

Now, it's time to vote. On the count of three, every one holds up a number of
fingers simultaneously. You would be surprised how often everyone in the room
holds up the same number of fingers at this point. Congratulations; you have
an Epic estimate.


## Bonus: Milestones

Remember that high level set of features you wrote down during the discussion of
the mocks? You have already done 90% of the work for milestone definition, might
as well take it over the finish line to de-risk your project. Reminder: the goal
of milestones is to deliver (usually three) potentially shippable product
increments along the way to a finished product.

Just write down labels for milestone 1, 2 and 3 on the board. Start with
the most critical features, and put them in the early milestones. You will
likely run out of room and have to bump features to later milestones. That's
fine! The goal is to keep the product potentially shippable; be ruthless in
prioritizing only features that would block a user from getting the primary
user value.


## What about T-Shirt Sizing?

On common method is to assign T-Shirt sizes to stories, such as Extra Small,
Small, Large, Extra Large, etc. This method has the benefit of being in a
non-numerical unit. It's very hard to hold a team accountable to a deadline
around an estimate of "Large".

This method also lends itself to relational estimation, versus absolute
estimation. But, you have to hold to that bar yourself. For example, you could
identify one medium sized story in the batch and randomly call it a "Medium".
Then, you can relatively estimate from there. If you do T-Shirt sizing a lot,
you should present the team with some of their previous complete stories, along
with their original T-Shirt sizes, to ground the discussion.

However, what if you also need to estimate how long the Epic might take in terms
of time? You might be tempted to equate T-Shirt sizes directly with a number of
weeks or sprints. But that's the same thing as equating story points to hours,
which is a huge no-no.
---
title: Saying the Same Thing Multiple Times
tags: manager product-management
toc: true
---

To communicate something to an organization of people, you need to repeat
yourself. The more people involved, the more you will need to repeat yourself
if you want everyone to really hear it.

Why? Not everyone will see the communication. Even if they
see it, it may not register with them. Even if it registers, they may
not buy into it. In general, the more you repeat yourself, the more likely
people are to hear it, internalize it, and buy into it.

> <br>
The 1st time people look at ad, they don’t see it.<br>
The 2nd time, they don’t notice it.<br>
he 3rd time, they are aware that it is there.<br>
The 4th time, they have a fleeting sense that they’ve seen it before.<br>
The 5th time, they actually read the ad.<br>
The 6th time, they thumb their nose at it.<br>
The 7th time, they get a little irritated with it.<br>
The 8th time, they think, “Here’s that confounded ad again.”<br>
The 9th time, they wonder if they’re missing out on something.<br>
The 10th time, they ask their friends or neighbors if they’ve tried it.<br>
The 11th time, they wonder how the company is paying for all these ads.<br>
The 12th time, they start to think that it must be a good product.<br>
The 13th time, they start to feel the product has value.<br>
The 14th time, they start to feel like they’ve wanted a product like this.<br>
The 15th time, they start to yearn for it because they can’t afford to buy it.<br>
The 16th time, they accept the fact that they will buy it sometime in the future.<br>
The 17th time, they make a commitment to buy the product.<br>
The 18th time, they curse their poverty because they can’t buy this terrific product.<br>
The 19th time, they count their money very carefully.<br>
The 20th time prospects see the ad, they buy what it is offering.<br>
  - [Thomas Smith, 1885, Successful Advertising](https://www.amazon.com/Successful-Advertising-Secrets-Explained-AnnuaL/dp/B00HERMNYS)

## Seven Corporate Communication Methods

The seven most obvious mechanisms for communicating information inside an
organization are:

1. Slack
2. Email
3. Wiki
4. One on ones
5. Small team meeting
6. Large cross team meeting
7. All hands meeting

Notice that these methods start out targeting specific groups surgically,
and get more general. That's intentional; you want the most targeted audience
to hear it the most often. Side benefit: by the time you are communicating in
a larger group, you have a good chance of having the most effected sub-groups
already nodding their heads.

For anything important that many people need to hear about, you should plan on
going all the way up to #6 or #7 *and all the steps before that*.

### Minor Communication Methods

In addition, you can also communicate via one of the lesser methods:

- Written status updates
- Personal blog

## Communication Anti-Patterns

Don't do these.

- Email a bunch of people without any clear calls to action.
- Write a book in an email.
- Write multiple paragraphs without headings, lists or other structure conducive to
skimming.

## Advanced Communication Tips

- Include a `TL;DR` section at the top of your email for the 75% of people who
will not read the entire thing.
- Make the email as short as possible to increase that percentage. Then make it
even shorter.
- Include a link to a wiki page or doc with more info.
- Include a FAQ in the wiki page or doc.
- Include a table of contents on that wiki or doc if it gets long enough.
- Send a pre-read for any meeting communication.
- On a wiki, assign TODO items right inside the wiki to ask people to certify
that they read it.
- Set a reminder to communication the same thing again in the future.
---
title: Two (more) Weeks in QA
tags: qa
---

Due to our regular QA engineer being out on vacation, I once again has the
opportunity to do a tour of duty in QA. This time, for a big mobile release.
I previously [wrote about a similar experience](http://chase-seibert.github.io/blog/2009/03/20/my-two-weeks-in-qa.html)
 eight (!) years ago, so I thought I would once again write down some reflections.

# Web vs Mobile

Last time, I was testing a web product. This time, it was a mobile app. That
turned out to make a **big difference**. For one thing, I spent a lot more time
manually testing on various physical phones. Our app is on both Android and iOS,
so that's at least two devices for every test. But you also have to test on
older slower phones, phones on older versions of the operating system, etc. Plus
it's all via touch interfaces. It ends up being more time than testing web
features for a few browsers.

# More Documentation Time

This time I spent a lot more time updating test plans. Because we have to test
on so many phones, we have an outsourced QA team for even more manual testing
horsepower. While I may test on five or six phones, I can send a test plan over
to a remote team to run it on fifteen to twenty phones.

The existing documentation turned out to be very useful. Not only did it remind
me about a bunch of test cases I would have forgotten about, but it had all kinds
of hints about the most efficient way to test a specific case.

# Communication

Because it was a big release, I spent a bunch of time coming up with daily
reports of whether we were on track for launch. At a high level, I wanted to
communicate how many bugs we had, new bugs we had discovered that day, any
blockers for release and what step on our release schedule we were on.

But I also made some fun graphs!

![img](/blog/images/qa_time.png)
![img](/blog/images/qa_type.png)

# Triage & Troubleshooting

This time, I got more involved in triaging new bugs. Not just reproducing them
and writing a good bug report, but also setting priority. This turned out to
often involve discussions with both the product owner and the engineers. On more
than a few bugs, we decided that while the severity of the bug was low to medium,
we actually did not want to fix it now because we don't have a good solution that
doesn't involve a medium sized refactor.

Similarly, we ended up having a lot of conversations about how to be more
defensive in the code to prevent errors like the one in a particular bug report
more generically. Some of these were also not feasible/wise to take on
immediately. They went into the backlog.

# What Was the Same

Once again, I was surprised my how many blocking issues I ran into just trying
to get a build running well enough to test. We have most of the kinks worked
out of our build pipeline, so there were not any problems getting a given
build onto a phone. But I was blocked for days at a time on things like third
party API integration staging environments being down. In one case, it took
way too much time to discover that the breakage was related to a new network
configuration in the office; we had forgotten to whitelist some IPs.
---
title: Tracking Your Time
tags:
toc: true
---

I've been using a time tracking tool pretty consistently for the last year to
categorize all of my time spent at work. It started with wondering how much
time I was spending in meetings. Eventually I found that it was easier to keep
up with than I thought, so I just stuck with it. Looking back at the year, I
can see some interesting data.

# Toggl

The primary tool I've been using is [Toggl](https://toggl.com/). It's a simple app that sits in you status bar and prompts you to start a timer for what you're currently working on,
if there isn't one running already.

![img](/blog/images/toggl.png)

I eventually settled on five primary "projects" that my time is categorized under:
meetings, coding, 1:1s, writing and other. In the end, I did end up spending more
time in meetings than doing anything else.

![img](/blog/images/toggl_year.png)

## Other/GTD time

Other eventually became just "GTD" time, which is short for [getting things done](https://en.wikipedia.org/wiki/Getting_Things_Done).
It's usually the first block of time in the morning, right after I grab coffee.
It consists of processing all my todos and notes from the previous day. Some tasks
I just categorize and set a due date for. Other tasks I complete right then, if
they are quick. My time spent on this category over the course of the year was
pretty constant.

![img](/blog/images/toggl_other.png)

I have a whole [separate blog post](http://chase-seibert.github.io/blog/2015/05/01/omnifocus.html) on GTD, if you want to learn more.

## Writing time

The amount of time I spend writing wiki pages, blog posts and longer emails also
stayed pretty constant over the year. If I had broken this down into blog time
and everything else, I would imagine that blogging is at most two hours a month.

![img](/blog/images/toggl_writing.png)

## Coding time

Coding is the area that I expected to tail off dramatically, and I was not
disappointed. This year I went from managing a team of three and being very
hands on coding, to three teams with 12 total people. In the past, I've noticed
that right when you get to 5 or 6 people, you basically cannot write code
anymore.

![img](/blog/images/toggl_coding.png)

April was a hackathon project that I spent way to much extra time on. October
was a major release that was all hands on deck. I spent a bunch of time
doing QA type stuff that I categorized as coding. Other than that, it's been a
very steady trend downwards.

## Meetings and 1:1s

Meeting time was somewhat steady. I see a spike in March that was a bunch of
interviews, including a potential aquihire. There is another big spike in
September, which was getting a new boss, splitting up a team and also
a bunch of late project schedule coordination meetings.

![img](/blog/images/toggle_meetings.png)

For one on ones, there was a steady increase as we added new team members, and
then a big spike in August as we added a few new people all at once.

![img](/blog/images/toggl_ones.png)

# Timebox

Remember that hackathon project? I also wrote a Slack bot to categorize
meetings on my actual calendar, which is an interesting way of cross checking
my manual tracking. It's a little hard to parse, because the bot is designed
to report on one or two weeks at time. But you can see that while overall
meeting time has gone up, I've actually been pretty successful at maintaining
and even increasing the amount of free time I have in blocks - time to actually
work on something substantial without interruptions.

![img](/blog/images/timebox_2017.jpeg)

In fact, my time this year was 57% free time with 267 hours in blocks of at
least 3 hours. Doesn't sound too bad when you put it that way!

# RescueTime

Finally, there is another fully automated time tracker that I started using
more recently called [RescueTime](https://www.rescuetime.com). It can give you
a breakdown of time spent by app/website. I'm spending more time in Google
Spreadsheets than in my terminal app and my code editor combined, which is
definitely a strong signal.

![img](/blog/images/rescue.png)
---
title: Don't Split the Team Until You Have To
tags: manager
toc: true
---

If you have a team that is unable to work effectively, there could be any
number of causes. If you're thinking about splitting the team into two or
more teams as a way to solve the problem, my advice is to fix the root
cause of the problem instead.

## Splitting the Team

There are various reasons for wanting to split up a team.

### Team Too Big

The ideal Agile team size is 5-8 people. If you have more than that, you
are likely experiencing ineffective team communication, long stand-ups and
bottlenecks around the product owner and designer.

Splitting the team may actually put more strain on communication in the form
of cross team meetings and emails. Likewise, stand ups may be shorter, but
you may need to go to more than one. Splitting the team would also require an additional product owner the designer to relieve the bottleneck. Going without
either is not ideal. Neither is having someone splitting their time between
two teams.

A more direct solution would be to simple decrease the size of the team. Move
an engineer to another team.

### Too Many Products or Product Owners

If you have multiple products or product owners, you might experience frequent
shifting between projects. Engineers may feel that they are not able to focus
on a problem for long, before they are pulled onto something else. Overall
velocity may suffer.

You could split up the team along product or product owner lines.
Make sure not to end up with teams that are too small. If you
only have one or two engineers on a team, you will be subject to wild swings
in team velocity when one or both people go on vacation, or are sick. You are
likely going to struggle to provide healthy levels of code review and mentorship
from engineers who have enough context on the work. Finally, you lose the ability
to flex resources and swarm everyone on one task if the need arrises.

Another solution would be to reduce the number of products the team is working
on, and funnel the product backlog through a single product owner.

### Prioritize Technical Work

Product owners are naturally going to prioritize feature work. If the team is
not getting the time they need to invest in technical platform and refactoring,
you may be experiencing instability in production, and what should be small code
changes resulting in much more work than expected, or frequent regressions.

Splitting the team so that one group can focus on platform work will result in
getting more of that work done. But there are non-obvious downsides that go with
it. It will be difficult to keep the two teams in sync about what projects are
going on. Both teams may feel that they don't get to do as much of the product
or platform work as they would like. The two teams may start to drift in their
alignment on technical solutions.

A better solution in this case would be to come to an agreement to
reserve the time for platform work in the team's normal sprints. A common ratio
is 20% of team velocity.

### Empire Building

Maybe you have a technical lead who wants complete ownership of a certain
component. Maybe you have some who wants to to manage people, and you need a
team for them to manage. Or a manager who wants to manage multiple teams.
These are all terrible reasons to split up a team.

For technical projects, you need to find a way for people to have ownership
regardless of team makeup. For people interested in management, this is probably
a negative signal about whether they are a good fit for management. But just wait
a while, there will always be more organic opportunities for that.

## Other Stuff

Keep in mind that by splitting the team, you're creating a potential dependency
for future projects. Dependencies are productivity killers. Don't take that
too lightly.

## Final Word

You want to avoid splitting up a team until you absolutely have to. You should
prefer moving people to other teams versus splitting the team. Make sure you're
solving the root cause of your problems.
---
title: Teams Working at a Sustainable Pace
tags: manager product-management 
toc: true
---

How do you make sure that your team is not signing up for too much?

There is no substitute for a well functioning team that has built trust
over time. When the product owner and
the technical lead have been through war together, they can
right size almost any project and prioritize it correctly. On new teams, how much work to sign up for can be a source of conflict. In that case, it's the technical lead's job to make sure the work load is realistic.

If you're having a planning conversation with the product owner, how should you
think about turning a "No, we can't do this" to a "Yes, and this is what we
can deliver and when"?

## Break into Milestones

Working sustainably is more about how much scope you are delivering per unit of time than it is about which things you are doing. Something that is 1,000 total man hours of work is fine if you're only delivering the 20 hour version this sprint.

Pick a work item and break it down into pieces. Can you simplify one of the pieces?
Can you postpone one of them? Can you agree to hack a solution for one piece for
the MVP and fill it in afterwards? Is one requirement accounting for a
disproportiuanate amount of the effort? Propose dropping it and see what people
say.

Set milestones up front. See how quickly you
can get to a working version. An early milestone does not have to satisfy
all the criteria. But it has to be working and shippable. If the team's priorities
shift, you can have a discussion about whether the last milestone is a good place
to leave the project for a while.

## Back to Scrum Basics

There is a reason why Agile principles are so widespread. They work.

- Stack rank the work items. Focus your energy on trying to cut items that are
the lowest priority for the product owner.
- Have a stable velocity. If your team has been running for a while, you
hopefully have predictable velocity and accurate estimates. In that case, it's
simple math as to how many stories you can do. Remember, you can
[estimate epics](http://chase-seibert.github.io/blog/2017/08/28/epic-story-estimation.html), too.
- Break stories into smaller pieces. If you can show that one piece accounts for
a lot of the work, maybe you can agree to do that work later.
- Agree to commit to less, but pull in additional stories if there is more time.
Again, textbook Agile.

For all these items, the Scrum Master is responsible for making sure the team is
living up to their own process. They should be the one to say whether the team
can commit to more work on not. This is one good reason for the Scrum Master to
NOT be one of the engineers on the team (or the product owner). There is a
perceived [conflict of interest](https://www.mountaingoatsoftware.com/blog/protecting-the-team-cuts-both-ways).

## Tie Breakers

If you still cannot decide between a too long list of work items, you might need
to invoke these roadmap planning judo moves.

*Note: no actual product owners were harmed in the making of this list.*

### The Outside the Box

Deprioritize work if there are significant issues with the design.
If there is similarly high priority work that has more solid design, it may make
sense to give this item more time to bake.

### The Big Picture

Deprioritize work if there are better technical solutions that the team can
pursue when they have more capacity in the future.

### The Deflection

Deprioritize work if there is another group that makes more sense to work on this item.

### The Disappearing Engineer

Deprioritize work if not every person-day of engineering you're assuming actually exists.
Does someone have a vacation coming up? Is there a project that you might need
to pull people off for? Are you accounting for interrupt driven work?

### The Consolidator

Deprioritize work if you have more work items than engineers. Consider having an engineer
focus on one task at a time. In many cases it would be better to go deeper on
high priority items in the backlog versus doing more discrete items.

### The Missing Link

Deprioritize work if there a dependency which is not already in production
and battle tested. It's risky to assume that two interdependent work items will
both land simultaneously.
---
title: How to Give 360 Peer Feedback
tags: manager reviews
toc: true
---

Managers have a lot of practice giving people feedback. Most other engineers
are only asked to give formal peer feedback once or twice a year. These are
known as [360 reviews](https://en.wikipedia.org/wiki/360-degree_feedback). From
a management perspective, the free text feedback portion is the most important
part. But, often the content in that section could be much more valuable with a
little more context.

## Make Sure You Have Context

If someone nominates you to give them feedback, they think you have
something to add. If you've worked with them on anything substantial, you probably
do have pertinent feedback, even if you have to spend time thinking about it.

If you really do not have enough experience working with someone, don't be
afraid to decline giving feedback. It will give the other party a chance to
find someone else. But in most cases, you should be spending the time to come up
with valuable feedback.

## Generating Feedback Ideas

Feedback has a half-life. It's most relevant right when you think of it - in the
moment. That's also when it's most effectively received. You should always try
to give people feedback as quickly as possible after the positive
or negative issue occurred. But, negative feedback should be given privately.

Managers already know it's a good idea to write down feedback as it comes.
It helps tremendously when reviews come around. This is good practice for anyone!
You can write down feedback in a shared document that you use for one on one notes.
It will help you remember specific examples later. More on that in a bit.

What else can you do to generate feedback? Here are some things that might jog
your memory.

- Email history to and from this person.
- Chat history.
- Code they have written, or code reviews they have done.
- Stories they have worked on.
- Documentation they have written.
- Retrospective notes.

## Be Specific, Give an Example, Cite Impact

Good feedback is specific and explains the impact. It contains a mix of positive
and negative feedback. It's been written in a thoughtful way. There are three
critical components:

1. Be **specific** - say *what* was good or bad about something they did.
2. Give at least one concrete **example**.
3. Explain the **impact**.

These rules go for both positive and negative feedback. The recipient will be
much more likely to internalize the feedback if you're specific, give an example
and explain the impact. It will also be much more actionable for their manager.

Don't be afraid to give it to the individual yourself. This is especially
true for positive feedback; it's very powerful for team health to call out praise
in a public setting. You could mention it in a team meeting, email, or via your
company's peer recognition program.

## Feedback Anti-Patterns

Often feedback is too generic. For example, "I like working with this person".
I call it *feel good feedback*. It's not specific. Giving an example and citing
the impact can transform your feedback into actual data.

Research has shown that you give the best feedback after getting to know someone,
but before forming a semi-permanent opinion of them. It turns out that the window
for the best feedback is [one to three years of exposure](https://en.wikipedia.org/wiki/360-degree_feedback#cite_note-13).

If you've worked with someone for less time than that, or more, be aware of your own
biases. It might make sense to bow out of giving them feedback, especially if
you're trying to choose between too many requests.  
---
title: Product Management 101
toc: true
tags: product-management
---

This is intended as a guide for new front-line product managers on my
team. It could also double as an introduction for people brand new to the
discipline. My views are inexorably colored by working as an engineer
exclusively on Agile teams at small companies.

## Basic Hygiene

From the perspective of the engineering team, these are the absolutely essential
functions of the product manager.

1. Be the domain expert. The team relies on the product manager to know the
business and to explain it a way that they can understand.
2. Prioritize work into an ordered backlog. Be aggressive about defining the
smallest scope so that the team can ship quickly.
3. Write detailed specifications and user stories.
4. Be present at team meetings and stand-ups religiously.

## Relationship Nexus

The product manager is the conduit between upper management, the consumer and
the execution team. Each group rarely interacts directly with the others.
In most situations, the product manager acts as a proxy for the other groups.
They need to build trust with each group, learn to speak each one's language,
and distill context in a different way for each of them.

The product manager is likely to have the best people skills of anyone on the
execution team. The best product managers I have worked with are also the
champion of team health.

The product manager has a special relationship with both the designer and the
engineering lead. The product/design/engineering leads should have a regular
sync where they talk about all the features that are in progress and coming up.

### One on Ones

The exact topics of your 1:1s will vary. The most important things are building
trust and repeating context so people internalize it. It can help to have a list
of [generic topics](/blog/2015/02/10/one-on-ones.html) to get conversations
started.

The minimum set of 1:1s looks something like this:

- Weekly 1:1s with the engineering lead and the designer.
- Weekly 1:1 with your boss.
- Semi-regular 1:1s with everyone on the execution team.

## Next Level

Both the product manager and the engineering manager have a responsibility to
protect the team from thrash. This is the [shit umbrella](https://roadmunk.com/blog/shit-umbrella/)
role - give the team the space they need to do the work.

Many teams do not have a dedicated QA person. In that case, the product manager
typically takes the role of identifying and documenting production bugs. Product
managers need to know how to write a great
[bug report](/blog/2016/02/26/QA-101-How-to-write-a-bug-report.html).

The very best product managers I have worked with have some level of technical
understanding of the implementation the team has created. It should be just
enough to sketch out how the product works at a high level. Otherwise, there
will be a mismatch between the [mental model](/blog/2016/04/15/mental-models.html)
of how it works, and how it actually works. Over time, that will result in
unexpected behavior and a poor user experience.

### Up-Leveling Technical Understanding

There is no magic bullet for this. One thing I would suggest is actually
white-boarding your take on system architecture with the engineering lead on a
regular basis. The product manager should drive. This will quickly highlight
gaps that you can dive into more detail on.

## Third Rails

There are handful of surefire ways to burn goodwill with engineers.

### Give an Estimate Without Buy-in

The product manager may feel pressured to say that something will be easy, or
commit informally to an estimate for a deliverable. This is always a mistake.
In the best case, their estimate is correct, but the team is unhappy. In most
cases, the estimate is dangerously off base.

### Commit to a Technical Decision Without Buy-in

The engineering team ultimately owns technical decisions, with the product
manager collaboratively involved. It's never OK to make a technical decision
without the team. Watch out for well-meaning non-technical stakeholders
dictating solutions. Those conversations should be redirected to clarify
requirements, and then the team should come up with the solution.

### Lack of alignment

The number one silent killer of projects is lack of alignment with upper
management. The product manager must be relentless about continuously
re-aligning on goals. Often, they are the only alignment touch point for the
team.

## More

See my [previous posts](/blog/tags/#product-management) that are relevant for
product managers.

## References
- [What It Takes to Become a Great Product Manager](https://hbr.org/2017/12/what-it-takes-to-become-a-great-product-manager)
- [What Makes a Great Product Manager](https://hackernoon.com/what-makes-a-great-product-manager-3c1d03b90356)
- [Good Product Manager/Bad Product Manager](https://a16z.files.wordpress.com/2014/08/good-product-manager.pdf)
- [Management Pattern: the Shit Umbrella](http://managementpatterns.blogspot.com/2013/01/pattern-shit-umbrella.html)
---
title: How to Handle Layoffs as a Manager
tags: manager
toc: true
header:
  overlay_image: /images/header-layoff.jpg
excerpt: "All companies go through layoffs. As a manager, you can count on
participating in one, someday."
---

As apposed to firing someone for performance reasons, layoffs are typically
for financial reasons. They are part of a natural cycle of growth and
restructuring.

If you are given the opportunity to have the actual termination conversation
with the people on your team, I suggest you take it. It will be difficult - but
this is fundamentally not about you. It's about those people who are losing
their jobs, and the rest of the team that has to move forward.

## Initial Shock

Most people in the company will not know about the layoff until it's happening.
As a manager, you might know the day before. You probably will not have any input
into who is being laid off.

Don't freak out. Seek out other managers. Go talk to them. It's is a good time
to capitalize on the relationships you're building all the time to try to bring
some stability to what's happening.

Quickly figure out the important details. Who is being let go? Do they get
severance? What about health care? Can they exercise options? What is the
company message about why this is happening?

## Come Up With a Plan

Part of your job in a layoff is to tell the same story as the company. Own the
message. Do not say that this was not your decision. Practice the story that you
will be telling over and over again in the coming days.

Start by writing it down. HR likely has an official version. Translate it into
your own language and decide what you want to emphasize.

- This is affecting a lot of people.
- It's about financials, not performance.
- Thank them for their work.
- There will be some logistics.
- Bring it back to the company vision and mission.

You may not have a lot of time to prepare. Role-play the conversation with
someone else, maybe another experienced manager. Have them ask hard questions.
Don't worry about how employees will take it. It's not going to be as bad
as you think.

## Layoff Day

Don't put anything on the calendar. Schedule a room as early as possible in the
day. Rumors will start circulating about what is happening. Get all your people
squared away before that happens.

Let the rest of the team know they are safe. Nothing is worse than sitting
around wondering if someone is going to tap you on the shoulder. For the team
that remains, how you handle today will be the most impactful thing you do all
year.

## The Termination Conversation

Look the person in the eye. Tell them right up front that the company is having
layoffs today and that unfortunately, they are in the affected group. Tell them
that as of this moment, that they are no longer employed here. Tell them you're
sorry. Then don't say anything for 10 seconds.

Your basic job is to empathize, provide clarity about some logistics, and
represent the company. When they leave the room, you want them to feel they
were afforded some respect in a truly crappy situation.

Give them a chance to process. They will have questions. Absolutely do not say
this was not your decision, and that if it were up to you, they would still be
employed. It was your job as a manager to make them more successful. It doesn't
cost you anything to own that.

Repeat what they need to do. They need to clean out their desk and go to HR
to sign some papers. Say it a few times; it's hard for anyone to process
information in this situation.

## Hard Questions

The conversation will probably be far short of your imagined worst case
scenario. If things do go fully sideways, just try to optimize for empathy and
clarity.

More likely, you will get questions similar to these.

### Why me, specifically?

Tell them it's about company financials, not their performance.

### Why didn't you give me feedback earlier?

The layoffs were a decision that was made in a very short timeframe. We
thought we would have more time to work on these issues.

### I thought this team was a priority for the company?

The team is still a priority, but cuts had to be made across the board.

### But, didn't we just hire more people?

Those hires are for different roles.

## Afterwards

The really hard part comes afterward. You have to bring together a bunch of
people who are emotionally hurting. Help them pick up and pieces. This looks
more like what you do all the time. It will be tough, but it's not outside your
wheelhouse.

Have one on ones with absolutely everyone. More often that usual. Pay special
attention to people who lost their manager, or people who are working on
different projects now, or with different people.

Make sure in all these conversations to bring it back to the company vision
and mission. Good luck.


## References

- [Delivering Big/Bad News](http://randsinrepose.com/archives/delivering_bigbad_news/)
- [Deconstructing Managers (Day #6)](http://randsinrepose.com/archives/deconstructing-managers-day-6/)
- [How to Lead When Working Through Layoffs](https://www.cio.com/article/2894431/layoffs/how-to-lead-when-working-through-layoffs.html)
---
title: Interviewing as a Manager
tags: manager interview
toc: true
header:
  overlay_image: /images/header-san-fransisco-2016-4003-2.jpg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Interviewing as an engineering manager is quite different from interviewing as an individual contributor."
---

For a recent round of interviews, I talked to about 50 companies. Of those, 24 moved on to phone screens. Just seven resulted in on-site interviews, and only three of those companies made offers. The whole process took about six weeks.

## Preparation

I started by purchasing the canonical [Cracking the Coding Interview](https://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/098478280X) book. The first ten chapters detail various types of technical questions. The most useful exercise was sitting down and doing the (approximately 60) example questions in those chapters. I recommend getting an actual whiteboard and talking out loud as you solve the problems. I took photos of the whiteboards when I was done in order to be able to quickly reference back to them, like flash cards.

[You can see my full solutions, white-board images, and notes on GitHub.](https://github.com/chase-seibert/cracking-the-coding-interview)

Working through the problems on a physical whiteboard was the most useful part of preparation. Trying to verbalize my thoughts, translate them into diagrams, and prove that my solution worked on an example set was the closest thing to an actual interview.

## Selecting Companies

My target companies were late-stage but pre-IPO, and they had to be in San Francisco. Initial brainstorming yielded quite a few candidates. Then, I looked at every connection I had on LinkedIn, and added the companies that met my criteria. I ended up with about 50 companies.

Next I created a spreadsheet of companies. Glassdoor ratings were a good source of objective data. _Note: Glassdoor can vary wildly by the type of role; try filtering by keyword, such as "engineering."_ Keep in mind that companies like Lyft and Uber have drivers skewing their Glassdoor ratings.

![spreadsheet](/blog/images/interviews-company-sheet.png)

[LinkedIn Premium](https://premium.linkedin.com/) was useful for directly InMailing recruiters. It was also interesting to see who was checking out my profile as I progressed through the interview process. I did not find it useful for identifying opportunities.

## Reaching Out

I've heard that the [average tenure](https://www.quora.com/What-is-the-average-tenure-of-a-software-engineer-in-a-silicon-valley-company) for software engineers in the Bay Area is two to three years. People move around a lot. In my professional network, this meant that I knew people at virtually all my target companies. I asked each of them for an introduction to the right internal recruiter via a combination of text messages, emails, Facebook messages, and LinkedIn InMails. If I didn't know someone at a company, I used LinkedIn Premium to InMail a recruiter directly.

Initial phone conversations with recruiters aimed purely to get on the same page about potential roles. The next step was a phone screen with a hiring manager. Those were usually focused on work history, and some initial behavioral questions about management. Rarely did we cover technical topics in the phone screen.

I found it useful to have a well-rehearsed synopsis of my background, types of roles I had held, and what I was looking for. I made sure to stress my philosophy of management, and how it might be different from that of other engineering managers. This helped determine whether a company was going to be a good fit.

## Phone Screens

Phone screens started out with a two minute spiel on who I am, what I've done, and what role I'm looking for. I got pretty good at telling it like a story, and presenting my career arc as a series of intentional choices. This story also came in handy during recruiter conversations and on-site interviews.

 Phone screens were split 80/20 between non-technical and technical topics. The bulk of the conversations were behavioral management questions. See "Common Behavioral Questions" for a representative list.

I thought about these conversations as two-way streets. The hiring manager was evaluating me, but I was also evaluating them and the company. I tried to  figure out whether we both believed that engineering managers should focus on non-technical stuff.

I created a gut-check rating for how well each call went. Looking back, a positive gut-check was not predictive of whether I would ultimately be offered a job. But it did correlate to my ultimate enthusiasm about the role after an on-site interview: If I was excited about the opportunity during the phone screen, my excitement level after the on-site interview was unchanged.

I scheduled my on-site interviews such that the roles I wanted the most were clustered last. I used my initial on-site interviews as practice. These turned up a number of behavioral questions, for which I was able to come up with polished answers by the end of my interview loop.

## On-Site Interviews

I was pleasantly surprised by the focus of my on-site interviews on management versus technical topics. All had substantial technical portions. One was almost purely technical.

On-site interviews were broken down into slots. All of them shared these features:

- One or two system designs on a whiteboard. For example, I had to architect a system to process web-hooks, or a link shortener like bit.ly; or implement RPC over pubsub. In general, I had over-prepared for these. I think I would have performed just as well with no preparation.
- Two or three behavioral slots, typically with other engineering managers.
- Meet your grand-boss, i.e., the manager's manager. In my case, this was not a surprise: It was always on the schedule up front.
- Short wrap-up with the recruiter.

Around half of the on-site interviews also had slots like these:

- Present a past project that was sufficiently technical, and dive into architecture decisions, etc.
- Meet with partners/stakeholders, e.g. for product managers, QA, etc.
- Meet with prospective team members, i.e. the people who would be your direct reports. In some cases, this was just the single most senior team member. In other cases, it was a few people at once.

In three out of seven cases, the company requested a follow-up conversation/sell slot with the hiring manager or grand-boss on a subsequent day.

Only two companies asked me to code. I WAY over-prepared for these, doing more than 50 coding exercises! Next time, I will replace most of that preparation with more behavioral questions.

### Common Behavioral Questions

This is an incomplete list of the questions I was asked. In my answers, I tried to be explicit about what action I took, and what the result was.

#### High-Level

- Why do you want to work here specifically?
- What is a manager to you?
- Why did you originally decide to be a manager?
- How do you manage people?
- What are your principles?
- Tell me about a time you failed.

#### Conflict

- Tell me about a time you had a technical disagreement with someone.
- Tell me about a time you had to override a technical decision.
- Tell me about a time you had a disagreement with upper management.
- Tell me about a time you said no to your boss.
- What constructive feedback have you gotten in the past?
- Tell me about a time you hurt someone's feelings.

#### Execution

- Tell me about how to handle a poor performer.
- Tell me about a time your project was behind schedule.
- Tell me about a time you influenced another team that was not performing.
- Tell me about a time you prioritized refactoring over feature work.
- Tell me about a time you made an organization-wide change.
- Tell me about your proudest project moment.

#### Growing a Team

- How do you think about growing a team?
- How would you evaluate a candidate for promotion to manager?
- Tell me about a time you had difficulty hiring for a key role.
- How would you decide whether to make someone else a manager?

## Making a Decision

Of the seven companies, I got offers from three. I pulled out of one because it was not a good fit. I got rejected from the other three.

Picking between the three companies was not a very scientific decision. I looked at the manager, team, project, and office location. I tried to estimate the total compensation over four years for each company. I reached out to a number of trusted people in my network to get their opinion on one company versus another. At the end of the day, my heart was with one particular choice.

## Negotiating

Of my three offers, base salary and RSUs tended to be in the same ballpark. Yearly bonus and signing bonus varied significantly. Only one company committed to RSU refreshers and a yearly bonuses in writing. My strategy was to maximize different aspects for each offer, such as base salary, signing bonus, stock options, etc. Then, I asked my top choice to close the gap on the other offers for those dimensions. The result was about a 7% bump in total compensation over four years.

## Lessons Learned

Here are some of my high-level takeaways for engineering-management interviews.

- Don't spend too much time prepping for system-design questions.
- Spend virtually no time preparing for coding questions.
- Over-prepare for behavioral questions. The on-site interviews where I did the worst were the ones where I regretted one or more of my answers to behavioral questions.
- Treat interviewing like a full-time job. It took me about six weeks of 40-hour weeks to network, line up phone conversations, and complete on-site interviews.
---
title: What I Learned at My Last Job
tags: career newboss
toc: true
header:
  overlay\_image: /images/hawaii-2018-3322.jpg
excerpt: "On the surface, what you learn at a particular job takes the form of bullet points on your resume. But there are also deeper, more private lessons."
--- 

At my last job, I learned what it really means to be a manager. For the first time, I managed other managers, and I had to build a team by hiring. I’ve added those things to my resume. But I also picked up little points of wisdom along the way. Those were even more valuable. Here are my key takeaways:  

## Pivots Are Hard

The term "pivot" is used when a company changes its business model. Specifically, I was pitched on helping the company transform from a content/SEO focus to a product focus. My teams built the company's first non-content products. 

This process was extremely painful. Three years later, even getting on the same page about what being a product meant was still an ongoing discussion. Basic concepts such as user engagement, what a product manager is, and engineering as a value center were still not universally well understood. 

**Learning: Non-product-oriented companies cannot become product-oriented without a LOT of pain.**

## C-Level Reporting Structure Matters

I did not fully understand the impact of the company being a “matrixed” organization. This turned out to mean that the company was split into many sub-organizations. Each one operated with independent leadership and engineering teams. In practice, it also meant that the various groups often worked against each other instead of with each other. 

As part of the product pivot, all the groups, including a unified engineering organization, were put under a CPO (Chief Product Officer). This produced a gap in how engineering was perceived. Instead of seeking to understand how engineering could be empowered, the company attempted to tell engineers what to do. 

**Learning: For engineering to be properly valued, it needs to report up exclusively through engineering leaders all the way to the CEO.**

## Trust Your Business Instincts

I probably should have learned this lesson previously. As engineers, we have a tendency to abdicate serious thinking about business problems to business experts. But twice now, I’ve seen that I should trust myself more. 

Once before, the red flag was a reliance on Facebook as a partner. This time, it was a reliance on Google and SEO. In both cases, those risks actually materialized into large negative impacts on the business. When you identify a red flag for the business, go into your role clear-eyed about that risk. People you interview with will have polished, buttoned-up answers to these concerns. Remember that they have had months or years to polish those answers! You are smart; don’t discount your intuition. 

**Learning: Take seriously any business risks you can self-identify while interviewing at a company. Don’t let anyone, even a founder, talk you out of your concern.**

## Tactical Stuff

Finally, here are some smaller, more tactical items I will take with me going forward. 

- The biggest reason projects fail is **lack of alignment** with upper management. You need to continuously realign. 
- Treat performance reviews as a year-round job. You should have a **living draft of reviews** for all your direct reports and regularly talk about the feedback with them. This is especially important if layoffs occur unexpectedly: you don't want to add to the surprises. 
- **Be flexible** about team composition. You don’t have to create the ideal team right away, especially if there isn't alignment with leadership. 
- In the face of ambiguity, a bias toward action should be tempered somewhat with **seeking first to understand**. 
- If you can’t sit down for a one-on-one with a high-level stakeholder, at least **try to put yourself in their shoes**. 
- Set expectations of **confidentiality** around personnel moves. Specifically when moving people between teams and when people are leaving the company, make sure anyone who does know ahead of time keeps it under their hat. 
- Be careful about executing a **succession plan** for yourself without knowing what your next role will be. Always do the right thing for the company, but you may be moved quickly into a new role that you’re not as excited about.
- Identifying a problem is not enough. You also need to **propose a solution**. Otherwise, it’s just complaining. 
- Think about who to CC on email outside your direct group. Any email that you send above your boss in the reporting chain should also include them. 

## Conclusion 

What I really learned was that in a management role, you need to think a lot more about the big picture. You need to think about not just the business fundamentals, but also the the highest level stakeholders. ---
title: Writing Your Self Review
tags: manager reading-list career
toc: true
header:
  overlay_image: /images/header-self-review.jpg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "The best feedback is tied to one or more specific examples, and
also highlights the impact of what happened on the team or company."
---

Many companies ask employees to write both self-reviews and 360 reviews of their coworkers at least
once a year. Especially for self-reviews, you should put a lot of time into coming up with specifics. For example, you might discuss
how quickly you were able to troubleshoot a production issue. You should try
to gather data and metrics as well. Make sure to specify what the
impact was in each case.

## Gathering Data

How do you gather data for your self-review? First, brainstorm all the projects
 you have worked on recently. You can look at old email, calendar items, task
tracking tickets, code changes, and Slack conversations. If you have a personal
growth plan, looking at your goals may jog your memory. You can also look at
your company's values and mission statement.

If this is your first performance cycle working with a new manager, it makes
sense to include the highlights of your historical accomplishments at the
company. He or she may not have gotten that full context from your previous manager.

The more number-based metrics you can gather, the better. It's totally worth
your time to write some lightweight code if you need to pull certain data. For
example, in the past I've spent a few hours coding solutions for [analyzing my calendar](https://github.com/chase-seibert/gcal-report), [my wiki contributions](https://github.com/chase-seibert/confluence-stats), and my
[code review comments](https://github.com/chase-seibert/github-org-stats).

## Writing

You always want your writing to be concise. Summarize the most important points
 at the top. Most content should be in bullet point form. Include all
 accomplishments, but organize them into categories. While your manager will
read the entire thing, it will help if it's scannable when you both sit down to
discuss it.

Most items should provide specific examples of accomplishments, and explicitly detail their impact on
 the team. It's not enough to say that you write very high-quality code. It's
much more actionable if you say you helped a teammate fix a specific bug in
 some code, resulting in system downtime being avoided.

It's worthwhile to try to phrase accomplishments in the same terms the company
 uses to evaluate people. Shamelessly use the exact words and phrases contained in the
review guidelines, mission statement, and company values.

Have a draft of your self-review ready for the last one-on-one with your manager
before it's due. It's fair game to talk about it in advance with your boss and
potentially change some items before you submit it. Your manager can help you
brainstorm additional accomplishments, examples, and impacts.

 Right after you have written your self-review is an excellent time to think
about updating your resume. Many of the accomplishments in your self-review, especially the
 data-oriented ones, are the exact things you should be adding to your public-facing
 resume. People often forget their accomplishments when it comes time to
update their resumes for a job search. Gathering specific metrics and data points
contemporaneously with the work that you are doing is much easier than trying
to dig them up later.

## Going Forward

 A good manager should be drafting your review throughout the year, and you should
also be drafting your self-review on an ongoing basis! Take notes on your
 accomplishments throughout the year, and write down specific quotes you hear about
 your performance. Those are fair game to include in a self-review, even if the person quoted is not giving your 360 feedback. If the director of your department once
called you out in an all-hands meeting for doing a great job, include that in your self-review!

 How do you remember to write down your accomplishments over the course of
 a year? Try using a personal Google doc and set a weekly reminder on your
calendar. Each time, think about one or two things from the past week that you can document. When your self-review comes up, pick out the most impactful items.
As a bonus, you should also raise those accomplishments in real-time with your boss
during normal one-on-ones. Just like a manager giving feedback to an employee,
 it's more impactful to highlight these items as they happen.

## Wrapping Up

Don't be afraid to toot your own horn! While there is a place in your
 self-review to reflect on your weaknesses, the bulk of the review should
focus on what you're great at. During your day to day you optimize for team
 success. When writing your self-review, you want to call out what you
individually contributed to the team.

 Be aware that writing anything well takes time. You will need at least two or
 three drafts of your self-review. So, start at least a week ahead of when it's
due. That also gives you time to gather any relevant data that is not readily at your
fingertips even if it involves writing a little bit of code. Good luck!
---
title: Getting Started Estimating with Story Points
tags: manager process scrum workshop
toc: true
header:
  overlay_image: /images/header-story-points.jpg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "How do you bootstrap a team to use story points?"
---

I’ve previously written about [why estimating work in story points versus hours can be useful][1]. Assuming you’re convinced, how do you actually begin? My advice is to first educate the team on the principles involved, and then just dive in with a basic grooming session. Expect growing pains! If you have one team member who does have experience with story points, have them lead the sessions. Otherwise, you may want to borrow someone, if possible.

# Getting Started Group Exercise

The idea with these [hands-on exercises][2] is to engrain the basic principles of wisdom of crowds, relative estimation and risk assessment. But they can also serve to get the team excited and motivated to try something new. Try these as the first part of a team off-site agenda.

## Wisdom of Crowds: Cow Weight (10 min)

First, put the team members in random order on a whiteboard. Then, ask the group to **silently** estimate the weight (in pounds) of Penelope, the cow in this image:

![][image-1]

It’s important that the team members not influence each other. No group discussion or clarifying questions! Have everyone record their estimates on post-it notes.

Now, going one-by-one in the random order on the whiteboard, ask each person  for their estimate, and keep a running tally of the current average of all responses.

It turns out the correct weight of the cow in that photograph is 1,335 pounds. How close was the overall estimate of the group? Did the estimate get more or less accurate as you added additional people?

_Note: this is from a classic [Planet Money podcast episode][3]. Interesting fact: it turns out that if you get enough people together, their collective estimate will be on par with experts who estimate cows at auction for a living! Both groups have about a 5% margin of error. _

## Relative Estimation: Countries (10 minutes)

Start by asking the team to estimate the square mile area of the following countries. Time how long it takes the team to come to consensus.

- Mexico
- France
- China

Record the answers and time, and move into the second part. Now, ask the team to estimate relative to the United States - i.e. your unit should be “multiples of the United States”. Remember to start the timer again!

- Canada
- Spain
- Russia

Given that the actual area of the Unites States is 3.7m square miles, calculate what their relative estimates equate to in square miles.

Now, compare the results for both parts with the [actual square miles areas of these countries][4]. As a group, calculate how far off the two sets of estimates were. Are the relative estimates more or less accurate? Which version took more time?

## Risk Assessment: Commute Times (10 min)

For each person on the team, ask them to think about how long their commute in to the office typically takes. Have them write their initial estimates down on post-its. Now, have them think about what risk factors potentially increase their commute times. Maybe there is traffic, or the bus is late, or they miss a train, etc. What if you have to change your method of commute all together some of the days?

Say you commute in to the office 230 days a year. How much would you need to increase your estimate by in order to make sure that your commute comes in under the estimate 95% of the time? That would mean you could exceed your estimate less than 10 days a year.

# Putting Principles into Practice

Theoretical exercises are all fun and good, but how do you actually run your first grooming meeting where you’re using story points?

_Note: Grooming is different from planning. Grooming is about estimating stories, planning is about deciding which stories to commit to for the sprint. _

## Grooming Meeting

The grooming meeting is about creating fully estimated user stories. You should plan to have at least one hour to groom two weeks worth of work. In order for the meeting to be successful, you should have all of the following at hand already for every user story:

- Written details, with acceptance criteria
- For UI changes, a visual mock, could be just a wireframe
- A product owner present to answer questions

Any engineer who could be assigned a task for one of the stories should be present. Often this is the entire team.

_Note: if this is the very first time your team has tried to estimate with points, first select a story that was previously completed which is of medium size; something one engineer can complete in no more than a week. Then arbitrarily call that a 3 point story, to bootstrap the process._

Start by taking a new user story off the top of the backlog. For each user story:

1. Have the product owner read the user story as written. The team should ask clarifying questions.
2. Write down the clarifications in the user story itself.
3. Pay special attention to documenting what is in and out of scope, and what unknowns or risks exist.
4. When people have asked all their clarifying questions, ask the team to *silently* estimate how “hard” this story is relative to other stories you have completed recently. When everyone feels ready to estimate, count to three, and have everyone put up a number of fingers representing their estimate.
5. If the team is at consensus on a story point value, record that and move on to the next story. If it’s close, you can also just agree on a conservative compromise.
6. If there is no consensus, discuss why people’s estimates are different, and go to step 2.

The question “how HARD is this?” should prompt the team to think about not just the known work involved, but the unknown. The story point estimate should be a combination of complexity, uncertainty and risk.

I suggest using Fibonacci numbers as your options for valid point estimates. This acknowledges the inherent margin of error in estimates, and also helps converge group estimates more quickly.

# Additional Improvements

As the team gets experience with story points, there are some additional concepts you can layer in.

## Definition of Done

One common learning for a team is that even with a well defined user story, there can be misunderstandings about what it means to “finish” a story during a sprint. Does it have to be code reviewed and merged? Does it need to be live in production? Does the PM or designer have to sign off on it for it to be “done”?

I would advise teams to start with a definition that includes being merged and in production. Otherwise, you risk the phenomenon of stories being “almost done”, but really requiring a lot more work that is only discovered when you do go into production.

## Checklists

If there are recurring risks or details that keep causing user stories to not be fully completed during a sprint, consider creating a user story checklist for the team to reference during grooming.

Your checklist might include items like:

- Is there a database migration involved? Remember - those have a dependency on another team to approve.
- If a PM needs to sign off, maybe it needs to be deployed to staging/production with some buffer time so the PM can actually look at it.
- Team often forget to schedule time to update technical documentation and written test plans.

## Breaking up Stories

It’s common for teams to discover, over time, that stories with larger point estimates like 8+ are the most frequently at risk of not being completed during a sprint. A typical solution is to have a cap on how large a single story can be, which forces the team to break it into smaller pieces during grooming.

# Conclusion

Hopefully this gives you some idea where to start if your team has never put story points into practice before!


[1]:	https://chase-seibert.github.io/blog/2016/05/13/agile-points-vs-hours.html
[2]:	https://medium.com/tech-travelstart/running-an-estimation-workshop-a240e7cf6bf6
[3]:	https://www.npr.org/sections/money/2015/08/07/429720443/17-205-people-guessed-the-weight-of-a-cow-heres-how-they-did
[4]:	https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_area

[image-1]:	https://media.npr.org/assets/img/2015/07/17/_aph4800-small-2-438fa2f8a441de68849dd8e12b69bb4a53fcc214-s800-c85.jpg
---
title: The Programmer Mindset - Main Debug Loop
tags: reading-list process testing
toc: true
header:
  overlay_image: /images/header_flight.jpg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Validating ONLY with tests is basically flying the plane on instrumentation, versus being able to look out the windshield. Flying visually and by muscle-memory is both more efficient and safer, in conjunction with instrumentation. You’re much less likely to hit a mountain by mistake."
---

When you’ve been coding for more than twenty years, it can be difficult to recapture [beginner’s mind](https://en.wikipedia.org/wiki/Shoshin), and explain how to think like a programmer to someone who is new to it. I remember an incident in college, when I had been coding for a comparatively short time, that crystalized in my mind the thought process behind writing code — what you might call the programmer philosophy. I was helping a friend complete a Computer Science 101 assignment. They were completely new to coding.

They had written an entire solution, on paper, beginning to end — maybe 100 lines of code. THEN they typed it all in to a text editor, and ran it. What do you think happened? They got about a thousand syntax errors. This is when they came to me, feeling like they had hit a brick wall. I had been sitting beside them in the same class — but critically, I had been coding for a while already. I had already internalized the basic thought process of writing code, without ever having to articulate it. Our professor had failed to impart that thought process.

# The Main Debug Loop

What I had to then explain to my friend is the thought process that I’m now going to call the “Main Debug Loop”. I believe that this is a natural mindset that develops in all programmers — assuming they successfully learn to code. It involves breaking down the problem into very small pieces. Small enough such that you’re writing 1-3 lines of code at a time. Every time you write one of these small chunks, you run the program. Usually it doesn’t work, and you try again. Slowly, you accrete code that you’ve convinced yourself works. You build up the whole solution iteratively.

The keys are two fold: you’re building incrementally, and you’re validating as you go. As you gain experience, you work on more and more complex systems. But this mindset scales to problems of any complexity — you just have to break them down more.

This is the main debug loop. Write code, run code. Running the code is the validation.

# Validation Happens in Layers

What exactly is validation? Here is an example of what I’ll call in-application validation. In web development, you write a few lines of code, save the file, and refresh you browser. Then, you interact with the page/application manually to see if what you just changed works. For speed, you’re probably only testing the happy path, or the one edge case that you’re currently implementing. For any other kind of development, the process in analogous, but the specifics look different.

Test validation does not happen in the application. Instead, you run a small set of synthetic assertions against the code you’ve just written. This a best-practice called [test-driven development](https://en.wikipedia.org/wiki/Test-driven_development). TDD is used in conjunction with in-application validation. In practice (and contrary to strict TDD), my observation has been that developers work in short loops, validating in-application, and then quickly fast-follow with unit-test coverage.

Another layer of validation is automated integration testing, using either a tool like Selenium for application layer validation, or Postman for API layer validation. You may also have exhaustive manual testing, potentially by dedicated QA engineers. Finally, you can use feature gating to validate in production. All of these layers of validation work on concert to ensure quality. When you’re writing the code iteratively, you typically utilize in-application validation and/or or test validation, because they are so much faster.

# Optimizing Loop Time

The main debug loop can be something that you execute hundreds of times an hour. Thinking and typing code are the natural bottlenecks — you’re a lot slower than the computer, after all. In an ideal world, running the code to validate what you just wrote is instantaneous. For the vast majority of my coding career, running small chunks of code averaged maybe 5 seconds. The overhead is due to latency in the file system registering that files have been updated, the runtime loading the change, and your own “human time” — interacting with the newly updated application to run the change and see the results.

The computer’s portion of the loop time is variable based on the language, the framework and the application itself. Scripting languages don’t have to be compiled before they are run. Some types of coding naturally involve more or less human interaction. For example, running a console command tends to involve less human latency than refreshing a web app. But because the main debug loop time is so universally critical to the developer workflow, the language and framework authors have a large incentive to optimize it. The developers likewise are incentivized to optimize their own application for it.

# Why Fast Loops are Better

To my mind, slow loop time recalls some of the worst debugging nightmares of my career, what I call “flying blind”. The worst cases have two properties: they are not reliably reproducible, and each attempt at reproduction takes a long time. For debugging, non-reproducibility by itself can lead to unacceptably slow loop time. If you have a deploy to production to see if it worked, or you have to run the code many times to see if it fails once, that’s a worst case slow loop time scenario.

It’s easy to see how this extreme scenario can lead to slow development, and low quality output.

In contrast, some of my most positive memories of being “in the flow” invoke the sensation of being in constant, fluid dialog with the computer. Like a back-and-forth conversation, you’re arriving at your conclusion collaboratively. You’re validating the code nearly as fast as you can think it and type it. For me, even a 10 second latency in being able to run and validate a change can break that flow.

Still, the assertion that shorter loops times are better rests on an unprovable assumption — that running more cycles for a given scope of work will result in higher output per time period, and/or higher quality. I believe this is true, but I concede that (no matter how personally counter-intuitive) it’s possible that over-all throughput and quality could be as high or higher with slow loop times.

# Are Short Loop Times Universal?

Loop times are not guaranteed to be short — in fact, technical entropy will exert constant pressure to increase loop times. Significant developer time needs to be expended to make sure that the test suite continues to run quickly, the application reloads code quickly, and that the UX itself (on user facing applications) affords developers the ability to quickly reload and validate.

Given that brand-new projects tend to inherit short loop time on small codebases from their parent language and framework, it’s no surprise that in my career-to-date at small startups, we tended to have short loop time. We had to expend effort to keep them short, but we were generally able to do so.

However, I have seen that in larger code bases, with larger teams, short loop time is not a given. Perhaps it is too costly to maintain short loop time as the complexity scales? What ever the cause, once the loop time hits the point of breaking flow, developers will naturally seek shorter loops, such as switching to test validation. In an extreme case, you may not validate in-application at all, and trust that your test are validating correctness.

To me, the retreat to test validation seems super dangerous — but the developers will do what they need to do to keep loop times short, even if they are not validating fully.

# Some Integrated Testing is Necessary

Without running code in a fully integrated environment at some point before shipping it, you’re running a greatly elevated risk of shipping bugs. This could look like validating in-application as you write the code, exhaustive manual testing or gated validation in production. Validating only in a synthetic scenario simply does not afford the same confidence as running everything integrated together. How many times have separately developed components not worked properly once they are eventually integrated? This happens even if the interfaces match perfectly — the composed behavior is very often still wrong.

No QA person on the planet would condone shipping something to production without running it the same way a user will experience it — integrated together.

> Validating ONLY with tests is basically flying the plane on instrumentation, versus being able to look out the windshield. Flying visually and by muscle-memory is both more efficient and safer, in conjunction with instrumentation. You’re much less likely to hit a mountain by mistake.

# Does Complexity Lead to Test Validation Loops?

Keeping loop time short is hard when you have complex systems, and large codebases. Hot reloading may very well take longer than 10 seconds by itself, in order to load and recompile a large codebase. Scripting languages have an advantage here, but have their own non-orthogonal costs. Even scripting languages may have unacceptable latency if the framework requires transpiling.

Service oriented architecture presents unique advantages and challenges for the main debug loop. On one hand, you are working on individual, smaller codebase most of the time. Hot reload times are shorter. On the other hand, running your application composed with services and external data-stores gets both very complicated and also takes a ton of compute resources. Before long, running it locally is not even possible.

In practice, I have noticed a correlation between large codebases, service architecture, and a retreat to test validation as the primary debug loop.

# Staging Environments to the Rescue

A staging environment is like a miniature version of production. It should have all the same services set up, as well as the same basic network architecture. It’s just scaled down significantly. Typically it has the exact same data-stores and schema, but totally different data-sets. Staging is totally isolated from production; you can’t talk to production versions of any services, and you can’t talk to production data-stores.

Depending on the sensitivity of the product domain, you may be able to sync production data down to staging, either in whole or sanitized. In many domains, that is not possible from a security perspective, so you create fake test data, with the entire engineering team using the same test data-sets and data-stores. You begin to have your “favorite” test users and records, and can bring them up in-application quickly.

Staging environments have a lot of uses - but how can they help keep developers in the in-application validation flow? Intelligent service routing can help solve the local machine resource problem, AND alleviate the burden of maintaining a local data set. The downside is that it requires that developers have an active internet connection to staging.

The premise is that you hook your development service up to staging, and route your individual in-application validation requests through the normal staging service graph — EXCEPT for one or two services that you’re currently developing. The staging network topology will pieces of the service call graph from the staging environment in the cloud back to your development box, likely over a VPN. It sounds really complicated, but this dynamic routing is a standard feature of service aware routing mesh frameworks like [linkerd](https://linkerd.io/).

# Conclusion

The switch from in-application validation to test validation in the primary debug loop lead to lower quality, slower velocity, and to context switching.

![](https://imgs.xkcd.com/comics/compiling.png)

System entropy towards test validation takes a LOT of work to counter. Maintaining short debug loops quickly becomes a full time job for someone, or a team of someones. But, even with as few as 50 engineers, organizations I’ve been in have opted to pay that cost. It’s possible (though I’m not yet convinced) that the cost grows exponentially as you scale up in terms of people and codebase size. In that case, I would expect companies at large scale to near-universally live with slow debug cycles and the primacy of test validation.
---
title: How to Choose a Team Size
tags: manager process scrum
toc: true
header:
  overlay_image: /images/header-sauron.jpeg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Teams operate best at certain sizes. Try sub-teams of 3-4 engineers, and interleaving active projects more often at the sprint boundaries."
redirect_from:
  - /2019/08/05/eye-of-sauron-force-concentration.html
---

# Force Concentration and Team Size
> Force Concentration – Mass the effects of overwhelming combat power at the decisive place and time. Synchronizing all the elements of combat power where they will have decisive effect on an enemy force in a short period of time is to achieve mass. Massing effects, rather than concentrating forces, can enable numerically inferior forces to achieve decisive results, while limiting exposure to enemy fire. — [Wikipedia](https://en.wikipedia.org/wiki/Force_concentration)

Teams operate best at certain sizes. Microsoft famously described right-sized teams as being able to share two pizzas. [Scrum best practice](http://rgalen.com/agile-training-news/2015/8/22/the-3-bears-of-agile-team-size) is similar, recommending seven individuals, plus or minus two - including all roles, not just engineers. Larger teams have obvious problems; communication overhead, coordination costs, etc. Less obviously, larger teams mean too-large scope — individuals can’t have context on everything the team owns, and can’t build the same level of trust with all other team members.

[Too-small teams](https://chase-seibert.github.io/blog/2017/12/19/dont-split-the-team.html) also have problems. At very small count, there are not enough people to do the work. Absences have outsized effects on delivery cadences, and you miss out on the network effects of estimating as a group and the inherent diversity of views of more people.

A dark pattern often repeated is unintentionally splitting a team into too-small sub-teams. This can be the unintentional consequence of working on too many different things at the same time. Individuals or pairs build context in one facet or project, iterate by themselves for weeks or months, and the team ends up fragmented. The biggest warning sign that this may be happening is lack of trust, and strong commitment to individual goals. This can lead to weak or no commitment to team goals.

How does this happen? Mostly it’s born from good intentions. Teams feel that saying “yes” to requests means they are delivering the most value for the company. If you are overly accommodating about commitments, the team can quickly find themselves executing many projects in parallel, with too few people on each one.

# The Impact of Too-Small Sub-Teams

Short-term, small sub-teams tend to deliver inconsistent results. A team of one or two individuals naturally has more variance in how much velocity they can put behind a project on a sprint to sprint basis. One person being sick, out of the office or pulled into other work can mean velocity is swinging wildly by 50% or more. Fewer eyeballs and less aggregated experience in the room for estimation will result in missing key risks.

Experienced individuals or pairs can often overcome these disadvantages. But it’s seldom sustainable. Even if it works for that sub-team, there is tremendous negative impact on the team at large, specifically in terms of team health. The ability of the over-all team to hit commitments, the team members subjective experience of teamwork, and ultimately team attrition will all be at higher risk. Some warning signs to look out for are people working unsustainable hours, un-even estimation and delivery across the team, and increased attrition.

In the end, teams need to realize that success is in the context of the team, not the individual. In some sense, the larger organization views the team as a single unit. Perception wise, even individuals delivering their own work consistently cannot long escape being dragged down by the poor reputation of a team that cannot be trusted to deliver team commitments. This will ultimately put even those individuals at risk of burn-out and attrition.

Like any group of people in a relationship, it takes constant work to fight entropy. For a team, the best way to continuously build trust in each other and garner the trust of the organization is to commit as a team and execute with teamwork on those commitments.

# Ideal Team Sizes

Using 7+/-2 as a rule of thumb (remember, this includes non-engineering roles), smaller teams of 3-4 engineers and 1-2 non-engineers are going to be most successful if they focus on one major project at at time. Two or more concurrent major projects would mean that the working set is broken down into too-small pieces. Larger teams, such as 7 engineers and 2 non-engineers, could work sustainably on two initiatives, maintaining two sub-groups that still achieve critical mass.

For over-all team health, you’re going to want to have a good mix of seniority in both sub-teams, and also rotate people between sub-teams regularly. Without rotation, you’re basically making two permanent sub-teams, and the cost of coordinating between them in terms of communication and dependency tracking will be high.

The organization cares about quarter level goals; but there are many sprints within a quarter and nothing is stopping you from switching projects as often as every sprint. You can deliver milestones on many different projects in a quarter by focusing exclusively on one or two at a time in short bursts. Work should be shipped to production and left in a clean state between sprints.

Time slicing at the sprint level can open up more opportunities on the team for technical leadership. If a project is only actively being worked on once every two or three iterations, that’s 2x or 3x the number of engineers who are currently leading a project. Projects have a higher bar for planning - so that the sprints close cleanly. They also have a higher average number of engineers executing on them. Both of these are positive attributes for projects when you look to demonstrate the growth of an engineer in a tech lead role.

There is a trade-off here between context switching more often at the sprint level, and the associated ramp-up that comes with it, versus finding the appropriate growth opportunities for all engineers on the team to be able to lead a large work-stream with other engineers involved.

# Making the Pivot

Switching the working model of a team is not easy. The basic principles and artifacts of Agile can help.

- Create a working agreement for the team, including a definition of done that allows switching context as often as every sprint. It might also make sense to emphasize that prioritization of projects won’t happen until the sprint plannings, need to remain flexible based on the latest information, and need to take into account existing organization level commitments.
- Tracking velocity and capacity at the team level will make for consistent sprint planning as people move between projects. Remember to roughly account for number of engineers on each project when pulling in stories.
- Daily standup will be useful to validate that project A was successfully back-burnered, and that everyone is now working on project B or C. A forum to raise blockers will also help first-timers ramp up on a new project.

Understandably, it’s difficult to commit to goals if you don’t feel like you have personal ability to impact them. As you get more senior, part of your job is being accountable for commitments that you are only personally a small driver of. Especially for more junior folks, this may be a new concept. It’s critical to have a process in place for making sure everyone on the team maximizes hands-on impact on many projects and goals — even as you try to right-size working groups in a team. Likewise, you need a sustainable system for continuous progress on longer term quarter and year horizon goals.

# Final Thoughts
> “Men and months are interchangeable commodities only when a task can be partitioned among many workers with no communication among them (Fig. 2.1). This is true of reaping wheat or picking cotton; it is not even approximately true of systems programming.” ― Frederick P. Brooks Jr., The Mythical Man-Month

Context switching between sprints is a real cost. But what’s the cost of static work-streams? You build up more and more concentrated, siloed knowledge inside the team. Team members become less able over time to be deployed against whatever the highest priority initiative is. It’s important to draw a distinction between context switching at the inter-day level, and context switching at the sprint level. The former’s cost is losing flow. The later’s cost is mostly in ramp up time. Losing flow is a pure loss - you will never make up that time. Losing time to ramp up actually pays dividends over time; knowledge is more diffused on the team, which means that more people can effectively trouble-shoot any given issue.

Consider the advantages of adopting a non-traditional model of execution for a medium to large team where you concentrate forces on projects, using sub-teams of 3-4 engineers, and interleaving active projects more often at the sprint boundaries.
---
title: Transitioning Teams as a Manager
tags: manager newboss
toc: true
header:
  overlay_image: /images/header-transistion.jpeg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "When a team gets a new manager, it can be a time of great uncertainty."
---

The manager should try to minimize that uncertainty for the team. Still, it’s natural for people to evaluate whether they still want to be on the team. Why?

# Change is Scary

Think about times in life when large changes happen. Maybe it's the death of a family member. Maybe it's moving to a new place to live, or starting a new job. All of these events naturally invite self-reflection. How are things going? People ask themselves if they want to continue doing what they are doing. These are times of self-reflection. Change is stressful, and more change is experienced, the more stressful it is. People react to this stress by making even more changes, in the form of big decisions.

Why is getting a new manager a high stress event? First off, the team’s relationship with their manager is the number one indicator of whether they are happy at work. Being happy at work is a leading factor on general contentedness. How will they prove themselves to a new boss? What will their next performance evaluation look like? If they had a great trusting relationship with their last manager, they may be worried that they will not be able to replicate that trust. It’s easy to assume that the unknown will probably be worse than the status quo. In particular, if they’re working on a specific promotion plan or timeline with their previous manager, they might be afraid that this could be reset.

Now, put yourself in the new manager’s shoes. You need a smooth transition plan. Ideally, there is a period of time where you are starting to have one on ones with your new team, but everyone is still reporting to the old manager. These could be three-way conversations, where the previous manager highlights a person’s accomplishments, growth plan and path forward, with you in the room. You can use high bandwidth in-person communication to create continuity between managers.

# Continuity

As a team member, you may be apprehensive of changing to a new manager if you feel it will disrupt the work you are doing. Previously, you knew for sure what you would be working on in the coming months and weeks. As the new incoming manager, you probably don’t intend to radically change the teams roadmap. But, you need to make sure that the team knows that! Be explicit about the fact that on-going work will not change. Set expectations that you don't intend on making any big changes, at least for a few months.

The team may also be worried that you will change how work is done. Again, you want to commit to not making any big changes for a specific period of time. If the team is operating at a six week cadence with certain artifacts and regular recurring meetings, you should keep all of those things intact. Don't try to introduce new process, which will just pile up even more perceived change risk for the team.

You want to pay particular attention to the leaders that are remaining with the team. Maybe there is a great tech lead or product manager who will continue to be on the team. You want to call out, in front of the entire team, that these individuals are doing a great job and that you trust them and will rely on them to help you get up to speed. In other words, you should make sure that people know that their contributions are valued, and that there won't be any large additional changes coming in terms of personnel.

# Consistent Message

You should explicitly communicate that you think the team is executing well and has strong vision. The message should be: this is a great team and you're happy to be part of it. You can use this as an opportunity to highlight why you wanted to join the team, and how that overlaps with the team mission. Pay special attention to projects that are on-going, and be explicit with the individuals on those projects that the work is not at risk. You should explicitly say, "hey, I agree that this project is important”.

You want to make sure that everyone on the team and around the team is on the same page about why this change is happening. If there are different explanations for why a management change is happening, it can cause more uncertainty. It may help to have two or three bullet points written down as concrete talking points for you, your manager, and the previous manager.

You want everyone affected by the management change to find out at the same time, and in person. If you don’t do this, be prepared for rumors to start. You should keep the initial set of people who know about the change to a minimum, before it’s announced. After the team is brought into the loop, you should send a wide communication to the affected parts of the rest of the company.

# Build Relationships

Your best tool in this transition is having regular one-on-ones with the individual team members. If you're not the type of manager who has one-on-ones with everyone weekly, this is a good time to over-invest in that. Initial one-on-ones should be about building trust, lending an empathetic ear, and just getting to know each other.

You should also start having frequent one-on-ones with the previous manager. You can use this time to build context on the people on the team, what they are working on, and how they work together. Your goal is to hit the ground running, so that you don't have to spend the first few weeks in information gathering mode.

# Conclusion

Remember that this is a natural time for team members to re-evaluate whether they still want to be on this team. Minimizing change and building trust quickly will help, but it may also make sense to allow mentally for some neutral attrition during this period.
---
title: 💯 The Agile Scorecard -- 12 Questions
tags: agile scrum process scorecard
toc: true
header:
  overlay_image: /images/header-sprinter.jpg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Lots of people claim to do Agile. Most believe it. Many are not doing the basics right."
---

I want to put a stake in the ground for an opinionated, simple, reductionist rubric that covers the basics of what it means to be an Agile team.

In 2000, [Joel Spolsky](https://en.wikipedia.org/wiki/Joel_Spolsky) wrote [The Joel Test: 12 Steps to Better Code](https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/). It’s an opinionated, simple, reductionist rubric for how good a software team is. Like Joel, I want this rubric to be a set number of yes/no questions:


> The neat thing about The Joel Test is that it’s easy to get a quick yes or no to each question. You don’t have to figure out lines-of-code-per-day or average-bugs-per-inflection-point. Give your team 1 point for each “yes” answer.

# The Questions
1. Do you use story points?
2. Do engineers estimate their work as a group?
3. Do you have retrospectives, with completed action items?
4. Do you have frequent stand-ups?
5. Do you have short iterations?
6. Do you have a definition of done, and are stories usually completed?
7. Do you have a stable velocity?
8. Do you ship to production every iteration?
9. Do you have a single product owner?
10. Do you have one strictly ordered backlog?
11. Do you have user stories with acceptance criteria?
12. Do you do regular demos?

These are in no particular order. They are also somewhat overlapping — not completing stories reliably can result in unstable velocity, etc.

# 1. Do you use story points?

Estimates are a core building block for a well-functioning team to be able to have a conversation about trade-offs between time, scope and quality. Relative estimation is one of the key innovations in Agile, and story points are the unit of relative “hardness” estimation versus previously completed user stories.

It’s common for teams to resits adopting story points, and continue giving estimates as delivery dates, or in time units. My opinion is that if you’re still doing time-based estimates, you have not understood a key concept of Agile. See [Using Points vs Hours for Estimates](/blog/2016/05/13/agile-points-vs-hours.html).

# 2. Do engineers estimate their work as a group?

Estimates are a whole lot more accurate when they leverage the wisdom of crowds, utilize blind voting, and are refined through group discussion. Individuals feel much more committed to a story estimate that they had a hand in generating.

Teams that have not understood this may have individual tech leads, engineering managers or even stakeholders estimate for them. God help you if your product owner is estimating the backlog items.

# 3. Do you have retrospectives, with completed action items?

Continuous improvement and empowering the team to make changes to their process are proof points of “**individuals** and interactions over **processes** and tools”. Teams will sometimes choose to stop having retrospectives, which can be a sign that they do not find them valuable. In my experience, this is usually because they do not feel empowered to make real change — and their action items commonly go un-completed.

# 4. Do you have frequent stand-ups?

Stand-ups are short, informal meetings to sync on progress and identify blockers. Ideally, these are daily. On most teams, stand-ups are in-person. In practice, there is a lot of flexibility in stand-up particulars. I’ve seen well-functioning teams hold these over video conference, or decide to do only three stand-ups a week, or do them entirely in Slack.

Other common anti-patterns are making them longer than 15 minutes, inviting too many off-team stakeholders, and holding them sitting down in a meeting room. To me, having stand-ups at least three times a week in any form is acceptable. Even if you’re not standing  

# 5. Do you have short iterations?

Sprints are a time-boxed, regular cadence where the team is delivering user value. They are a forcing function for accurate estimates, clear acceptance criteria, continuous delivery, and quick process iteration. Sprints should be short — between one and three weeks. That’s because if you want to get better at something (like team execution), you need to do it more often.

This item is a two-parter; you need to both have sprints, and also they need to be short. If your team does not have a fixed cadence, or if the cadence is measured in months instead of weeks, you’re not pressure-testing your estimates and you’re not getting better as quickly as you could be.

# 6. Do you have a definition of done, and are stories usually completed?

My main execution metric for an Agile team is what percentage of stories are being completed for any given sprint. I would expect this to be 95% if things are working smoothly. How do you know whether a story is done? The criteria should be written down. Typically this includes things like meets all acceptance criteria, signed off on my the product owner, code reviewed, deployed to production, etc.

Not having a solid definition is a recipe for miscommunication about whether something is finished, what your velocity for the last sprint was, and whether you’re on track for larger milestones.

# 7. Do you have a stable velocity?

If you are doing group-based estimation, estimating accurately, and reliably completing stories, then your team velocity should be stable. It should be well known — something that the team looks at during every planning session. The team probably knows what their velocity is, off the top of their heads.

Velocity typically stabilizes to +/- 10% pretty quickly, subject to expected fluctuations for vacations, holidays, changes in personnel, etc. Absent those factors, if velocity is not stable, you are likely not completing stories reliably.

# 8. Do you ship to production every iteration?

It’s hard to get customer feedback regularly if you can’t show them working software. Shipping to production is the ultimate proof point that you are delivering working software. To do it, you have to open a pull request, get it code-reviewed, tested, merged and deployed. There is no “done” if it’s just working on your local development machine.

Shipping a change is up to the product owner. Often they will want to bundle many sprints worth of work into one release. But, this should not prevent you from shipping to production continuously, to a small number of users, behind some kind of feature gating.

# 9. Do you have a single product owner?

Teams can have many stakeholders. But having a single product owner is critical. This typically looks like one person who takes all stakeholder feedback and produces a single prioritized backlog of work. This is the product manager role in many companies.

If you have multiple product managers all creating a backlog for a team, prioritization becomes very unclear. In practice, you will still prioritize them somehow — potentially by having one of the engineer leads do it. In those cases, you’ve just made the engineer lead the product manager. They will not be very good at balancing stakeholder requests, and prioritizing the right things, as compared to a real product manager. Not have a single product manager is a trap — one that I’ve seen organizations fall into fairly often.

# 10. Do you have one strictly ordered backlog?

Every team ends up executing tasks and projects in a specific order, at least if you’re looking at what got done after the fact. Defining the order of work such that you know before a sprint starts what you’re committing to is a fundamental part of Agile. Beyond a single sprint, you should have the next 3-5 major things at the top of the backlog in some order. It’s fine if the order changes as new information comes in — that’s the product owner’s job.

Without an ordered backlog, the team does not know what’s coming up next. What does it mean for the backlog to be **strictly** ordered? It means that items are in an ordered list where there is a specific item at slot #1, slot #2, etc. This is as opposed to having a bucket of “P0” items. Again, in reality, the team will need to make a decision eventually about which one to do first. A strictly ordered backlog is the product owner’s primary responsibility to the team.

# 11. Do you have user stories with acceptance criteria?

User stories are written in the form “As a $user, I want to $goal, so that $reason”. They also have documented acceptance criteria. This is important so that everyone on the team knows what user value they are trying to deliver. Miscommunication about the goal of a story can contribute to missing commitments at the last minute, as the product owner and engineers realized they have not been on the same page all along. The acceptance criteria are a specific set of items that are in scope to be delivered — which is just another opportunity to reduce miscommunication.

The more subtle value of well-written user stories is that they allow engineers to propose alternatives. Often the same (or similar) user value can be delivered many wants. Giving the engineers more granularity on the user values being delivered makes it more likely for them to come up with a cheaper alternative. Often this can be product behavior that does not match the product owner or designer’s initial vision — but which is acceptable.

Not every item in a backlog needs to be a user story. Many items are better represented as tasks, with language that’s primarily for the engineers.

# 12. Do you do regular demos?

A demo is part of the regular cadence of sprint ceremonies. This is where the engineers demo the user value they have delivered, the product owner accepts the story as “done”, and stakeholders get to give feedback. In my experience, this is often neglected. Many teams mark stories as done as they go, with either no review or implicit review by the product owner, and no formal demo. Formal stakeholder demos are rare.

Still, on the teams I’ve been on that have done this, it’s been enormously valuable both for the actual feedback and also for team engagement.

# Scoring
> A score of 12 is perfect, 11 is tolerable, but 10 or lower and you’ve got serious problems. The truth is that most software organizations are running with a score of 2 or 3, and they need serious help, because companies like Microsoft run at 12 full-time. — [Joel Spolsky](https://en.wikipedia.org/wiki/Joel_Spolsky) in [The Joel Test: 12 Steps to Better Code](https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/)

Of the last dozen teams I’ve been on, all have been somewhere between 10 and 12 “yeses” on this scorecard. It is eminently achievable. If you’re not doing something on this list — why not? Did you try it, and the team decided to move away from it, maybe during a retrospective? Or have you not given it a chance?

It’s common to malign Agile these days. A refrain I hear all the time is “Agile doesn’t work here”. But many of those teams have not tried doing Agile correctly.
---
title: Trust Eats Process for Lunch
tags: agile manager process reading-list product-management
toc: true
header:
  overlay_image: /images/header-trust.jpeg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "A set of individuals can become a self-actualized team that can deliver working software despite uncertainty, not when they have perfect documentation or process, but when they trust each other."
---

Agile is a great framework for new teams to bootstrap initial expectations about roles and responsibilities. As the team progresses from forming through storming, norming, and performing, success depends on evolving those baseline expectations into real, earned trust. No process can fix a team where trust does not exist. Once trust is built, the team can be successful with any process.

It’s right there in the [Agile Manifesto](https://agilemanifesto.org/):

> Individuals and interactions over processes and tools
> Working software over comprehensive documentation
> Customer collaboration over contract negotiation
> Responding to change over following a plan

# My Favorite Career Moments

Whether it was as an individual contributor, tech-lead, or front-line engineering manager, the best projects in my career correlate 1:1 with having excellent trust with a product partner. Especially in the second case, I could have easily balked at the idea of committing an entire team to a fixed deadline project with no defined product spec. But, I would have missed out on one of my favorite career moments.

## [Bullhorn Reach](https://www.bullhorn.com/uk/resources/bullhorn-reach-achieves-milestone-100000-users/)

![Bullhorn Reach](https://www.bullhorn.com/uk/wp-content/uploads/sites/2/2014/10/Bullhorn-Reach-Gets-100000-Users-Infographic.jpg)

Early in my career, I had worked my way up to senior engineer, and then a tech lead of a small team. I had an excellent relationship with a pretty junior PM, after working well together on normal product backlog stuff for about a year. We were an Agile team. One day, the CEO asked if just the two of us wanted to work on a special project.

We took a PowerPoint mockup directly from the CEO, threw out most of our process, and built a new product from scratch. In a couple of weeks, we had the basics up and running, on totally new infrastructure. Over the next 9 months, we iterated and eventually ended up with a $10m revenue stream. Along the way, we added more team members and added back a more Agile process. But the early weeks and months were the best part.

Using the basic Agile framework and roles, we broke stories down into their barest essence. We tracked our work on a physical bulletin board. We had daily stand-ups and demos. But we didn’t estimate anything. If I had a product question, I would just turn my chair and say it out loud, and we would answer it together, right then. At the same time, the PM was learning to code. We would spend our lunch breaks pairing together.

None of this would have been possible without a baseline of trust. I trusted this PM to make the best product decisions he could, he trusted me to make quick technical decisions, and the organization trusted us to deliver on an open-ended problem. We both trusted that the company would recognize the effort, in the end.


## [NerdWallet Mobile App](https://www.nerdwallet.com/l/app)

![NerdWallet app](https://s3.amazonaws.com/cdns3.nerdwallet.com/img/landing/2020/062920_ASO_app-LP/credit+score.png)

When the General Manager scheduled a 1:1 and asked me if I could ship the company’s first mobile app by Christmas, I thought he had gone insane. The company didn’t have any mobile engineers. I don’t even do front-end, my specialty is back-end services. And, Christmas was *six weeks* away at this point. He asked me what I needed, I told him, and we went and got it done.

In the end, we gathered six of the best engineers in the company, canceled everything else, and locked ourselves in a room. We had a working app in two weeks, and we got a more polished version submitted to the app stores in plenty of time for the holiday deadline. Plus, we shipped on both iOS and Android.

The core of the team was me as the TL, plus my existing excellent PM and design partner. There is no way we would have gotten this done with a new engineering/product/design cohort; the trust we had built up to that point was critical. We defined the scope in about two hours. We had wireframe mockups by that first night. We picked a technical platform, React Native, over lunch. We all trusted each other and knew we had to optimize for development speed.

Again, we threw out a lot of Agile processes, but the core remained. We didn’t do estimates, but we did refine the scope regularly based on what we learned. We canceled all meetings, including retrospectives and 1:1s, but I felt like people were never more engaged and growing. We worked at a sustainable pace, no overtime, by remaining flexible about scope.

# Some not-so-great Moments

## Hearsay Social

This was a different team that was struggling with the basics. Missed deadlines, confusion about the scope, quality problems, you name it. Coming into the team, I attempted to quickly move from zero process to an Agile process. But, we didn’t have the requisite trust built up. The team ended up losing some senior talent, restaffing, and THEN adopting Agile.

In retrospect, there was a real lack of trust between the cross-functional leads on the team. The senior engineer didn’t trust product to prioritize correctly. Product didn’t trust the engineers to make scope/time trade-offs. I didn’t trust the team to execute against a deadline.

My main learning from this experience was that Agile is not going to fix a lack of trust or team cohesion. No process can fix that, at least not by itself. What we should have done is focus on building trust first, and then work on process second.

## NerdWallet (again)

My last team had been re-orged, and their product roadmap was put on hold. There was a vague idea for a new product, but no specifics. I was pushing to continue wrapping up the existing work, paying down technical debt, etc, until we had a product spec. Specifically, I was determined that we needed a product manager to guide the work, or we couldn’t start it.

The counter-proposal was that I fill in as the product manager, in the meantime. In retrospect, this probably would have worked fine. But I was too hung up on the process, and the idea that the team could not be successful without a traditional product owner. The issue exacerbated a relationship with our head of product that already had some trust issues, due to a previous misunderstanding.

This hurt my creditability with the head of product and ended up with me leaving the company. Again, the core issue was trust. No amount of adhering to process would have repaired that trust — and if we had that trust, we would not have needed to adhere so religiously to process. If I had handled this differently, maybe this could have been a third highlight project.

# Conclusion

Just recounting the first two projects, I can’t help feeling that these should have been stressful, and likely failures. But, they weren’t. The lesson that I draw from them is not that Agile is too heavy-weight, or not necessary, or should be paired down. It’s that a team can do anything with enough trust. For me, Agile has always been best for steady-state teams, and it builds teamwork and trust in the team that you can leverage for the genuinely exceptional times when you throw it all away and just execute the heck out of something.
---
title: Growing an Engineering Manager
tags: manager playbook
toc: true
header:
  overlay_image: /images/header-growing-em.jpg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "So, you want to be a manager?"
---

I’m always looking for prospective new engineering managers. As a manager, having bench strength in this area is useful in case there is a sudden vacancy due to promotion, attrition or internal movement. Sometimes, the right backup is an engineer already on the team.

# The Ideal Candidate

When I think about who might be a good first-time manager, I’m looking for experience and leadership.

Experience might look like having been at companies and on teams of various sizes, and with various seniority compositions. If someone has 10 overall years of experience, they have probably worked for at least 2-3 companies. They are more likely to know what “good” and “bad” look like, in terms of team health and performance.

Leadership could look like an engineer in a traditional technical lead role. But, it could also be the team’s Scrum Master, super-mentor or unofficial team counselor. If the engineer is not already the technical lead of the team, they should at least have the explicit technical trust of the team members. It’s more important for a first-time manager than an established manager to be able to leverage existing technical expertise when stepping in to a people leadership role.

The ideal candidate has a demonstrated strength in both verbal and written communication. They should also be process oriented; it’s a good sign if they are already running various team processes. If you ask them for thoughts on their peers or current manager, they should have insightful, valuable feedback.

The absolute gold standard for a first-time manager is that they are already effectively doing the job. It shows that this person takes initiative, and does not wait around for someone to bless them with authority, before going ahead and personally fixing something they think is broken. It shows a bias to action, and a strong sense of ownership. I would expect these folks to already be having more impact from their non-technical contributions vs their technical contributions.

# Examine Motivations

Assuming they are open to being a manager, I next look at their stated motivations.

One good motivation is that they realize they can have more impact managing a team than they can have writing code and technical specs. Another good motivation is that they get more satisfaction from coaching and growing people than from solving a technical challenge, or shipping something. These are person specific; many individuals actually won’t maximize their impact as a manager, or feel fulfilled if they spent an entire week or month without writing code.

If someone wants to become a manager so that they will have authority over others, or get paid more, those are bad motivations. In any case, these generally are not true. First-time managers may be surprised how little authority they actually have. They will have responsibility and accountability, but perhaps less actual decision making power (and information to make those decisions) than they expected. Most companies also pay engineers and engineering managers in the same band similarly, and don’t think of the change to a management role as a promotion.

Some of the best prospective managers may be reluctant to lead others. They may be demonstrating leadership, be dissatisfied with their technical impact, and intrinsically motivated to coach others — look for these people!

# Getting Started

If this is a path the person wants to pursue, I start them out with some self-learning resources. For example, I really like the [Manager Tools Podcast](https://www.manager-tools.com/) for an introduction to a variety of management concepts. I typically recommend that the person start thinking about a more formal personal organizational system, such as [Getting Things Done](https://www.amazon.com/gp/product/0142000280). Here’s a [reading list](https://chase-seibert.github.io/blog/reading-list/) of other books and articles.

I may also ask them to write-up some thoughts about why they want to be a manager, and how they are hoping to grow. If an engineer is willing to sink hours of time in to listening, reading and discussing management fundamentals, they are demonstrating the growth mindset (and will) necessary to make the switch.

A prospective manager should start to form their own fledgling best practices on issues like:

- One on ones
- Running meetings
- Execution model
- Career level framework

# Looking for Opportunities

Once I’m convinced that this is the right direction for this person, I start looking for opportunities to get them exposure to management responsibilities.

For example, I might invite them to sit in on my staff meeting, team talent reviews, or hiring committees. I may delegate some meetings to them to run. I will likely ask them to ramp up on doing more one on ones with their peers. Afterwards, it’s a good idea to circle back with their peers to get feedback on how some of these things are going.

Eventually, they will be ready for more formal pseudo-management responsibilities. For example, they could be the official mentor for an intern. This is low risk due to the limited time period of an internship. They could try their hand at turning around a case of poor performance with another team member. This is higher risk, but mitigated by the fact that the low performer is more likely than not to leave the company, one way or the other.

A prospective manager can try mentoring a high performer, maybe someone I am building a promotion case for. Have them write-up a promotion document forces them to really engage with how to articulate performance in-level, and how to pitch it to other managers. If possible, I would include them in some pre-calibration conversations with peers.

There may be other opportunities that come along for on-the-job learning that are more exceptional edge cases. For example, if the person’s current manager is on temporary leave for any reason, that could be a good opportunity to try running a team in a low stakes environment. I might try them leading a team for a fixed period of time like a sprint or a quarter, such as when a team loses their current manager and we have not yet hired a new one. In either case, I would want the engineer to stop coding and fully delegate this and other technical contributions like writing technical specs, during this period. This will give them a sense of what it feels like, and  whether they would be happy in this role full-time.

# Making the Leap

Armed with my own observations and also 360 feedback from their peers about these trials, I’m ready to make a decision about whether this person is ready for a real management opportunity. Companies and teams are always looking for new engineering managers. You won’t have to wait long!

One thing I can start doing proactively along with this candidate is preparing our skip level manager. I want them to be totally aligned with putting this person in a management position when an opportunity does come along. I can also start making sure there is a transition plan and a back-fill ready to step in to this person’s current role.

Something which the candidate themselves can help with is coming up with a succession plan for their current role. If they are the tech lead of their team, how can they hand it off to someone else? Can we train that person up, now? They can also help make themselves dispensable. Delegate everything. As we get closer to an opportunity, stop coding. Stop doing code reviews. Identify an additional manager mentor.

The ideal opportunity often looks like taking over as the manager of the team that they are currently on. In that case, they have excellent context on the people and problem space. But, it might come in the form of a different, very small team, or a new team. I know I will need to remain flexible about moving this person out of their current role, and that they will need to be flexible about team and timing.

---
title: Going Fully Remote as an Engineering Manager
tags: remote manager
toc: true
header:
  overlay_image: /images/header-remote.jpg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Starting on the remote work journey"
---

Many tech companies have been working remote for most of the year due to COVID, expecting to return to the office sometime in 2021. I have recently learned that my company will [not be returning](https://blog.dropbox.com/topics/company/dropbox-goes-virtual-first) to regular office work. This announcement marked a mind shift for me, where I need to stop just surviving remote work, and start thriving in remote work. Not only for my own sake, but also for the teams I lead.

I plan to document my learnings over the next couple of years in a dedicated series of blog posts.

# Baby Steps

Like everyone else, I have already tried to incorporate some basic best practices for remote work. I have slowly created a comfortable and ergonomic home office. I’ve tried to be more intentional about getting out of the house, for both exercise and socialization. I’ve created more separation between work and home life, such as by taking work apps off my personal phone. These are all smaller, personal changes.

# Preconceived Ideas & Questions

Coming into this, I have a number of half baked preconceived notions about how to do remote work well. First, I want to use this as an opportunity to brush up on basic hygiene stuff like meetings, and 1:1s. I want to have fewer meetings in general, move some meetings to be fully async, and more maker time for people on my team. I want to learn how to lead better remote first meetings.

I want to invest in documentation, next year. Especially for onboarding new people. How do we give them the best possible experience, on par with in-person onboarding? I’ve generally been strong in my career in written communication. How do I impart that to a team?

How do you keep people engaged with their teams remotely. When I look back at my career, so many of the memories involve getting together in person. Having a beer at a late planning session. How do we create natural work appropriate socialization when we’re all remote, without it feeling forced?

I expect this change will not be someone everyone wants to go through. I expect some existing employees to be turned off. But, I also expect new hires to be very engaged, because they self-selected in to a fully remote company. Still, there will be painful conversations about existing employee location changes, and compensation changes. I suspect that remote work, on balance, may be a better fit for more experienced hires, because they are generally more self-sufficient.

# Getting Serious about Learning

What I have started doing is reading a lot. Here are the most helpful resources for remote work I’ve found, so far. I’ve compiled them, together with learnings, in a [GitHub repo](https://github.com/chase-seibert/remote-work).

## Best Reading

- [The Holloway Guide to Remote Work](https://www.holloway.com/g/remote-work/about) (**H**)
- [Remote: Office Not Required](https://www.amazon.com/Remote-Office-Required-Jason-Fried/dp/0804137501?utm_source=zapier.com&utm_medium=referral&utm_campaign=zapier) (Base**C**amp)
- [Everything We Know About Remote Work](https://buffer.com/resources/remote-work/) (**B**uffer)
- [Remote Work for Design Teams](https://www.dropbox.com/s/qzg9byoifn7iqdd/InVision_RemoteWorkforDesignTeams.pdf?dl=0) (**I**nvision)
- [The Remote Work Handbook](https://static1.squarespace.com/static/59e6566eb1ffb64ca45fbabe/t/5e7a52b10fdceb59f97f991b/1585074878083/Michael+Hyatt+handbook.pdf) (**S**quarespace)
- [How To Embrace Remote Work](https://trello.com/en-US/remote-work-guide) (**T**rello)

## Learnings & Best Practices

**Communication**

- Have ONE of each communication tool: synchronous, asynchronous, and long term storage, be explicit about what method to use when (S) (T) (C)
- Be explicit about tone, ex: “I’m kidding”, if there is any tone confusion, up level to higher mode (video) (S) (T) (C) (H)
- Communicate with repetition (I)
- Ensure visibility of work (newsletters, boards, exec reports, demos) (I)
- Conscious shift to async communication (C)
- Default status to “On Track”, status updates only for when things are at risk of NOT being on track (H)

**Meetings**

- Start and end rituals
- Most meetings need to be recorded, and have transcripts
- Meetings can be asynchronous, long form threaded conversations (B)
- Don’t mix in-person and remote attendees; even local people should dial in if there is even one remote person (T)
- High fidelity and async → make screen share recordings with voiceover easy (C)
- Making meetings less abundant / more scarce, people make better use of the meeting time that does happen (C)
- Reducing meeting time comes with increased time spent on written communication (H)

**Brainstorming**

- Use an online whiteboard
- Dedicated time for folks to show off/share one thing they are working on (I)

**Timezones**

- Ask questions publicly, not privately (B)
- Recommend 4 hours overlap (C)
- People need to be able to find anything they need w/o asking or waiting for someone else, i.e. most work needs to be possible async (C)

**Culture**

- There should be hundreds of Slack channels devoted to socializing/off-topic
- Standing team interaction time (games, meditation, cooking) (I)
- Remote can amplify existing cultural problems (I)
- Foster 1:1 peer connections to combat loneliness (B)
- Live video “work side by side” sessions (S)
- Consider company sponsorship for interest groups (C)
- Trust inside a team tied to 75% less stress, 50% higher productivity (H)
- Transparency by default can aid in building trust with remote teams (H)

**Hiring**

- Ideal traits: bias to action, can prioritize, good written communication, trustworthy
- Written (not just verbal) communication is so critical that you should evaluate it in hiring — insist on and evaluate a cover letter (C) (H)
- Retaining existing folks with org knowledge is even more important in remote first (C)
- If you pay top-band rates to every hiring market, you get an unfair hiring and retention advantage (C)
- Engineers: ask the candidate to do real work, and pay for it, versus a coding interview, see also contract to perm (C)
- Fully remote companies hire people 33% faster, and have 25% lower attrition (H)
- Best selling points for candidates: flexible schedule, work from anywhere, no commute (H)

**Onboarding**

- Buddy system, check-in via Slack multiple times a week

**Focus / Time Management / Productivity**

- People need long stretches of uninterrupted time to get work done (C)
- Real risk that people work too much, long hours (C) (H)
- Best review of available research is not conclusive re: remote productivity (H)

**Technology**

- Separate work/home devices (laptop and phone) for better separation (C)

**Cost savings**

- Remote saves about $13k/year per employee in tech hubs vs having a dedicated office (H)

# What’s Next

Again, I have not incorporated these leanings yet into daily practice. I hope to put some of these in to action over time, and write about my experiences. I also hope to find answers to some of the questions rolling around in my head, here at the start of this journey. How will the productivity of engineering be effected? How often will we collaborate, in person? How does this change how we hire people? Will engineering compensation change?

Stay tuned!
---
title: Killing Features and Complexity to Reduce “KTLO”
tags: ktlo manager 
toc: true
header:
  overlay_image: /images/header-ktlo.jpeg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Deleting code is the highest return on investment activity for a software engineer"
---

In software, there is a term of art for work that’s required to “keep the lights on”, aka “KTLO”. It’s defined as work that’s NOT adding new product value. It includes traditional “on-call” activities, such as responding to pages, and remediating outages. It also includes fixing bugs, and resolving customer requests. A third subset of KTLO is doing required migrations to new versions of a platform, framework, or operating system. 

KTLO is not the same as technical debt. Technical debt is typically thought of as an existing backlog of stuff to fix. KTLO is the work that you’re actively doing all the time, just to tread water at your current technical debt level. KTLO is also distinct from refactoring work. Refactoring is certainly important, but I’m going to label that work “Foundational”, i.e. work to reduce KTLO. “Product” work is the final bucket of work. Together, these tree classifications of work account for all engineering velocity. 

![](/blog/images/ktlo-pie-chart.png)

# What is a “bad” level of KTLO?

Getting to zero KTLO is not the goal. Some level of maintenance will always be required — think security patches, or even just migrating to new internal platforms in a medium sized company. The only way to have zero KTLO is to never change anything, including fixing the site if it’s down. In any case, zero KTLO would be the wrong trade-off between the amount of effort to “bullet proof” everything, versus adding more value. 

I’m going to arbitrarily throw out 20% as a healthy KTLO rate that most teams should aspire to. This comes from a general rule of thumb that I have seen many teams at many companies coalesce on. Namely that 80% of the roadmap should be product led and prioritized, and 20% should be engineering led and prioritized. 

At the same time, I’ve seen first-hand that 30% KTLO is around where a team starts getting stressed. 40% seems to be a breaking point, where it starts showing up as a top contributor to attrition. Just recognize that whatever the level of KTLO is, this is a tax you’re paying on engineer velocity for other stuff. 

# Why it gets bad 

How do you get to the point where a product requires a high level of KTLO work? One prioritization decision at a time 😉 

It’s easy to blame product management for these incentives. “I would love to refactor this thing, but my product partner just wants to add more value!” But, that’s not fair. Let’s play five questions:


1. Why do I have to prioritize user value? Because product values it, and they own prioritization for the team. 
2. Why does product prioritize user value? Because that’s what they are incentivized on. 
3. Why does the company incentivize user value over other stuff? Because the market rewards growth. 
4. Why does the market reward growth? That is literally how the stock market works.
5. Ok, but why are you beholden to the stock market? That’s the deal you make when you go IPO.

This is also unfair. In practice, I see engineers themselves prioritize user value. No one (for functional equivalents of zero people) wants to work on code that they did not write. They want to re-write it, first. Otherwise, it’s not exciting. Company culture can exacerbate the problem, if it does in fact reward new development over maintenance or foundation work. You should fix that.

# Fix the Incentives 

Personal finance analogy time. Adding new product value like like buying a stock. You’re taking a bet, hoping for top line growth. Refactoring something is like buying a bond. You’re more sure it will be valuable, but the upper bound of the magnitude is lower. Killing existing stuff is like paying down debt. You’re 100% certain it’s valuable, and it’s going to free up resourcing in the future to invest in other things. 

It’s relatively easy to measure the impact of delivering new product value. Users, and internal stakeholders, can see the working product. The company likely has people and an apparatus whose job it is to measure whether the product value is also generating revenue. For accounting purposes, your company classifies engineering salaries as R&D. They actually get a [partial tax credit](https://www.mossadams.com/articles/2019/september/software-companies-r-and-d-credits) for the portion of software engineer salaries that goes to new product development.

It’s less common to be able to measure the value of KTLO, or Foundational work. When your team goes into hero mode and puts out some fires, they are adding value by keeping the product running smoothly. But, they are also costing the company time and money, namely their salaries. This “routine maintenance” does not count for tax credit purposes. Because your company is already separating salaries for new product development and KTLO into buckets, I recommend that you measure Foundational work as reducing the nondeductible portion of the pie. 

For accounting purposes, how many dollars your company spends on the “new user value” part of software development, divided by total revenue, is called R&D efficiency. It’s another common business measure related to KTLO. Notice that you can more of less ignore the cost of KTLO, as long as you’re generating a lot of revenue. That has also been my anecdotal experience of various companies culture’s around KTLO — it only becomes a focus when you stop growing revenue. 

Eventually, every company will stop growing. You can’t hire your way out of KTLO, forever. The sooner you can start talking about the impact of reducing KTLO in terms that the business understands, the more you will be able to incentivize it properly. 

# How to fix it

Beyond incentives, you really need to be able to measure KTLO load, in order to demonstrate the impact of reducing it. One way to measure it would be for everyone to track their time, and categorize time as KTLO vs not. But, virtually no one is going to do that. Also, any measurement could introduce its own perverse incentives, such as mis-reporting time spent to make it look like you’re reducing KTLO. 

For some sub-types of KTLO, you could measure things like number of pages, outages, or P0 bugs. You still have to be aware of the perverse incentives issue. You could also measure total time spent on non-KTLO stuff, such as total JIRA ticket volume, minus the KTLO specific tickets. 

In the end, I’ve found that simply asking folks what percent of their team’s time is spent on KTLO is fairly accurate. At least, people on the same team can roughly agree, and estimates separated by time tend to come back very similar. 

However you measure it, you then want to set a goal. What’s is a good level of KTLO? Again, 20% is a common rule of thumb. Once you have a goal, you want to identify work that can reduce KTLO. 

In general, fixing bugs will not reduce KTLO by itself. If you burn the bug backlog down to a point where you are spending less time than you otherwise would have fixing bugs, THEN you have reduced KTLO (at least until it builds back up). The key piece there is that you would otherwise be fixing a substantial number of bugs. If you rarely fix bugs anyway, reducing the bug count does not save KTLO, although it’s a good idea for quality reasons. 

This can be counter-intuitive, and can result in teams spending a lot of effort on something that does not actually reduce KTLO load. For that reason, I recommend killing things as the primary method of reducing KTLO. Fewer features, and less code, generally mean less KTLO. The highest value targets are going to be things with a lot of complexity, but relatively business value. 

Another clear win for KTLO is automating stuff that you would normally be doing manually. Again, you need to double-check that the team is actually doing the manual work today, versus ignoring it. 

No matter what Foundational work you end up doing, you should circle back afterwards and measure that it actually reduced KTLO. 

# References 
- [Why Do People Neglect Maintenance?](https://news.ycombinator.com/item?id=21208947)
- [Nobody Ever Gets Credit for Fixing Problems that Never Happened](https://web.mit.edu/nelsonr/www/Repenning=Sterman_CMR_su01_.pdf) 
- [Why Do People Neglect Maintenance?](https://themaintainers.org/blog/2019/7/30/why-do-people-neglect-maintenance)
- [Software maintenance](https://en.wikipedia.org/wiki/Software_maintenance) (wikipedia) 

---
title: "How to Escalate"
tags: manager process playbook
toc: true
header:
  overlay_image: /images/header-godzilla.webp
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "The magic of clean escalations is that 50% of the time, you come up with a compromise before you actually escalate. That's just good collaboration."
---

In the context of business and organizations, escalations are when more people are brought into a decision to resolve an issue. 

Note: no [fictional characters](https://en.wikipedia.org/wiki/Alice_and_Bob#Cast_of_characters) were harmed in the writing of this blog post. 

# Example Escalations

What the organization structure looks like dictates how to properly escalate an issue. Different situations call for different types of escalations. 


## Simple Escalation: The Easy Case 


> Sometimes escalation looks like breaking a tie.

Alice and Bob are engineering managers in different parts of the company. Bob is asking Alice and her team to implement a dependency, like single sign-on functionality. This is a requirement for a feature that Bob’s team is building, and they cannot do it themselves. Alice’s team has a lot on their plate right now, and can’t commit to something new. Alice and Bob end up involving Carol, the VP of engineering, to help make the decision. 


![](/blog/images/escalations1.png)


This escalation is made easier by the fact that Alice and Bob report directly to Carol. 


- Alice and Bob probably have good context on what each other is working on 
- Alice and Bob probably have a good relationship with each other 
- Carol can probably make the prioritization decision herself 


## Single Escalation: Sending up a Flare 


> Sometimes escalation looks like pulling the cord in an assembly line.

Chuck’s team is understaffed, and he wants to hire people. But, he needs approval. Chuck escalates to his manager Craig. Craig thinks about moving people around in his organization but decides to hire instead. Craig also needs approval for hiring, so he escalates to his manager Dan. 

![](/blog/images/escalations2.png)

This escalation is also fairly easy. Chuck cannot resolve the issue on his own, so he has to escalate. Dan can approve new headcount directly. Why is this relatively easy? 


- Craig and Dan probably have good context 
- Chuck, Craig, and Dan probably all have good relationships with each other 
- If the issue cannot be resolved, Craig and Dan are only disappointing their direct reports


## Double Escalation: The Common Case 


> Sometimes escalation looks like a ladder.

Erin and Eve lead teams in different departments. Erin’s team is writing a new email product. She wants Eve’s team in the platform organization to implement an IMAP service. Eve’s platform team is working on a multi-year strategy to simplify the set of platforms that they own. They are looking to reduce complexity, not add it! 


![](/blog/images/escalations3.png)


Things get more complicated if two parties in different parts of the organization need to be brought in. These are more likely to involve competing priorities, egos, and differences of opinion.


- More stakeholders will probably be involved in the decision making because it involves high-level strategy 
- Because of the distance between the teams in the organization, and also because of the levels of escalation, individuals are less likely to have good context
- The two teams don’t share any management structure; in the worst-case scenario, this could be escalated up to the CEO

This is the first scenario where hesitation to escalate comes into play. No one wants to escalate something to their boss, if they can solve it themselves. As various parties in the chain grapple with this, each is incentivized to broker a compromise themselves. Every link in the chain will attempt to “short-circuit” the escalation, by talking to their partner in the other side of the chain. For this reason, the worst-case escalation to the CEO should only happen if the situation is otherwise unresolvable.

This type of escalation can take a lot of time. Each step up in the ladder will likely discuss the issue, whether it’s over email, Slack, or in person. Any conversation will involve some lag time, especially scheduling and waiting for a meeting. Because of the importance of context, and also the need to compromise, one on one conversation is needed. 

# How to Escalate

So, how is something escalated properly? First, let’s look at an example of a bad escalation.


## Dirty Escalation: Making Frenemies and Pissing off People 


> Sometimes escalation looks like Godzilla stomping on your city. 

Faythe and Frank are involved in a double escalation. Without discussing the plan for the escalation with Faythe, Frank sends a heated email to just his manager, Grace. Grace goes directly to the VP of engineering, Heidi, skipping several levels of management. Heidi decides in Grace’s favor, and Grace communicates it directly to Frank and Faythe, plus Faythe’s manager. 


![](/blog/images/escalations4.png)


This is a “dirty” escalation, where the conversation was not properly laddered up on both sides. Faythe may feel like Frank “went over her head”. Faythe is likely to resent the decision and resent Frank. Faythe (and her whole management chain) is probably not happy with Grace. Even Grace’s manager will be surprised if Heidie ever brings this up. 

This kind of dirty escalation can happen fast. All it takes is one email, and someone (maybe unintentionally) forgetting to CC someone’s manager. Even if the intention was just to raise visibility, the other party could feel like it was done purposefully to try to increase leverage and drive to a specific outcome. Bad escalations often have this sense of unevenness between the two parties. 


## Clean Escalations: The One Pager 

What would a clean escalation have looked like, in this case? 

- Frank and Faythe **together** draft a short document about their escalation
- Frank and Faythe forward the document to **both** of their managers
- Repeat 


![](/blog/images/escalations5.png)


The document should not be drafted by just one person, alone. This can lead to phrasing that puts one person on the defensive. It would not accurately represent both sides. 

An escalation document should include a TL;DR statement. It should communicate the one question that needs to be answered. It provides context for people who are less familiar with the situation. At each level, the document should be forwarded to both management chains, together.

Often the mere act of sitting down and trying to write up a proposal will lead to Frank and Faythe to come to a compromise. Partially this is because writing down details forces clarity on the situation. It also reveals options that neither party had thought of independently. A big part of last-minute compromises is that everyone is reluctant to involve their manager, asking for a decision. This makes it more likely that each side will accept a compromise that they would not otherwise have accepted, simply to avoid escalating.

## Who to escalate to 

Assuming no local compromise can be reached, it’s time for Frank and Faythe to escalate to both of their managers, together. It’s important that an escalation go to the same level on both sides. The goal is for the two new escalated parties to have equal authority. The important thing is to not skip anyone in the management chain. Even if the decision is skipping a level, visibility should not.

Once a decision has been reached,  Frank and Faythe need to document what the decision was. This is best done as a new section in the original document. They should also email out a version of the final decision to the involved parties, as well as anyone else who may need a heads up.


## Plan for escalation worst-case scenario 

It’s possible to anticipate escalations. Any time there is a disagreement between partners in remote parts of the organization, an escalation is likely. The worse case escalation refers to the highest possible person in the organization that an escalation could raise to. For example, an escalation between engineering managers is likely to max out at the VP of Engineering. But, an escalation between an engineering manager and a product manager could go all the way to the CEO. Sometimes just pointing out the worst-case escalation is enough to broker a compromise.

It might make sense to preview a likely escalation to some of the affected higher-ups, to gauge how likely they are to be supportive. But, this needs to be done carefully so that it’s not treated as an escalation, prematurely. It helps to explicitly say “this is not an escalation (yet); no decision is needed right now”. 


## How to repair a dirty escalation?

If a bad escalation has already happened, it’s possible to bring the situation back under control. One method would be for the original parties to respond by saying that they feel they can resolve the situation themselves. In general, advertising that the issue is being taken off email and back into one-on-one discussion may be enough to stop the escalation. It should be paired with a promise to follow up with an update to the group. 
---
title: "Google Sheets for Managers"
tags: manager
toc: true
header:
  overlay_image: /images/header-gsheet.png
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Spreadsheets could replace 80% of software"
---

Spreadsheets “for managers” is a tongue-in-cheek way of saying that none of these features are particularly impressive to have mastered. But, they will help you greatly in almost any knowledge worker role. 

This blog post has a companion [Google Sheet playground](https://docs.google.com/spreadsheets/d/198ZwkV79iiZvLKDzI1HecdRfG_6NAgcnk-2XlIPH-98/edit?usp=sharing), with examples. Also, I’m not attempting to be exhaustive with any of these topics; merely to provide an introduction and basic syntax examples. 

# Understanding Cell References

A cell reference in a spreadsheet is a variable that points to another cell. For example formula `=A1` would copy the value from cell `A1` to the current cell. Bonus: you can give a cell an easy-to-reference name via `Data → Named ranges`. 

![](/blog/images/gsheet1.png)

## Copy Formula

When a range of cells is selected, there is a box handle in the lower right of the range that you can use to copy the formula to other cells. This is mostly used to “fill in” the rest of a column (or occasionally a row) with the same formula. 

*Note: you can quickly fill the rest of a column with a given formula by holding down* `COMMAND` (on a Mac) *and double-clicking on the copy formula box handle. Alternatively, you can script this with [ARRAYFORMULA](https://www.benlcollins.com/formula-examples/array-formulas-forms/). This is especially useful if you want to automatically apply the formula to new rows in the sheet.*

Array formulas should reference entire columns, for example: `=ARRAYFORMULA(YEAR(C2:C))`

When copying a formula, it’s often useful to keep either the source column number or source row number (or both) fixed. For example, if you are a value in one specific cell, such as the dollar cost average per engineer, you may want to reference that value in another formula. In this case, you would use an absolute reference like `$A$1`, which would always point to `A1`, no matter where you copied the formula. 

It’s also possible to use a column-relative or row-relative absolute reference, such as `A$1` or `$A1`. 

# Text Munging

These cover the majority of basic text manipulation. 

![](/blog/images/gsheet2.png)

Official documentation is here: 

- [CONCAT](https://support.google.com/docs/answer/3093592)
- [CONCATENATE](https://support.google.com/docs/answer/3094123)
- [SEARCH](https://support.google.com/docs/answer/3094154)
- [SUBSTITUTE](https://support.google.com/docs/answer/3094215)
- [INDEX](https://support.google.com/docs/answer/3098242)
- [SPLIT](https://support.google.com/docs/answer/3094136)
- [IFERROR](https://support.google.com/docs/answer/3093304)
- [MID](https://support.google.com/docs/answer/3094129)
- [LEN](https://support.google.com/docs/answer/3094081)
- [TEXT](https://support.google.com/docs/answer/3094139)
- [IF](https://support.google.com/docs/answer/3093364)
- [AND](https://support.google.com/docs/answer/3093301)
- [NOW](https://support.google.com/docs/answer/3092981)
- [YEAR](https://support.google.com/docs/answer/3093061)
- [TO_DATE](https://support.google.com/docs/answer/3094239)
- [DATEVALUE](https://support.google.com/docs/answer/3093039)

Honorable mentions: [REGEXMATCH](https://support.google.com/docs/answer/3098292), [REGEXREPLACE](https://support.google.com/docs/answer/3098245), and [COUNTIF](https://support.google.com/docs/answer/3093480).

# Import & Export CSV

Spreadsheets allow for easy import and export of CSV data. See `File → Import`. Importing CSV from some data source, manipulating it, and then doing a pivot table is like 20% of being a manager. 😉 

You can use `awk`, `sed`, and sometimes just `grep` to do some basic pre-processing of a CSV file. 

# Filters and Slicers

Selecting an entire sheet and doing `Data → Create a filter` is a handy single-stop shop to a bunch of easy sorting and filtering. 

![](/blog/images/gsheet3.png)


The resulting filters on each column are especially useful for filtering OUT rows that are not relevant. For smallish datasets, you can just un-select the values you want to filter out. 

![](/blog/images/gsheet4.png)


One common issue with filters is that they apply globally to a shared document. This is only what you want about half the time. The rest of the time, you DON’T want a filter that one user applies to their view of the spreadsheet to apply for everyone else. For example, if that user is filtering down to just rows that are relevant to them, such as their direct reports.

In this case, you can use a `Slicer` to define filters that do not modify the original document for other people. 

![](/blog/images/gsheet5.png)

# Pivot Tables

Think of a pivot table as a `GROUP BY` clause in SQL. It’s a general-purpose way to aggregate data and displays things like counts, averages, and sums by one or more dimensions. 

First, select the data you want to pivot on and go to `Data → Pivot table`. 

*Note: you can make sure that your pivot table will pick up new rows in the source sheet automatically by updating the data range to reference entire columns without specific row numbers, such as `A:C`. When you do this, you will likely need to add a filter to exclude blank values.*

![You need to select a destination, or have it create a new sheet for the results.](/blog/images/gsheet6.png)

Then use the editor to `Add` rows and values. Here, I’m summing Widgets by Team. 

![](/blog/images/gsheet7.png)

# VLOOKUP

If Pivot Tables are analogous to `GROUP BY` in SQL, then a VLOOKUP is like a `JOIN`. You can join to data in another sheet in the current Google Sheet document, and you can also join to external sheets. 

This is great for at least two use cases:

- You want to treat one sheet as a source-of-truth enum-like mapping, i.e. a translation table. 
- You want to reference another dataset either to not repeat data or because you know that the source-of-truth sheet will change regularly, i.e. a list of employees and their current manager. 

*Note: I always have to look up the syntax of this, and do some debugging. 😉 The key is to remember that you’re looking up the value* ***in the first column*** *of the range.*

The arguments of the `vlookup` function are:

1. The value to search for, i.e. `A1`
2. Where to do the lookup, again it’s going to look at the first column in this range
    1. You can provide a range in the local sheet like `A:C`
    2. You can provide a range in another sheet in the current document like `'Filters and Slicers'!A:C` (you must use single quotes if the name of your sheet contains spaces)
    3. You can provide a range in another Google Sheet document like `importrange("https://docs.google.com/spreadsheets/d/1YH6-gXcIix5Pql97nqVaENxN8rUbL-JarOrpnH9DNZT0/edit#gid=1511625096","employees!A:G")`
3. The index of the value in the range to return. If you find yourself wanting to return a value with a negative index (i.e. to the left of the range), you can instead change the range to an array where you reorder the columns. For example: instead of `A:C`, use `{'Filters and Slicers'!C:C,'Filters and Slicers'!A:A}`.
4. Returns exact matches only when set to `False` (you probably want this)


![](/blog/images/gsheet8.png)

# Conditional Formatting

Conditional formatting is just styling cells based on their contents. 

Common use cases:

- Use background color as a heat map to denote high/low values
- Use background color to highlight rows that are not filled out, i.e. you’re asking others to fill them in 
- Alternating row background colors for readability 

Start by selecting a range, and going to `Format → Conditional formatting`


![“Alternating colors” is how you format alternating rows with contrasting background colors](/blog/images/gsheet9.png)


Example of highlighting empty cells:

![](/blog/images/gsheet10.png)


Example of a heat map:

![](/blog/images/gsheet11.png)

# Conclusion 

If you use a tool long enough, you may begin to assume that everyone knows it as well as you do. I’m certainly not a spreadsheet master, but I’ve come to realize that some people barely use them at all. For me, they are the primary tool I turn to in many different contexts. It’s been said that spreadsheets could perform the function of 80% of custom software, and I would agree. For me, basic functional spreadsheet knowledge is right up there in terms of utility with writing SQL, or coding itself. 

---
title: "Setting Direction: Define a Vision"
tags: manager reading-list playbook
toc: true
header:
  overlay_image: /images/header-vision.png
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "This post is a collaboration with Jason Ramirez, Frances Klein, and Maria Handberg"
---

For a new team just starting to work together, an existing team starting on a new mission, or a team that is not excited about their work, going through the exercise of creating a formal vision can get them energized and aligned. Even teams that are executing well towards an informal vision can benefit from re-articulating and writing down a formal version.

## What is a vision? How is it different from a mission?

A mission statement is **one sentence**, in the format of an "end statement" of what will be true if the product is successful. The mission statement needs to be aspirationally possible but does not need to be constrained to a certain timeline.

The written vision statement is comprised of multiple end statements, is feasible to accomplish in a three-year timeframe, and gives specifics about how the user will accomplish various jobs to be done, without being overly prescriptive. A useful stylistic choice is to tell a story from the perspective of a user.

**Example: Netflix Streaming**

You can imagine a new initiative at Netflix circa 2008 to kick off the move to streaming.

Mission:

> Watch any movie or show, anytime, from any device.

Vision:

> Sally is waiting for a bus. She opens her phone, goes to the Netflix app, and browses thousands of TV shows and movies. She selects a show she's already watching, "The British Bake Off", and Netflix picks up where she left off. Because she's on cellular, the video quality is only 480p, but it starts playing right away. She doesn't have the video on her phone; it's all streaming live from Netflix servers. When the bus comes, she pauses and closes the app. Sally gets home from work and launches the Netflix app on her TV. She chooses "The British Bake Off" again, and picks up right where she left off. This time, the video is in full 1080p quality. She can pause, rewind, and resume whenever she wants. When she's done with "The British Bake Off", Netflix recommends another show, "Yelling at Chefs with Gordon Ramsey".

**Visuals are not necessary, but they help**

Believe it or not, circa 2008, the above vision statement may have been hard for even technical audiences to understand. If you've never streamed video before, you may not have any frame of reference for what it would be like, as a user. You almost certainly have never seen an app on a TV; what does that even mean?

Just like when we demo working software to users to get feedback, showing your audience something visual to help them understand the vision can help them vocalize their assumptions and put their requirements in concrete terms. Actual designs should not exist at this point; the team should iterate on those based on the vision, once it's set. But, these can be rough mocks, or even a video. You can imagine a commercial-style video of the above Netflix vision, following Sally around during her day, but not necessarily showing the exact UI of the Netflix app on her phone, or the TV.

## What is not included in the vision?

Pixel-perfect designs are not part of the vision. Likewise, product specs, technical specs, and the roadmap are not part of the vision. All of these necessarily come later, and flow from the vision.

What about business goals, or metrics? These can be included, in the form of end statements. For these, the end statements can be for the business, versus for the end user. For example: "Netflix will have 100m monthly subscribers for its streaming service", and "Netflix streaming subscribers will log in to the app on average 3 times a week".

## Why do you need vision?

Every team will have some form of shared vision, whether or not it's written down. You have to have a mental model of what you’re building towards, and what the goal is. If there is nothing formally defined, the team will invent their own individual versions, in their heads. Writing it down and saying it over and over helps make sure that individual's mental models are as convergent as possible.


> … capture and encode that vision, taking care to preserve all the richness and wonder and awe so that the other people who see it can also feel the thump-thump in their hearts and the resolve in their minds. — [Julie Zhou](https://medium.com/the-year-of-the-looking-glass/designs-north-star-d469193063c5)

The audience for the vision is not just the team working on the product. It's also the entire company, particularly executive management, and eventually external audiences as well.

For the team, the vision is about alignment, excitement, and engagement. The team members might actually be actively asking for a vision, even if they are not quite sure what a vision is, or why they want one. For the team, alignment means getting everyone rowing in the same direction. Excitement is good for its own sake, but can also motivate the team to build faster, and at higher quality. The team is hopefully engaged in coming up with the initial vision, and the vision also helps them stay engaged and connected to the work.

For the executive audience, the vision is about alignment for prioritization, goals, and funding. The apparatus of company decision making needs to agree to fund this project, which takes the form of a budget for the staffing of the team. The executive audience also needs to align on the business goals of the project, including the timeline of potential milestones. Those can typically come later, after the project is funded. The vision is what convinces stakeholders that sufficient business impact is possible in an acceptable timeframe.

Sharing a vision with anyone is also a great opportunity to get feedback. This is gold! Make sure to add a frequently asked questions section to the vision document to capture and address these.

## Who should create the vision? How?

The vision can come from anyone or any group. But, the closer you can get to the ideal of the actual team who will do the work generating the vision, the better. Hopefully, they have the best local knowledge of the product space, the users, and the technology. To the extent that they can own the vision, it will help them stay engaged and motivated. If you have to generate a vision tops-down, consider generating just the mission statement, or just the mission statement plus some "must-haves" pieces of the solution, as you see it.

It also helps to include a cross-functional set of collaborators. You want the working team for the vision to be as small as possible, but no smaller. Consider including just one person for each of the roles: product manager, designer, engineer. It's also reasonable to expand to include many engineers, especially if it's the entire team that will build the product. But, be aware of the cost of having too many people in a brainstorming session, and plan to run more structured sessions to account for that.

Something like a [Google Design Sprint](https://www.google.com/search?q=Google+Design+Sprint) is a good place to start, if you're looking for a small group or team-based vision exercise, and can afford to spend up to a week of the team's time doing it.
No matter who creates the vision, you should get some initial input from key stakeholders like executive leadership. What do they see as "must-have" pieces of the solution? Also, plan ahead to get feedback and incorporate the feedback into your draft of a vision, before finalizing it. You want to present your vision to at least three different audiences, listen closely to what they say, try to figure out when the audience does not understand the vision, and update the vision so that it better connects with the audience.

## How will we know when the vision is successful?

You will know that your vision is successful when an executive audience can paraphrase it back to you accurately, and when you can ask any team member what the vision is, and they will recite it back to you, word-for-word. Both of these are practically impossible, but that's the goal.
You will know that your team vision is compelling if you feel confident that you can send this document to anyone in the company, and they will get excited about the product.

Don't expect your vision to stand for time immemorial. Mentally be prepared to update the vision significantly at least every year. The team is going to learn new things as it builds, and the company and business context is going to change. A good outcome would be evolving the vision every year, to incorporate new information.

---
title: "Headcount Tracking for a Medium Sized Org using a Spreadsheet"
tags: managers process template
toc: true
header:
  overlay_image: /images/header-crowd.png
  overlay_color: "#000"
  overlay_filter: "0.01"
excerpt: "Don't do what I did, and spend 10 years headcount tracking in text documents"
---

While the simple question of "how many people work here?" should be easy to answer, even slightly more nuanced questions like "who is on which team?", or "how many people have we hired that have not started yet on these three teams?" can quickly turn into one-off manual fact finding and counting exercises. But, if you build a practice of documenting employees, new hires and open headcount in a spreadsheet, you can quickly answer such questions with accuracy.

This is probably most useful for organizations between 20 and 100 people. For smaller organizations, you can keep track of this all in your head and not miss anything. For larger organizations, you may run into problems keeping the data updated across many different teams and managers.

# Why use a spreadsheet?

Like any form of accounting, you need to start with raw row level data. In dual-entry accounting for finances, you track individual transactions, tally those into account balances, and double-check the balances against reality. If you **just** maintain a dollar balance figure, and you add/subtract from that directly, you will very quickly get to a spot where your balance is incorrect, but you are not sure why. For headcount tracking, this means tracking individual people. If you add up all the people rows, you can double-check the total count against another source of truth like Workday.

Once you have an spreadsheet of current employees (aka "butts-in-seats" or BiS), it's natural to layer in future employees you know about ("accepted-not-started", or ANS) and also open headcount ("Open HC"). That way, you can get a wholistic view of total staffing. A spreadsheet will also give you collaborative editing/sharing with other managers, arbitrary pivot tables to visualize things like headcount per team/sub-team/manager/role, and you can even fork off new copies of the spreadsheet for use cases like reorg planning or layoffs.

You can start to see why it's useful to maintain a separate, custom source of truth. For some of the use cases above, you're not just looking at reporting structure information, i.e. who reports to who. For example, you may want to slice by team, and teams can have individuals on them reporting into different managers. This is one of many use cases where popular tools like Workday do not give you a complete picture, of even the full set of individuals. Instead, I recommend compiling this spreadsheet once, and then regularly updating it with managers in you organization.

# Creating the Spreadsheet

Hopefully, you have some source-of-truth system that you feel confident will give you an export with the full, correct set of individuals. Maybe you need to export several data sets and merge them. Either way, this is worth doing if you're talking about an organization with more than a few teams worth of people. It's better to start with a complete dataset and massage it, than to start with an incomplete dataset and be missing people. Eventually, you will also want to add ANS and Open HC rows, which will often come from other systems, or be manually entered.

You will likely be able to copy & paste rows of email and name. Much of the rest will need to be hand-entered. The "fill" feature of your spreadsheet can be useful here, for repetitive data entry. This is another reason why this system probably will not work with 500+ employee organizations; you likely do not even know the team/role of each person! Granted, you could crowd source that, once. Either way, future incremental updates will be much more lightweight.

## Example Template

Here is an example template in Google Sheets: 
[Headcount Tracking for a Medium Sized Org](https://docs.google.com/spreadsheets/d/1vpIOK44wH6oJxlnl6Owqvu1brzztuH8G2syyLdGRGYk/edit?usp=sharing). You can fork a copy with File -> Make a copy. *Note: these names are from a [random data generator](http://www.randat.com/)*.

![Screenshot individuals sheet](/blog/images/headcount-spreadsheet-individuals.png)

![Screenshot pivot sheet](/blog/images/headcount-spreadsheet-pivot.png)

## Columns

You can add any columns you want of course, as long as you're willing to do the data entry. These are some fields which I have found useful.

- Email address gives you a unique identifier, and allows for copying/pasting the entire column for use cases like sending an email, creating a Google Group, or creating a Slack channel
- Full name for the sake of humans reading the spreadsheet
- The name or email of the person's manager, for filtering, especially when you ask individual managers to make sure their rows are complete and up to date
- Team, which may not be a 1:1 mapping by manager, and is the grouping you will most often want to visualize by
- Team+1, Team+2, etc, can be added to support arbitrary levels of groups in your org structure. Team+1 is the parent organization that a Team is in, if you have such named structures.
- Role is useful for making sure that you're not missing a cross-functional team member like a product manager or a designer, and also for the case when headcount is allocated separately by role
- Optional: Start and end dates for tracking ANS and attrition
- Optional: Req ID/Link for linking open headcount to external applicant tracking systems---
title: "The Software Engineer's Easy Mode Guide to US Tax Withholdings"
tags: personal-finance
toc: true
header:
  overlay_image: /images/header-tax-withholdings.png
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Set withholdings to zero, and pay up to an extra 15% of your bonus and RSU vest in the form of quarterly estimated taxes"
---

For years I've struggled with trying to minimize the difference between taxes withheld and taxes owed. It's quite complex, with salary, bonus, and RSUs changing every year. That is, until I had a breakthrough. The year after I made quarterly estimated payments for the first time, it finally clicked.

*Note: this blog post is only talking about your federal tax liability, not state taxes, social security, or medicare. That's because the federal taxes are the largest chunk out of your paycheck, and are generally the only piece you can or need to plan for.*

# Definitions and Background

Somehow, I had muddled through for about 20 years, filing taxes and not really understanding what was going on. That's possible for most people, thanks to the magic of automatic withholding. All the same, it's better to actually understand some of the basic terms and concepts. Here is what I learned, slowly.

- **Withholdings** are deductions from your paycheck that go immediately to the federal government to cover taxes.
- **Gross income** your total salary, bonus, and RSUs, for the year, before taxes.
- **Tax liability** is how much you actually owe in taxes, for the whole year. *How much you withhold does **not** change how much you owe*; it will get equalized when you file your taxes at the end of the year.
- **Marginal rate** is the percentage of the last dollar you earn that you owe in taxes. This is different from the first dollar. That's because we have a graduated tax system with [tax brackets](https://www.investopedia.com/terms/p/progressivetax.asp).
- **Effective rate** is the percentage of your tax liability versus your gross income.
- **W4** is the form you file with your employer that tells them how much to withhold from your paycheck.
- **RSUs** are one of three buckets of compensation for many software engineers. In addition to your regular salary, and your annual bonus, RSUs are quarterly stock vestings that you get taxed on every quarter, when you receive the stock, even if you don't sell it.

# The easy way to calculate what you owe

Your regular W4 withholdings will cover the full tax liability of your salary alone. For example, if you have no bonus or RSU vest, and you fill out the W4 correctly, you will withhold the correct amount from each paycheck so that the difference between taxes withheld and taxes owed for the year is negligible.

Things start getting tricky with bonus and RSUs. Early in your career, these are small. Over time, they become a larger and larger part of your compensation. For taxes, that's a problem, because **bonus and RSUs are not withheld at the correct rate -- you owe more!**

> Your W4 withholdings are designed to account for the tax liability on just your salary. Bonus and RSUs are withheld at a flat 22%. You need to account for the delta between 22% and your marginal tax rate, by using either extra W4 withholdings from each paycheck, or with quarterly estimated taxes.

You use marginal rate here, instead of effective rate, because you will be paying the most for these "last dollars". Another way to think about it is that you have already paid bellow your effective rate for your salary dollars. Let's assume that your marginal tax rate is the maximum of 37%. Your W4 is withholding enough taxes to cover your salary piece, but you need to make your own plans to cover difference between 37% and the flat 22% for your bonus and RSUs combined, or 15% of those pieces.

*Update: I ran this for my 2022 taxes. It turns out that this method gets you close, and it is guaranteed to be conservative; i.e. you will over pay and get a refund. In my case it was high by about 10%. This is because of the way that tax brackets work. I suggest you over pay the first year, and adjust down from there.*

# Three ways to pay what you owe

## Pay at tax time

The first method to pay is to wait until tax season. It's not uncommon for folks to get RSUs for the first time, and be surprised when they start filing taxes and figure out they owe tens of thousands of dollars in additional taxes. The IRS will also charge you an interest penalty if the difference between withholdings and tax liability is large enough. For me, it was more about not wanting to be surprised.

## Extra W4 Withholdings

The second method to pay is with extra W4 withholdings. The W4 form was updated in 2020 to allow you to input any amount of "extra withholdings", as a dollar amount. So, you can add your bonus and RSUs for the year together, multiple by 15%, and divide by the number of pay periods (probably 24) to calculate your W4 extra withholdings. You file an updated form with your employer, and that should be it. Just double-check your next paycheck, and look for the "extra withholdings" label.

W4 extra withholdings have the benefit of being adjustable throughout the year. But, as the percentage of your total compensation that comes from bonus and RSUs goes up, this could be a significant portion of your paycheck. In my case, it could be up to 1/3 of the gross amount, just the for **extra** federal tax! Combined with all the other deductions, it could easily eat up most of the paycheck. You could end up budgeting your annual bonus plus your quarterly RSUs to cover your month to month spending, which is the opposite of what most people want to do.

## Quarterly Estimated Tax Payments

The third method to pay is quarterly estimated tax payments. It's not scary; it takes about 10 minutes a quarter, and you pay online. You don't need to fill out any forms or paperwork. The main downsides are that you need to remember to pay them, you need to save the money from your bonus and RSUs to pay them, and you need to make four **equal** payments for the year. The equal payments piece is a bummer; it means you need to make the most accurate calculation possible at the beginning of the year, and you cannot adjust as you go.

# Double check your work

An easy way to double check your work is:

1. Add up your total salary, bonus and RSUs for the year
2. Use an [effective tax rate calculator](https://equitable.com/retirement/products/variable-annuities/investment-edge-annuity/estimate_your_effective_tax_rate) to find out how much you will owe
3. Look at a paystub and project how much is being withheld from salary
4. Calculate the 22% withholding for bonus and RSUs
5. The difference between #2 and #3 + #4 should match your estimated tax payments for the year

# How does this change for a married couple?

Not much changes, just set both of your withholdings to zero, and add your bonuses and RSUs together for the 15% calculation.

# Why not use a CPA?

Feel free! But, it's not really that complicated. If you've used Turbo Tax before, you have proved you can do it. A CPA will actually just use a custom tool very much like Turbo Tax to file your taxes for you. All of the withholding business is just an optimization. It's also about **understanding** how it works. No one is ever going to care about your money as much as you do. Also, if you don't understand it, how will you know if the CPA is doing it correctly?
---
title: "How to run a Retrospective"
tags: process agile manager scorecard
toc: true
header:
  overlay_image: /images/retro-start-stop.jpeg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Retrospectives are a good catch-all for team health, bonding, and continuous improvement"
---

A retrospective is a team meeting following a sprint, a quarter, or a specific project. The team talks about what went well, what didn't go well, and what they could do better. Everyone has a chance to voice their ideas on how to improve the team. Over time, the team builds a set
of best practices that they truly feel ownership over. You may even surface some feedback that
you never would save seen, otherwise. 

As an engineering manager, my teams always did retrospectives every two weeks, at the end of a sprint. I've probably been in something like 250 retrospectives, and I've run about 80% of those. Running a retrospective is easy. If you're looking to get started as a facilitator, this blog post should get you up and running quickly. 

*Note: the manager's role in a retrospective is to facilitate the conversation, not steer it. You want to be cautious about offering your own ideas. Besides facilitating, the most useful thing you can do is make sure the action items get completed.*

# Basic Retrospective Format

Your first job as a facilitator is to create psychological safety. I like to include this quote in the agenda doc, or on the calendar item itself. 

> Regardless of what we discover, we understand and truly believe that everyone did the best job they could, given what they knew at the time, their skills and abilities, the resources available, and the situation at hand."
-- [Norm Kerth, Project Retrospectives: A Handbook for Team Review](http://retrospectivewiki.org/index.php?title=The_Prime_Directive)

The most common retrospective format is brainstorming, voting, discussion, and action items. Here is a sample agenda for a 60 minute team retrospective:

1. Give the team a prompt, for example "How could we be moving faster?". Write the categories Start, Stop, and Continue on the whiteboard. 
2. Silent brainstorming, team members write ideas down on sticky notes (10 minutes) 
3. Team members take turns putting their stickies on the whiteboard, quickly summarizing each one (10 minutes)
4. Silent grouping into themes, for example "Testing" (5 minutes)
5. Voting on top themes, everyone gets 3 votes, put stickers or use a whiteboard marker to vote on a theme (5 minutes) 
6. Discuss the top 3 themes, and generate one action item per theme (25 minutes)

![Start, Stop, Continue Retrospective Whiteboard](/blog/images/retro-start-stop.jpeg)

## Variations on Prompts

This is the most effective way to steer the retrospective, as a manager. If they team is having issues reliably executing, the prompt should be some variation of that. Other prompts:

- How could we increase the quality of our work? 
- How could we be working together as more of a team? 
- How could we run more efficiently? 

## Variations on Categories

Most retrospectives will follow this basic format. To keep things interesting, the facilitator can change up the brainstorm categories. 

* Wishes, Complaints, and Puzzles 
* Headwinds, Tailwinds, and Anchors 
* Glad, Sad, and Mad 

## Action Items 

As the facilitator, you want to steer the team away from having too many action items. 
You want the team to commit to actually completing a small set of action items, versus 
having an exhaustive list. Remind the team that they will be able to generate more action
items next retrospective. 

Common action items:

- Update the team's [working agreement](https://medium.com/ideas-by-crema/how-to-create-an-agile-team-working-agreement-7bc98df008a1)
- Update the team's [definition of done](https://www.leadingagile.com/2017/02/definition-of-done/)
- Update the team's user story, test plan, or deploy checklist template 

# Alternative Retrospective Formats

If you're looking for additional ideas, I recommend the book [Agile Retrospectives: Making Good Teams Great](https://www.amazon.com/Agile-Retrospectives-Making-Teams-Great/dp/0977616649). It covers not only many different brainstorming prompts, but also entirely different agendas and formats. 

## Sailboat Retrospective

In a "sail boat" retrospective, teams that find themselves in dangerous waters imagine challenges they face as anchors, headwinds, and treacherous rocks. 

![Boat Retrospective Whiteboard](/blog/images/retro-boat.jpeg)

## Timeline Retrospective 

In a "timeline" retrospective, teams look back at a long term project that is wrapping up, 
and try to recall what happened when, and how they felt about it. 

![Timeline Retrospective Whiteboard](/blog/images/retro-timeline.jpeg)

Looking for even more ideas? Check out [Retromat](https://retromat.org/) which can generate
a random retrospective agenda for you. 

# Agenda Add-ons 

If the retrospective is your only reliable, frequent whole-team meeting, you might want to include some additional agenda items.

* Reviewing the previous sprint, what was shipped, and what slipped? This can help generate ideas for the brainstorm. 
* Kudos! Go around the room and thank each other for something specific. This can help break the ice and prime individuals to share. 
* Do a light anonymous team health survey. How are you feeling from 1-10 about a few key questions. Record the scores over time. 
* Quick +/-/delta about the retrospective itself. What should we do differently next time?

# Tooling for Virtual Retrospectives

Most of the retrospectives I have been involved in were in-person, using sticky notes and a whiteboard. Virtually, you can replicate that experience with something like [stickies.io](https://stickies.io/). You could also use Miro, Trello, or any collaborative text editor. 

![Miro Retrospective Example](/blog/images/retro-miro.png)

# The Retrospective Scorecard 💯

Is your team following best practices? 

- [ ] Retrospectives happen every sprint 
- [ ] The whole team is present 
- [ ] Generate a small number of action items 
- [ ] Previous action items are completed ---
title: How I use OmniFocus Get Things Done
tags: reading-list gtd manager
toc: true
header:
  overlay_image: /images/omnifocus_inbox.png
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "This is my personal system for staying on top of everything."
redirect_from:
  - /2015/05/01/omnifocus.html
---

*Updated June, 2022.*

[Getting Things Done](https://en.wikipedia.org/wiki/Getting_Things_Done) is a personal organizational system for 
keeping track of todo items, or tasks. Being a manager can mean being bombarded by asks, notes, ideas and action items
all day. Not to mention emails, Slack messages, and even in-person requests. Sometimes, you have a great idea during a
1:1, while walking the dog, or even taking a shower. 

This could be overwhelming. Say you're still thinking about that thing your boss said to you in a 1:1, and whether you
need to take an action on it, and you get an emergency Slack from a direct report. How are you going to remember the 
thing from your boss? How are you going to be *sure* you remember, and take action if needed? 

That's the real power of Getting Things Done. With GTD, you capture the ideas and asks as soon as you hear them. The 
real power of GTD is in helping you focus, and giving you "permission to forget" (for the moment). If you trust your
system, you can be confident that if you capture it, you will take action on it. Paired with a daily practice of 
process the "inbox" of captured tasks, and heads-down time to actually do some of these tasks, nothing will ever 
slip through the cracks again.

For more on the theory of GTD, I recommend starting by reading the classic [GTD book](http://gettingthingsdone.com/). 
It's 20 years old, but just as relevant today. The book was originally written for a pen and paper capture system,
but here is one way to translate it to modern digital tools.


## Daily Inbox Processing in OmniFocus

Personally, I use [OmniFocus](https://www.omnigroup.com/omnifocus) as my GTD tool. I prefer open source or free tools when possible, but this is one purchase that's worth every penny. You can also expense it to work.

Here is an example combined daily inbox processing and truncated heads-down working session processing tasks. 
<a href="https://youtu.be/VT3r0OLRtSo">https://youtu.be/VT3r0OLRtSo</a>

<iframe width="560" height="315" src="https://www.youtube.com/embed/VT3r0OLRtSo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


## How to Capture Tasks in OmniFocus

I enter tasks in my inbox whenever I think of them. This is most often during a 1:1 or meeting. I used to feel
slightly self-conscious about taking notes like this while in the middle of talking to someone, but I did it 
anyway. Otherwise, I would forget! These days, the meeting is often over Zoom anyway, and it feels more natural. 

Just as often I'm capturing ideas on my phone, or totally outside work. One of the things I love about OmniFocus
is that it's so quick to capture ideas -- there are no unnecessary steps or buttons, and no lag. I even have a 
Shortcut set up so I can ask Siri to "add something to my work todo list" from my watch. 

![omnifocus add](/blog/images/omnifocus_add.png)

Once I've quickly captured the task or idea, that's it. I give my attention back to something or someone else. 
The following morning is usually the next time I will look at any given item. I typically process my inbox first 
thing in the morning. If something will take less than two minutes, I do it right then. Otherwise, I assign it 
to a project. If the task has a natural due date, I also schedule it. 

![inbox](/blog/images/omnifocus_inbox.png)

During the day, in between meetings or during free time, I start completing items and checking them off. But that's
a nice-to-have. What is absolutely required is heads-down time to do real work. That's when the bulk of the tasks
on my list actually get completed. For me, it's about 90 minutes a day, ideally in one big chunk. I block this off 
on my calendar so that it's free every day, and I defend that time aggressively. 

You can organize tasks into projects, favorite them, and even use tags. The logistics are not important, my 
personal organization tends to evolve over time. Completing a tasks often involves work like sending an email, 
writing a doc, or talking to someone over Slack about a decision. But it can also be something simple like adding
an item to the agenda for an upcoming 1:1, or meeting. 

The last major piece of the system is the weekly review. For me, this is 30 minutes once a week. I look at all
my projects, and brainstorm whether there are more actions in the form of tasks to add. This is also when I tend
to realize that a project may be stuck, or actually isn't that important anymore. For me, this time is usually the last
thing on my calendar for the week. 


## Recurring Task Hacks

I make extensive use of recurring tasks. These as tasks that once you "complete" them, they spawn an identical 
task at some configurable time in the future. I have a "Daily" project with a few of these tasks, such as 
processing my inbox, or updating the agenda for the meetings coming up tomorrow. 

![omnifocus_repeat](/blog/images/omnifocus_repeat.png)

I also use these repeating tasks as a "super power", to very reliably execute small tasks consistently, over time.
This is a type of work that many people are not capable of doing, at least over a long period of time. Without
a very good system, there is a tendency to forget or get distracted by other things. With my system, I know I will
keep doing something reliably, until I decide it's no longer a priority. I've used this for things like doing a
regular talent review for my area every 3 months, or keeping the headcount tracker up to date. 

I've also used this system try to push small culture wins over time, almost imperceptibly. For example, I might 
post to Slack every week something I'm excited about, or want to celebrate. Most people can do this once, or a 
few times. I can do it every week for a year or more, no problem. 

Finally, I've used recurring tasks to hack myself. If there is something I'm trying to learn to do better, or 
just a mind shift I want to make, I often elevate it temporarily to a daily task. For example, at times I've had
daily tasks that said "Smile more", and "Remember to think about how other people feel". 


## It's OK to Delete 

I capture everything. Even things that end up not being good idea, or are not actionable. Maybe it's decent idea,
it's just not a high priority. Over time, I've developed confidence to delete these items. Whether it's a project 
that's been languishing in my backlog for weeks with no actionable work, or a task that I've been procrastinating 
on, almost 50% of the items I enter get deleted, not completed. What I've realized is that 
these are not important. By virtue of not rising above other tasks or projects, over time they have settled into  
their naturally low priority spot. Often they are completed simply by telling someone else you're not going to do
them, after all. 


## The Magic is the Practice 

The practice of having a dedicated time to process your inbox is the key to GTD. 
The magic happens when you start to trust that if you capture something, it will get done.
For work to happen, you still need dedicated heads-down time. That time is precious -- you 
owe it to yourself to have a system that makes the best use of it. ---
title: "How to Run a Zoom All Hands"
tags: manager checklist
toc: true
header:
  overlay_image: /images/header-all-hands.jpeg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "If you’re working in corporate America, you’ve probably been to a lot of All Hands presentations."
---

All Hands are a key tool for communicating big points, giving the audience the best chance to hear and internalize those ideas, and creating transparency. The ideal All Hands also generates enthusiasm, though this is easier said than done with Zoom. If you’re a leader of an organization, you should plan on doing an All Hands roughly once a quarter. 

# Initial Setup 

Before doing anything else, lock down a date and time, and put it on the calendar. An All Hands has so many attendees, that you probably will not be able to find a time when everyone is free. Instead, make sure you’re clear of key conflicts like an all-team offsite, and otherwise let attendees schedule around you. Putting a date on the calendar 2-3 weeks ahead of time is also a good forcing function for actually nailing down an agenda, versus procrastinating. 

As part of the calendar item, create the Zoom link. You should configure the Zoom meeting ahead of time to enable cloud recording, close captions, alternative hosts, and mute-by-default. 

You can also add an optional Questions & Answers (Q&A) link to the calendar item. This could be just an empty doc, an anonymous Google Form, or you may have a fancy vendor tool for Q&A. Allowing people time to submit questions ahead of time generates more questions, and also gives you a chance to polish some of your answers. 

Some people do not read every meeting invitation that flies by. In addition to sending the calendar item, you should also announce the All Hands over Slack, or email. 

# Slides 

You can use any software for this, as long as all presenters can access and edit the document. You should avoid switching between different software during the presentation, if at all possible. Look for a recently updated template for presentations at your company, and start with that. Add your co-presenters to the document right when you create it. Flesh out a basic agenda with empty slides, and go from there. 

Once you have a rough draft you’re happy with, you should share the document with a subset of the leads from the org. This can help catch any mistakes in the content, and also gives you some heads up on likely reactions and feedback from the org. 

## Self-Serve Links

Your first slides are probably going to be an intro slide that shows while people are still joining the Zoom and the agenda slide for the All Hands. You should also include an easy-to-type URL for the presentation itself, right on the screen. Attendees should be able to join the Zoom, and type in the URL for the document, if they want to follow along. This doubles as a way to access the presentation, later. You should also include this URL on the last slide. Individual slides can also have their own easy-to-type URLs, where appropriate. 

## Slide Show

For the intro slide, I like to create a slideshow of recent team photos and pair it with some energizing music. Seeing themselves is a great way to get people engaged. An easy way to create this slideshow is to ask the entire team for photos up front, then use the [Apple Photos slideshow feature](https://support.apple.com/guide/photos/create-slideshows-phtae8c6d40/mac) to export a video file and embed it into the presentation document. If the video is relatively short, consider looping it and not embedding the music audio. Instead, you can play audio separately from Apple Music, or similar. Either way, you need to check off the box to “share computer audio” in Zoom, when you share your screen. 

# Agenda 

For the agenda slide, a simple bulleted list of 4-5 topics will suffice. Typical agenda items include an ice breaker, new people who have joined the org, a reminder of the org mission, recent wins, and upcoming initiatives, and Q&A. An hour is a good amount of time to plan for an All Hands, with one-third of the time for Q&A — so plan for 40 minutes of content. 

## Ice Breaker 

You want to maximize the opportunity to get people engaged, right at the start. Nothing is engaging as participation. The easiest ice breaker for a large group is to use Zoom breakout rooms, and give people a prompt of something to talk about. Five people for five minutes is a good rule of thumb. 

Easy conversation prompts:

- What did you have for dinner last night? 
- What movie did you recently watch? 
- What’s one surprising thing you learned recently? 

Another easy ice breaker is to prompt people to share a recent photo from their phone, in Slack. There are also web apps that are specifically for ice breakers, like [gatheround.com](https://gatheround.com/). 

## New Faces 

Shouting out to new folks who joined the team is also a good way to get people engaged. Plus, it lets you highlight that the team is growing. This is one slide that you want to crowdsource, to make sure you don’t miss anyone. You should include new hires, anyone who has recently transferred onto the team, and any new cross-functional partners. 

If one of the new faces is a leader for the organization, consider having a dedicated slide for them, and a few minutes set aside for them to introduce themselves. 

## Mission 

Key messages should be repeated over and over again so that everyone can internalize them. Having one slide here with the mission statement for your organization is one way to make sure everyone stays aligned. 

## Org Chart 

As a follow-on from the mission, a simple slide with “this is us” and “this is where we fit into the larger organization” can help orient new people. 

## Recent Wins

You should recap any big wins that have occurred since your last All Hands. These can be launches with business impact, new learnings, or social events that happened inside the org. A quick demo of a new product experience can be a good use of five minutes. In all cases, try to have the individual in org who led the effort present the win themselves. Make sure that the credit for each win goes to the right individuals and teams. 

## Upcoming Initiatives 

This can cover any other things that are happening, which everyone should know about. Maybe you had a great Hack Week, or you just started doing org-wide demos, or off-sites. Again, have the individuals most responsible for the work speak to it. 

## Product & Strategy 

All Hands are a great time to introduce anything new that everyone needs to be on the same page about. You should include any large product efforts coming up, or any changes to org strategy. If you have visuals, include those. 

This can also be a good section to include a Zoom poll. Ask people before and after this section whether they understand the product strategy of the org. Usually, you will see an improvement. In any case, it’s a good way to keep people engaged. 

## Swag

If you’re going to give away some company-branded merchandise, All Hands are a great time. Reward people who showed up and stuck around until the end of the presentation. This can look like a Google form where people sign up for the specific swag that they want, and enter their mailing address if needed. 

## Q&A

This section is often the most valuable part of the All Hands. If you put out a call for questions ahead of time, start with those. But also take live questions from the audience. If you don’t have pre-submitted questions, you may want to “plant” a question asker in the audience, to get the ball rolling. Often people are hesitant to be the first one to ask a question. If all else fails, you can always “ask yourself” a question. 

Make sure to set aside a lot of time for Q&A. It’s frustrating as an audience member when the regular presentation time runs over, and time for Q&A is short. A good rule of thumb for a one-hour All Hands is 40 minutes of content, and 20 minutes of Q&A. You should practice presenting your content as a group, to check whether it’s less than 40 minutes. 

# Avoiding Technical Difficulties 

You have probably been in All Hands where the presenters could not get their audio/video working, control the slides properly, play a video, etc. It’s harder than you think! What you don’t want to be doing is debugging any technical difficulties with a live audience. 

You should schedule a dry run with all the presenters. Book this ahead of time to be immediately before the actual All Hands. This lets all the presenters use the same laptop and Zoom ID that they will use for the real deal. It also makes sure that none of the presenters are late to the All Hands, due to a prior meeting running over. Don’t skip the dry run! Even if you’ve done a thousand All Hands, I promise this will catch some snafu. 

During the dry run, test that the actual Zoom ID for the All Hands is working. Sometimes these get deactivated, potentially if the owner of the Zoom or the calendar item has changed. Make sure all the presenters can join, and toggle their audio and video. 

Plan to have all presenters join from their laptops. Don’t attempt to mix presenters on laptops with a conference room A/V setup — this is asking for disaster! Conference rooms, conference room cameras/mics, and specifically jacking a laptop via a cable into a projector are the primary sources of technical difficulties. People should use the tools they are most familiar with. Plus, if any attendee is on Zoom, having all the presenters be on Zoom is the most inclusive option. 

During the dry run, decide whether you are going to have one person drive the slides, or whether you are going to pass around control during the presentation. I have yet to see either the Google Slide feature or the Zoom feature for this work. Even if it works — presenters usually get confused and mess it up. The safest option is to have one person drive, and use audio cues like “next slide please”. It’s not a big deal — the audience is used to it. The most important thing is not to get flustered. 

To share the actual presentation, enable slideshow/presenter mode in the software, and use regular Zoom screen sharing. Keep in mind that if your presentation is full screen for you, you will not be able to see Slack/Zoom chat, etc. Having a second monitor is a good solution. Otherwise, you can share a dedicated browser window (remove the other tabs). 

Right before the All Hands starts, remind folks in Slack that it’s about to begin. 

# Presenting 

Hopefully, you’re relatively good at this, already. The most important things are to speak loudly and clearly and to be enthusiastic. Trust the content that you’ve put into slides to tell the story — don’t feel like you need to read each bullet point. 

Enthusiasm is so important, that you should tailor your slides for it. Put something ridiculous on a slide, if it’s going to make you as the presenter smile. Jazz up the wording of a bullet point so that you’ll have an impish grin on your face when you say it. In general, have less text on the screen than you think you need, and also say less, and speak slower. 

When you’re not presenting, give your co-presenters a boost by dropping encouraging emojis into Slack, or using Zoom reactions. 

# Afterward

Remember to send the presentation out to all attendees after the All Hands, along with a video of the Zoom. So many people forget to do this! Try setting a reminder for yourself ahead of time, so that you don’t forget. 

If you promised to follow up on something, or answer a question, make sure to do it. 

# All Hands Checklist 💯 

Here is a quick checklist for planning an All Hands, to make sure you don’t forget anything. 

- [ ] Calendar item created for All Hands 
- [ ] Calendar item created for dry run immediately before the All Hands 
- [ ] Calendar item has Zoom ID 
- [ ] Zoom has recording enabled 
- [ ] Zoom recording is set to “cloud”, not “local” 
- [ ] Zoom has close captioning enabled 
- [ ] Zoom has mute by default enabled 
- [ ] Zoom has alternative hosts set 
- [ ] Calendar item has Q&A link 
- [ ] Announce the All Hands a week ahead of time via email/Slack 
- [ ] Set a reminder to send out the video recording after the All Hands 
- [ ] Request for team photos for the intro slideshow sent 
- [ ] Find an up-to-date template for presentation decks 
- [ ] Deck created
- [ ] Deck shared with presenters (can just be an empty draft)
- [ ] Deck has an easy-to-type link to itself on the first + last slide 
- [ ] Create an intro slideshow + pick an intro music track 
- [ ] Pick an icebreaker 
- [ ] Crowd-source “new faces” slide for the deck 
- [ ] Share deck with key org members for feedback 
- [ ] Dry run: test Zoom ID 
- [ ] Dry run: have presenters join individually from their laptops 
- [ ] Dry run: test sharing the intro video + music 
- [ ] Dry run: test switching presenters 
- [ ] Send out a reminder announcement just before the All Hands starts 
- [ ] After: send out a link to the deck + video recording 
- [ ] After: send separate communication for any big announcements, don't assume everyone saw All Hands ---
title: Let's Work Together
tags: newboss
toc: true
header:
  overlay_image: /images/header-alameda-beach.jpg
excerpt: "The first step is getting to know each other"
redirect_from:
  - /2018/05/14/care-and-feeding-of-chase.html
  - /2018/05/14/lets-work-together.html
---

I started coding in middle school and got into web stuff after Apple World 1996.
I did some freelance projects during college at Boston University, where I majored in
Computer Science. I went to a tiny startup called [Bullhorn](https://www.bullhorn.com/) after graduation. It
was a 10 person company that grew to 300+ by the time I left. I did everything from coding 
a custom email server to racking physical machines in a data center.

At two subsequent companies, I focused on Python back-end work. I also
transitioned to be a full-time manager. I found that managing people is the
way that I can have the most impact. Specifically, I can help promote healthy
relationships, excellent communication and high-leverage work on the team.

I fell in love with mentoring new engineers and first-time managers. I geeked out on
culture and process initiatives like hiring, promotion process, tech talks,
and blogging.

Outside work, got into basketball, ultimate frisbee, RPG video games,
photography, gardening, personal finance,
[reading science fiction](https://www.goodreads.com/user/show/9297327-chase-seibert)
and [cooking](https://www.dropbox.com/s/j16y65jm26780n8/cookbook.pdf?dl=0).

## Principles

Management is a constants stream of decisions, with increasing ambiguity as your
scope goes up. When I doubt, I try to optimize for these three things. 

### Autonomy

Teams will be most successful if they can determine their own solutions, and set 
their own priorities. Leadership can help by setting context on the WHY, and may 
have concrete suggestions on the WHAT. The team doing the work should flesh out
the WHAT, and come up with the HOW. The more that leadership leans in to defining
the WHAT, the more responsibility they have to articulate the WHY. 

### Healthy Relationships

Teams will be most successful if the individuals trust each other completely. 
Building this trust is job number one for an engineering manager. The engineering 
manager, the product manager, and the designer should be sharing information openly 
between them, assuming good intent, and in general alignment about the WHAT. In 80% 
of cases, serious constructive feedback between leads on a team is the responsibility 
of those leads to fix themselves. 

### High Leverage

Teams exist to produce impact. The single biggest lever for impact is picking the 
right projects. This means being correct more often than not about which projects will
have the most impact, and scoping the work on those projects to most efficiently realize 
that impact. 

## Frequently Asked Questions

### What are your expectations for your direct reports?

- Grow healthy relationships between your product partner, your design partner, and your direct reports.
- Hit your sprint and quarter level commitments 90% of the time.
- Personally set direction on one or two things a year that have impact.
- Come to 1:1s engaged, and with a written list if stuff to talk about.
- Follow up proactively on action items.
- Excellent written communication.

See also [My Expectations for Managers](/blog/2022/12/21/my-expectations-for-managers.html)

### What can you expect from me, as a manager?

- Reliable 1:1 time, every week. I will come with things to discuss.
- Transparent and timely information passthrough, when appropriate. 
- If I ask you to attend a meeting, I will make sure the meeting is well run.
- Autonomy to make your own decisions almost all the time.
- Flexible work hours, location, and vacations.
- I will almost never ping you outside business hours.

### How would your team describe you?

Unflappable, practical, informal, disciplined.

### What is your biggest weaknesses?

I need to write things down, otherwise I will drop the ball on things. 

### What are your biggest strengths?

Organization, process, attention to detail. I excel at building healthy relationships.

### What's the most surprising thing you've learned about yourself?

I'm introverted, but I can get energy from spending time with people as long as
I know them well. I find that it takes a few months to get comfortable with a new
team or business partner, and six months to build full trust. 

### What do you need to be successful?

- Weekly 1:1s with my boss, with written notes.
- As much transparency as possible, especially any context from executive leadership.
- Meetings that start on time, stick to an agenda, and have written notes.
- Dedicated blocks for heads-down work, and understanding if I decline meetings during those hours.

### What motivates you?

I'm motivated by projects and initiatives where I can:

- Set long term goals
- Take my time to think things through
- Collect evidence
- Refine a process iteratively
- Offer guidance to others

### Have you taken any of the various "work personality tests"?

So many! Even if none of these are very scientific, I find that
they are good conversation starters, and can be a good shorthand
for how to work together.

- Enneagram 8
- Myers Briggs ISTJ
- Insights BRGY
- Strengths Finder Analytical, Focus, Discipline, Futuristic, Relator
- BlueEQ 70%
---
title: "Thanksgiving Epic Estimation Workshop"
tags: process scrum workshop
toc: true
header:
  overlay_image: /images/header-thanksgiving.jpeg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Epic estimation is as much about best-effort definition of scope and milestones as a cross-functional team, as it is about putting estimate values on roadmap items"
---

I've given this Agile planning workshop a few times. It's Thanksgiving themed, because that is the time of year when we are making plans for the next year. It can help decide what some reasonable commitments are for large, multiple quarter efforts. 

Send out these pre-reads ahead of time:

- [Getting Started Estimating with Story Points](https://chase-seibert.github.io/blog/2019/01/30/getting-started-with-story-points.html)
- [Estimating Epic Stories in Three Steps](https://chase-seibert.github.io/blog/2017/08/28/epic-story-estimation.html).

----------

# What is this workshop for? 
(1 minute) 

In this workshop, we are going to learn how to estimate Epic level roadmap items. For example, these could be the primary projects for a team of engineers for an entire year. 

## When Estimating Epics

- We acknowledge that the estimates are likely to be wrong by up to 50%
- We don't know the exact scope of the Epics, i.e. we probably do not yet have product specs, or mocks 
- We want to limit the time we spend on this estimation

*Note: This is designed to teach the fundamentals of Epic estimation in a hands-on way, using analogies to real-world events that everyone is likely to be familiar with. We're not going to actually estimate any actual software engineering Epics.*


# Review Story Points Fundamentals 
(5 minutes)

We're going to practice basic blind-voting using story points in a few minutes. First, let's quickly recap [Getting Started Estimating with Story Points](https://chase-seibert.github.io/blog/2019/01/30/getting-started-with-story-points.html). 

Story points allow for quickly crowdsourcing low-granularity estimates, by leveraging: 

- Wisdom of Crowds — estimation is a team sport
- Relative Estimates — estimate relative to recently completed stories
- Risk Assessment — identify unknowns and use them to increase the estimate 

## Why not time-based estimates?

- Time-based estimates are less psychologically safe; “2 to 4 weeks” feels higher stakes than “3 to 5 story points”. There is less ego involved in story points *because* they are abstract 
- Time-based estimates tend to increase incrementally (2 weeks vs 2.5 weeks), story point estimates tend to make big jumps (5 points vs 8 points). This more accurately models the actual risk of the unknown. 
- Individuals feel like they failed if a 1-week task takes 2 weeks, but not if a 5 point story ends up taking the equivalent amount of additional time — assuming we still finish it in the sprint

*Note: plan to table any extended discussion of the merits of story points; that's not the point of this training* 


# Epic Estimation 
(15 minutes)

Let's recap [Estimating Epic Stories in Three Steps](https://chase-seibert.github.io/blog/2017/08/28/epic-story-estimation.html)

1. Start with historical list of **actual** user story points spent on previous Epics
2. Discuss what's in/out of scope — simple bulleted lists
3. Blind voting until we are within 1-hop Fibonacci numbers, then DRI decides 


*Note: If you've been estimating User Stories with points, there is a JIRA report for the actual historical story point values of your past completed Epics. This isn't the same as the initial estimate of those Epics; it's the sum of the story points for all the completed User Stories, once you were actually “Done” with the Epic.*

*Note: the "DRI" (Directly Responsible Individual) should be a specific engineer who will lead the work, and who is primarily responsible for the commitment.*

## Sample Previous Epics

Let's say we have the following actual Story Point counts for completed Epics. These are not strictly Fibonacci numbers because it's a sum of the **actual story points delivered**, not the original estimates. 

| **Previous Party**     | **Description**                                                                                                                                                   | **Actual Story Points** |
| ---------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------- |
| Casual Halloween Party | - 3 friends <br>- kids dressed up<br>- wine<br>- handing out candy<br>- walking around neighborhood                                                               | 8 points                |
| Easter Sunday          | - 6 friends<br>- Ham and Scalloped Potatoes<br>- ice bucket drinks                                                                                                | 23 points               |
| Christmas Day          | - research, bought and wrapped all the gifts<br>- holiday cards for 200 people, addressed them manually<br>- dinner Roast Beef and popovers with a chocolate tart | 85 points               |

## New Epic: Thanksgiving Dinner

Let's estimate a new Epic for a Thanksgiving Dinner. First, we need to make sure we're all on the same page about the scope. We need a product owner to answer questions. To facilitate doing this quickly, you can use post-it notes (either physical or virtual).


![](/blog/images/thanksgiving-epics.png)


### User Story Refinement

1. Brainstorm user stories that go into this Thanksgiving Dinner Epic 
    - Examples: specific dishes, groceries, cooking, what else? 
2. Write down any **acceptance criteria** folks can think of, and group them into User Stories 
    - Document acceptance criteria that's in scope (product owner makes the call) 
    - Document acceptance criteria that you have decided is **out** of scope
    - Once you're happy with the user stories and acceptance criteria, it's time to estimate the whole Epic
3. Rounds of blind voting ♻️
    - Decide which user stories are in scope for the Epic; a great way to increase confidence in the estimates is to de-scope anything the team + product owner can agree on 
    - Ask: “How **hard** is this, **relative** to the other Epics?” 
    - Count to three, and have everyone hold up a number of fingers that corresponds to a number of Epic story points
        - 1 finger == 13 points, 2 fingers == 21 points, 3 fingers == 34 points, 4 fingers == 55 points, 5 fingers == 89 points
        - These should be **relative** to the previous Epics!
4. If estimates are off by MORE than one finger's worth:
    - People who gave the highest and lowest estimates discuss the **risks**, and how **hard** this Epic is 
    - Update acceptance criteria 
    - Kick off another round of blind voting (back to #3)
5. If estimates are WITHIN one finger's worth:
    - Engineering DRI makes the call, estimation ends 

## Facilitator Prompts

If the conversation is not surfacing some key criteria, the facilitator can ask questions like:

- How many guests will there be? Did we remember to invite people? 
- Are we making elaborate place settings? 
- How do we order the stuff going in to the oven, are there dependencies? 
- What if we get a different number of guests than we planned? High risk events should be accounted for in the estimates.
- What if any given dish is missing/ruined? Medium risk events should have backup plans/mitigation. 
- What if the host/cook gets the Flu the night before? Low risk events can be ignored. 

# Create Epic Milestones 
(15 minutes)

Now, we are going to break the overall Epic into Epic Milestones. In real planning, these Epic Milestones would be your actual quarter-level roadmap items (assuming the overall Epic is too large for one quarter). 

![](/blog/images/thanksgiving-milestones.png)


1. Start arbitrarily with three milestones, M1, M2, and M3
2. For each User Story
    - What is the priority of this item? 
    - If we can't get to every criterion (post it) in a User Story, can we move some of them to future milestones?
3. Rounds of blind voting per user story *for the first milestone only* ♻️
    - Ask: “How **hard** is this, **relative** to the other User Stories?” 
    - What can we remove from the scope to reduce the estimate of this user story? 
    - Update the acceptance criteria, split apart user stories, etc. 
    - If a story is over an arbitrary size (i.e. 5 pts), break it up. Can we move some of the pieces to a future milestone?
    - If there is substantial **risk**, how can we mitigate, or is there a scope change that would reduce it? 

## Facilitator Prompts

- What if we ordered out for some of the food, or asked a guest to bring it?
- Can we make some of the food ahead of time? 
- Is there a smaller but sufficient version of a user story that we could do in M1, and do the full thing in a later milestone? 


# Q&A 
(5 minutes)

- What do folks feel like they learned today? 
- How can we apply this to our real Epic estimation? 
- Quick [plus/minus/delta exercise](https://www.scatterspoke.com/retrospective-library/plus-minus-delta-retrospective#:~:text=What%20is%20a%20Plus%2DMinus,something%20positive%20or%20something%20negative.) for this workshop 

---
title: "Setting Direction: Create a Strategy"
tags: manager reading-list template playbook
toc: true
header:
  overlay_image: /images/header-strategy.jpg
  overlay_color: "#000"
  overlay_filter: "0"
excerpt: "Good Strategy Bad Strategy using Apple circa 1997 as an example"
---

What is strategy, versus vision or mission? Is a business goal a strategy? It's hard to tell whether a strategy will be successful, up front. When you're creating a strategy, it can seem impossible to achieve. In retrospect, a good strategy seems like it was obvious, even though it was anything but. 

In this post, I'm going to use the rough outline of a case study, to root the conversation. Because this is such a well know case study, the strategy may seem obvious in retrospect. Having been alive and following this company during this period, I can attest to the fact that it was *not* obvious what the strategy should be, or that it would be successful. 

# Case Study: Apple in 1997

In 1997, Apple was on the brink of bankruptcy -- in fact they had about [90 days of runway](https://www.nytimes.com/2018/08/02/technology/apple-stock-1-trillion-market-cap.html) left. Expenses were high, revenue was flagging, and market share was down. These trailing indicators were not the *reason* the company was under-performing, though. Steve Jobs came back, and had to decide how to turn the company around, starting with diagnosing *why* the company was under-performing. 

Jobs' diagnosis was that the [company had stopped innovating](https://www.macrumors.com/2011/09/19/steve-jobs-apple-almost-went-bankrupt-because-it-failed-to-innovate/):

> When I left Apple ten years ago, we were ten years ahead of anybody else. It took Microsoft ten years to copy Windows. The problem was that Apple stood still. Even though it invested cumulatively billions in R&D, the output has not been there. People have caught up with it, and its differentiation has eroded, in particular with respect to Microsoft. And so the way out for Apple -- and I think Apple still has a future; there are some awfully good people there and there is tremendous brand loyalty to that company -- I think the way out is not to slash and burn, it's to innovate. That's how Apple got to its glory, and that's how Apple could return to it.

This seems obvious, in retrospect. It's also deceptively simple -- who does not want to be innovative? It *is* simple, but it's not easy. How did Apple under Jobs (again) turn this diagnosis into a strategy, and how did they execute on it?

I'm going to borrow a strategy format from [Good Strategy Bad Strategy: The Difference and Why It Matters](https://www.amazon.com/Good-Strategy-Bad-Difference-Matters/dp/0307886239) by Richard Rumelt, to illustrate.

```
Apple Strategy 1997

Our primary business challenge is that was are failing to 
innovate. Microsoft has caught up with MacOS, while we have 
been standing still. We need to leverage our excellent people, 
and our brand loyalty, and innovate our way back to 
differentiated products. 

Our current market share in personal computing is 3.8%, down 
from 10% just 5 years ago. 

Guiding Principles:
1. Focus our efforts on fewer things
2. Exploit our design and engineer excellence by creating 
delightful products 
3. Exploit our brand loyalty by focusing on high margins 

Actions:
1. Cut our workforce by 30% to increase cashflow 
2. Cut current product roadmap by 70% to focus resources 
on a few innovative investments 
3. Buy NeXT to be the foundation of an innovating 
next-generation operating system 
```

In the following years, Apple released the G3 Mac (1998), which had a radically delightful design aesthetic. It was a hit with the market. It released the iPod (2001), which was the first product in the category to be successful, due to a combination of excellent design and brand loyalty. Apple also released Mac OSX (2001), a technically innovative and aesthetically beautiful operating system. None of these product decisions were obvious at the time, but together they comprised a series of market hits that continued though the iPhone (2007), and turned the company around. Apple was successful because they focused, and they executed.


# What is Strategy? 

Let's break down this strategy, using the format in the Rumelt book. The hypothetical Apple strategy is a short, written document that diagnosis the problem, defines how the company will focus efforts, and contains specific actions. This matches Rumelt's definition of a good strategy: 

> The kernel of a strategy contains three elements: a diagnosis, a guiding policy, and coherent action. -- [Good Strategy Bad Strategy: The Difference and Why It Matters](https://www.amazon.com/Good-Strategy-Bad-Difference-Matters/dp/0307886239)

# ![](/blog/images/strategy1.png)

The hard parts are making the *correct* diagnosis, making the difficult decisions to focus and then actually executing successfully. I love this format because it's prescriptive about what a good strategy looks like. It allows you to get to the heart of the actual strategy work immediately. 

## Diagnosis

Steve Jobs' diagnosis was that the company has failed to innovate. That did prove to be a decisive challenge to the business, that once solved, did lead to success. That's not to say that it was the *only* strategy that would have been successful. This diagnosis is a good example of what Rumelt calls a key insight.

> The first step of making strategy real is figuring out the big ‘aha’ to gain sustainable competitive advantage—in other words, a significant, meaningful insight about how to win.

The book tackles strategy at the company level. The diagnosis should identify the most decisive challenge to the business, and also the cause of the challenge. It's OK if the challenge is written in terms of the business, and not as a user problem. The diagnosis simplifies the problem space down to one critical factor. 

There is no silver bullet to coming up with the right diagnosis. You might start by asking "five whys" about a few important business problems. Perhaps they will lead back to the same root issue. But in the end, picking correctly requires an intuitive leap. 

> At the core, strategy is about focus, and most complex organizations don’t focus their resources. Instead, they pursue multiple goals at once, not concentrating enough resources to achieve a breakthrough in any of them.

At the company level, there should be one diagnosis, not many. The ideal strategy focuses resources on a small set of actions, and those actions exploit a strength, or take advantage of a weakness. 

> Many bad strategies are just statements of desire rather than plans for overcoming obstacles.

You should gather data and see if it backs up the diagnosis. What is a single piece of data that makes the most compelling case for the diagnosis? 

Don't be tempted to use the data to set a goal at this stage; a goal itself is not a strategy.  The diagnosis should not read like a wish, or a hope. 

Finally, the diagnosis needs to tell how the challenge will be overcome. 

## Guiding Principles 

Apple decided to focus on fewer things, and leverage some key strengths versus the competition. Focus itself may not need to be stated -- focus is assumed as part of Rumelt's definition of a good strategy. 

> Good strategy requires leaders who are willing and able to say no to a wide variety of actions and interests. Strategy is at least as much about what an organization does not do as it is about what it does.

Guiding principles inform how the business will make trade-offs. How will you choose between different actions? This helps make sure that the actions are coherent, together. It also allows the decision making to scale across the organization. Guiding principles are a good opportunity to maximize existing strengths. 

## Actions 

Apple took concrete action, some of them, like layoffs, being quite difficult. The actions were coherent with each other; they all followed a theme, and were mutually reinforcing. 

> A good strategy includes a set of coherent actions. They are not “implementation” details; they are the punch in the strategy. A strategy that fails to define a variety of plausible and feasible immediate actions is missing a critical component.

Actions need to be both specific and achievable. It's notable that the actions were *not* building the G3, the iPod, and OSX. That would be far too specific. These specific products were likely not envisioned for a year or two after the strategy was defined. However, you can imagine individual teams at Apple, like the desktop Mac hardware team, coming out of layoffs with a mandate to create innovative desktop computers, leveraging Apple's design aesthetic strengths. This is how strategy flows down across the company.


# Multiple Levels: Make it a team sport 

The book is short on this topic. By focusing on company level strategy, it fails to address how strategy is distributed. Indeed, one of the main points is that company strategy should have a singular focus. At the same time, a large company will in fact do many things at once. It doesn't actually make sense to focus 100% of a company on one thing. In the best case, this looks like local strategies that align to the company strategy, and being prescriptive without being specific about what bets the teams in the company should make.

> Strategies focus resources, energy, and attention on some objectives rather than others. Unless collective ruin is imminent, a change in strategy will make some people worse off. Hence, there will be powerful forces opposed to almost any change in strategy. This is the fate of many strategy initiatives in large organizations. 

Depending on whether the customer of the sub organization is internal or external, it starts to make more sense to phrase the local diagnosis in terms of the customer. In this way, each organization and sub-organization can have their own strategy that aligns to the company strategy. At the team level, the actions are going to be much more concrete than at the company level. 

There are times of year that are naturally more strategy focused. But setting strategy is something you should be doing all the time, as a leader of an organization. Instead of thinking about it like a once-a-year waterfall process, think about it more like cyclical continuous refinement. 


# What is the difference between strategy and vision?

There is a lot of stuff that comes *after* the strategy. It's common to refresh strategy yearly, and then do headcount planning and potentially reorganize teams. That's a complex topic, on its own. Strategy is sometimes conflated with vision and mission, although they are actually separate. Roadmap planning in another common follow-on from strategy work. 

> Despite the roar of voices wanting to equate strategy with ambition, leadership, “vision,” planning, or the economic logic of competition, strategy is none of these. The core of strategy work is always the same: discovering the critical factors in a situation and designing a way of coordinating and focusing actions to deal with those factors.

Strategy is upwards facing. It's about making a compelling case to your leadership that you have a plan which will lead to business success. For an executive, the leadership audience is the board of directors. For a director, it's the executives, etc. Strategy also encompasses work from many teams, either an entire company, or an entire sub organization. 

Vision is downwards facing. It's about making a compelling case to the teams and individuals that they should be excited about the work. It's about inspiring builders and creative workers. It's about connecting the work to the user. The audience can be a single team. 

[Defining a vision](/blog/2022/02/14/define-a-vision.html) should be done after creating a strategy, for the simple reason that strategy informs what to focus on. It's true that you can't make sure a strategy is achievable without concrete actions, and eventually estimates. But, the greater risk is defining a vision and roadmap that do not address the critical business challenge. After creating a strategy and defining a vision, the next step is to start [building a roadmap with high level estimates](/blog/2017/08/28/epic-story-estimation.html).

# Example Template 

> A hallmark of true expertise and insight is making a complex subject understandable. A hallmark of mediocrity and bad strategy is unnecessary complexity—a flurry of fluff masking an absence of substance.

```
[NAME] Strategy [YEAR]

[LINK TO PARENT STRATEGY]

Out primary business challenge is [THREE SENTENCES]

[BACK IT UP WITH DATA]

Guiding Principles:
1. Exploit our [STRENGTH] by [DOING X] instead of [DOING Y]
2. Exploit our [STRENGTH] by [DOING X] instead of [DOING Y]
3. Exploit our [STRENGTH] by [DOING X] instead of [DOING Y]

Actions:
1. [THREE SENTENCES] [ACHIEVABLE GOAL]
2. [THREE SENTENCES] [ACHIEVABLE GOAL]
3. [THREE SENTENCES] [ACHIEVABLE GOAL]

```

The key stages in drafting a strategy are all around feedback. First, get feedback from your peers. You all need to be on the same page about the primary business challenge, and the most important actions. At every review, you want to answer the question, "would I fund this initiative"?

Then, get feedback from your leadership team. Ask the to be brutal. Get feedback both 1:1 ahead of time, and in a group setting. Plan to do multiple rounds of edits. Ask how this relates to other strategies. 

Finally, incorporate feedback from your organization itself. This often looks like the addition of actions that line up to the strategy. There is a danger in both communicating the strategy before you have upwards alignment, and also waiting until the strategy is locked to ask for additional actions. You need to find a balance. Give teams permission to go off and create their own strategies that line up to this. 
---
title: "80 Percent is Done"
tags: manager productivity
toc: true
header:
  overlay_image: /images/header-80-20.jpeg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "80% of impact for anything comes from the first 20% of the work"
---

It was a meeting with my design and product partners. Just the day before, we had been brainstorming ideas for the strategy for the coming year. In the 24 hours between, I had written up a draft of a strategy. It was more to gather my thoughts than anything else. My design partner smiled, and I worried that maybe I had overstepped. But he wasn't annoyed or surprised. Instead, he was complimentary. He said, **"I'm always impressed by how quickly you get to an 80% draft, and how you're OK with sharing that early work".**

It's true. In a leadership role, we're asked over and over again to produce clarity from ambiguity. At every level of an organization, leaders are synthesizing the all the information they have, and outputting new information. Neither the inputs nor the outputs resolve all or even most of the ambiguity. As leaders, we need to be comfortable with both receiving and emitting ambiguity — it's never going to be perfect. 

In many different situations, both large and small, I practice familiarity and comfort with ambiguity by striving to synthesize information, in writing, aiming to get to 80% fidelity in 20% of the time. 

# The 80/20 Rule

This is a form of the Pareto Principle, also known as the 80/20 rule. 

> The Pareto principle states that for many outcomes, roughly 80% of consequences come from 20% of causes (the "vital few"). Other names for this principle are the 80/20 rule, the law of the vital few, or the principle of factor sparsity. — [Wikipedia](https://en.wikipedia.org/wiki/Pareto_principle)

The principle applies in many contexts. 80% of land is owned by 20% of the people. 80% of sales come from 20% of clients. 80% of bugs come from 20% of the code. **My corollary is that 80% of impact for anything comes from the _first_ 20% of the work.**

In my experience, this is true of all kinds of different work. In the first 20% of a software project's lifespan, you will get a prototype that is 80% indistinguishable from any form of that product that will ever exist. Maybe this corresponds to the initial MVP of the product, which required one quarter of work. 

For pure information processing, i.e. reading and writing, the first 20% of the time can be as little as one hour. 

Assuming you're with me so far, a couple of insights follow naturally. 

1. It rarely makes sense to wait for more information; if you have an hour right now, start synthesizing (writing)
2. Pass the information on quickly; by holding on to information to "perfect" it, you're blocking downstream people from 80% of their immediate impact 

Together, these ideas are a powerful way to break through personal procrastination and paralyzation. 

# Trust Your (Professional) Instincts 

Learn to think like a consultant. Companies pay consultants to synthesize information, and come up with recommendations. They have very little context in the actual business, maybe 20% of any leader on the inside. They have "business" training and experience — but this is not their primary value. Leaders use consultants to outsource creating clarity from ambiguity. In practice, very often their recommendation is something that the leader has already proposed, but which is now made into a compelling written artifact. 

The 80/20 rule is your permission to resolve this ambiguity yourself. *You* can create the written artifact, even without all the information you would like to have. It will be 80% as good as anything anyone could come up with, regardless of time. It's faster than engaging any consultant. 

This applies no matter what the context is. In my career, I can think of times when I did not trust my own instincts about things like the valuation of a start-up I was thinking of joining, or the fundamental product flaws in something we were building. In all cases, my first take was 80% correct. I've learned to not dismiss my own analysis, just because I'm "not an expert". 

# Don't Optimize for Perfect Information

When faced with ambiguity (i.e. in every situation, ever), it's tempting to wait to get all the information you think you need. Even in the best case, you're making a sub-20% optimization at the cost of time. It's not worth it. In the worst case, your conclusion will arrive too late to matter. 

There is opportunity cost to waiting for more information, both in terms of other things you could be working to synthesize, and in terms of downstream leaders that you're depriving of information. In these cases, do the 80% version and then send it out. Done is better than perfect. 

As an added bonus, you often capture some outsized value for getting it done quickly, or being the one to get it done first. In a sea of potential ideas, the ones that take form first will invariably have outsized influence in the formation of the final, coalesced proposal. "Getting there first" is not-too-subtly anchoring the group's thinking. 

# When to Go to 100%?

When does it make sense to continue to put effort into something past 80%? Maybe never. But, sometimes the goal posts move significantly. What was an 80% solution in a fixed space is now just part of a solution in a much, much larger space. What you can do is sprint to an 80% solution in the new problem space. It may take as little as an hour. 

This may all sound like half-assing your way through everything. But, it's actually very hard to send partially baked ideas out to your colleagues. It's hard to take many different signals and synthesize them down into something coherent. Writing well is hard. Don't confuse the number of minutes something takes with difficulty. 

In high school and college, I used to feel guilty about doing this. If I wasn't always pushing to 100% effort, was I wasting my potential? Eventually, I came to two realizations. First, I never fully stopped thinking about any problem that was still in progress. Even if the 20% effort had not started yet, I was invariably thinking about the problem in the background, whether it was while walking to class, eating, or taking a shower. That effort count too — especially in salaried employment, we're never really not working. 

Secondly, sprinting to 80% of the impact means you can then get started on the next thing. Now, I relish the prospect of checking off an item on my list at 80%, and looking down to see what I get started next. ---
title: "What are Healthy Relationships?"
tags: manager
toc: true
header:
  overlay_image: /images/header-healthy-relationships.png
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Strong relationships built in the small moments will be what you lean on to get through tough times"
---

Five years into my career, I had gone from being one of the few engineers at the company, to being the expert on a medium-sized engineering team. If someone misunderstood how something worked, I could list all the ways there were wrong. If someone had a product idea, I could find all the ways it might not work. I even remember joking about this by saying, "if you need someone to poke holes in an idea, you know who to come to". No one ever said anything directly, but my manager started getting feedback about how I was coming across. I got some formal 360 feedback that was filled with phrases describing me as "always needing to be right", "thinks he knows everything", and "too cocky". 

<iframe width="560" height="315" src="https://www.youtube.com/embed/C6BYzLIqKB8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>

I wasn't sure how to process the feedback at first, but eventually I realized that the biggest thing limiting my impact was my relationships with other people. This was a formative piece of feedback in my career, and marked the first time I started thinking about my "soft skills". 

Healthy relationships at work are not just about the people on your team, or even the people you interact with. They are also the way you interact over email, in a large Slack channel, or in a document. It's OK to occasionally cross into curmudgeon territory, but don't make a steady diet of it. If it's too frequent, too visible, or inappropriate for the audience, it can cross the line. You can't go wrong if you consistently take ownership, assume good intent, and radiate positivity.

# What Good Looks Like

You always want 360 feedback from your relationships to be excellent. In terms of having healthy relationships, this looks like consistent positive comments about your professionalism, tone, and collaboration. Feedback that is specifically about you or your relationship should not be negative, even if there are challenging projects, or circumstances. You want feedback from others to be critical of outcomes, not of you personally. 

For engineering managers, this means feedback from your direct reports as well as your peer product managers, designers, and engineering managers. Your manager probably gets feedback from these folks in one on ones, and from anonymous surveys. Feedback can also come from exit surveys when people leave the team. 

As you get more senior, the pool of people who have visibility on you grows. At some point, you can reasonably expect feedback to potentially come from anywhere in the company. It starts to reflect not just on you, but also your whole team, and your leads. 

Model the relationship that you want in your own interactions with folks. Demonstrate trust, always keep things professional, and criticize the work, not the person. Also, keep an eye out for common work stressors. 

# Common Stressors on Healthy Relationships 

Having healthy relationships does not mean that nothing will ever go wrong. But when things do  go wrong, your relationships are not the cause, and your relationships are not burned resolving the issue. 

## Blockers 

Whatever the issue is, you want to talk about it with your team, peers, and your boss. This happens in one on ones, and team check-ins. To some extent, it's OK to vent or be frustrated in a one one one with your manager. That's part of what they are there for. But in a team meeting, you want to quickly move forward constructively. You can do this by taking a lens of extreme ownership, where you take responsibility to solve the issue together. 

If you have an idea for a solution, use your manager as a sounding board to help you work through it. It's OK to come to them without a solution, but be prepared for ideas from them. You should expect that 80% of the time, your manager will propose a solution you can act on yourself, versus having them act.

What if the blocker is another person? Remind yourself that you need to assume good intent, ask open-ended questions, and frame things in a positive light. Don't turn a non-people problem into a relationship problem. 

## Uncertainty 

The level of uncertainty and ambiguity you deal with on a regular basis is only going to increase as your seniority and scope increases. At a certain level, no amount of uncertainty is anomalous or outside the range of expectations for your role. Uncertainty itself should not be an excuse for unhealthy relationships or interactions. 

Sometimes, you need to live with the uncertainty for a bit. You may catch yourself making the  mistake of trying to resolve uncertainty too quickly, versus not quickly enough. Sometimes what is required is just to sit patiently with the uncertainty. In general, your best tools here are going to be perspective, curiosity, and reaching out to the right people. 

If you need to escalate to resolve the uncertainty, be sure to follow [healthy escalation best practices](/blog/2021/04/05/escalations.html). 

## Lack of Confidence 

What if you think that a peer is not performing in their role? First, you should give them feedback. Then, try coaching them. Finally, give their manager feedback, either directly, or though your manager. Your working relationship with the person should never suffer. Even if the person is under-performing, you don't want your relationship itself to become a contributor to blocking the team. 

Be mindful that it's not your decision whether a cross-functional peer, or anyone outside your org, is performing well in their role. You cannot make that determination unilaterally with the information you have. Don’t assume you know what is expected of them outside the context of your roles and responsibilities with each other. What you can do is make your respective roles and responsibilities clear, in writing. 

Never balk at performing your own role based on a sense that someone else is not performing well. A specific case that you may see is when you are not happy with the level of detail in product specs. Do not refuse to estimate work, or put items on the roadmap. This can put your relationship with the product manager at risk. Instead, work with the whole team to refine and scope the work together. 

## All Hands Q&A

The larger the audience, the more potential risk there is to damage relationships. One of the larger forums that you may be a part of in your work is an All Hands Q&A session. You never want someone to be able to interpret a question you're asking as having bad intent. Even if a question is anonymous, make sure it comes across in a healthy way. Avoid making unvalidated assumptions, and keep the tone positive. Ask yourself, how would you feel if this question was on the front page of the New York Times? 

Asking provocative questions is valuable, but not if they negatively affect morale, or reflect poorly on your team. It doesn't matter if you're 100% right, if you don't find an effective way to ask the question. The ratio also matters, you don’t want to be the person always throwing bombs. 

# How to Optimize for Health Relationships 

## Communicate with Ruthless Positivity 

It's up to you to radiate the positivity that you would want to see in anyone. Relentlessly assume good intent. Psychological safety is a two-way street; in your interactions with peers and your manager, given them space to feel safe. Tailor your communication to leave the door open for constructive discussion. Take accountability for how you communicate, how it's perceived by others, and it makes them feel.  

If you find yourself communicating with a sense of frustration or entitlement, take a step back. If you can't find a way to say something that does not pose a risk to healthy relationships, save it for a one on one with your manager. You might try explicitly labeling it as a "rant". As always, praise in public, and criticize in private.

## Prioritize Relationships

How and whether you show up for people affects the relationship. You should make time for regular one on ones with your team, as well as key peer relationships. Communicate that the relationship is important to you by keeping the time on the calendar, and coming engaged with topics to discuss. 

Get to know people on a personal level. Spend the first part of one on ones and smaller meetings asking folks what is going on outside of work. What are they excited about, right now? Smile and laugh with them. Make it genuine. Strong relationships built in the small moments will be what you lean on to get through tough times. 

Don't underestimate the concentrated power of sharing a meal with someone, in person. 

# More Story Time

Much later in my career, I was fired from a role due to unhealthy relationships. My results were great. My peer, team and direct lead relationships were great. But, I had pissed off my great grand boss, and I didn't last long after that. There was no warning, and no feedback. I'm not even 100% sure what happened, but I can guess. 

It came down to a refusal to engage my team, when I balked at moving forward with a project that didn’t have a product manager. I had given direct feedback, but my tone was "this is crazy", "this is unreasonable", and "YOU need to solve this". That interaction put me on a knife's edge. Later, there was a simple misunderstanding, but my credibility with this person was shot. I had burned the relationship, and it was unrecoverable. 

Outside of layoffs, I have more often seen attitude and unhealthy relationships result in termination than actual performance issues. Sometimes you can do enough damage to your internal reputation that it’s impossible to recover. There is no PIP for that. 
---
title: My Expectations for Managers
tags: manager newboss
toc: true
header:
  overlay_image: /images/headers-notebook.jpeg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "These are my basic expectations for managers that report to me"
redirect_from:
  - /2020/02/13/expectations-for-managers.html
---

When working with a new manager that's reporting to me for the first time, I like to 
share this document. Hopefully, it helps us get on the same page about our shared 
expectations for your role. I hold myself to the same bar on all of
these -- you can expect the same things from me that I expect from you. 

These are probably not controversial. If they are, let's talk about it! 


# Weekly 1:1s

You should have weekly, regularly scheduled 1:1s with your direct reports. For most roles,
this will be somewhere between five and eight 1:1s a week. For product teams, you should also have weekly 1:1s with your direct product manager and design partner. Let people reschedule as needed, but
don't skip too often. 

You can expect the same from me. You can also expect that I will have monthly skip level 1:1s
with your more senior individual contributors, and any managers that report to you. These are 
mostly for feedback, which I will share with you as appropriate. I also use them to develop
bench talent. 

How you run the 1:1s is up to you. You will find that I tend to run mine with written agendas, 
and lots of transparency. 


# Tight Meetings

If we're in a meeting together, it should be well run! As an engineering manager, either you, 
me, or my manager will probably run 80% of the meetings we're in together. Even if it's a meeting 
hosted by a cross-functional partner, you should take responsibility for making sure it's well run. 

You can expect that if I invite you to a meeting, that it will have an agenda ahead of time. I will 
make sure that we stay on topic, take notes, and end on time. 

If I take an action item in a meeting, I will resolve those 100% of the time (or get back to you 
and say I won't be doing it after all). 


# Excellent Execution 

When you commit to a deliverable on a quarterly roadmap, those should be delivered on time 90% of the time. If a team has four or five main items on their roadmap, that means that one item 
might slip every two quarters. The expectation that 9/10 items ship on time can surprise people; that is a higher bar than industry average for not slipping commitments. 

I hold myself to the same bar for the roadmap delivery across all my teams, collectively. That doesn't mean that I expect teams to kill themselves to hit unrealistic timelines. What I expect is that after a period of forming/storming/norming, a team gets proficient at estimates that include an 
appropriate risk buffer. 

The real secret sauce is when a team develops a working relationship with their product owner
such that they can seamlessly trade off scope, time, and quality. In my experience, these
teams can deliver on any timeline; because the scope is fluid. 

At the end of the day, I expect the engineering manager and the product owner to agree on 
whether something was delivered on time, and that it satisfies the deliverable. As long as 
you agree, I'm happy on execution.

You can expect me to jump in and help identify and mitigate large risks that could derail us. 

Also related to execution:

- Commitment misses should be on the lowest priority items
- Be able to “do the math” to rationalize quarter level commits based on team velocity
- If you’re going to slip, communicate it early, for example 1/3 of the way in to a quarter
- There should not be confusion about the exact commitment, for example code complete versus shipping to production 


# Product Partnership 

You and your product manager should be in sync. I expect the 360 feedback for each other to be 
excellent. It's unusual for there to be a dysfunctional relationship between an engineering 
manager and their product partner. When it happens, you can expect that I will dive in and 
try to debug that as a top priority. In this state, the team has a very low chance of success. 

For a deep dive, see [What are Healthy Relationships?](/blog/2022/11/18/what-are-healthy-relationships.html).


# Impact Outside your Team 

You should have one major piece of impact a year, outside the scope of your team and mission.
This could look like shipping an internal tool, revamping an interview question, or 
updating the company career framework. I can help you identify opportunities. If you sign up to 
take on something like this, I expect you to proactively drive it forward. 


# Influence the Roadmap 

You should also come up with one major product or foundational initiative a year. These will mostly
be in the scope of the team mission, but could be anything related to the overall product, or 
internal processes. As you get more senior, this means understanding the business context almost
as well as a product manager, and understanding the technical context almost as well as a senior
engineer. Trust your own insights, and put a stake in the ground about something you would like to 
see happen.

This doesn't mean that it's always going to get onto the roadmap. An ideal outcome is that it 
influences the product org, and helps set strategy going forward. If this exact idea ships, 
it's a bonus. That will probably happen about 25% of the time. 

You can expect ideas from me, as well. These are never directives, but I do expect you and your product partner to follow up and either validate or invalidate the idea. 


# Be Comfortable with Uncertainty

Dealing with uncertainty and ambiguity is part of the job. This will only become more important, 
the more senior you get. You should never let uncertainty become a blocker for the team, or an 
impediment to healthy relationships. 

You can expect a reasonable level of transparency from me about any given situation. I won't
necessarily commit to pushing to resolve any given uncertainty as soon as possible. That's often 
a premature optimization. But, I will tell you when I plan to sit with the uncertainty, and when
I intend to resolve it. 


# Grow a Successor

It's not always possible, but you should try to have a succession plan for both yourself, and the primary technical lead on your team. Pick one person, and create a growth plan for them. Document 
where you think they are already ready for a next level opportunity, and where they still have to grow. This will come in handy, often on short notice, when an opportunity opens up. Ideally, we've already talked about this person, and have the beginnings of a transition plan in place. 

# Deliver Business Impact 

You're primarily going to be judged on your track record of delivering business impact. This should not be a surprise; it's the cornerstone of most written company expectations for any role. 

As an engineering manager, we are going to primarily deliver impact through excellent execution. What if something is delivered on time, but does not result in the impact that we hoped for? I expect this to happen on individual projects maybe 50% of the time. I do hold engineering managers accountable for their portfolio of projects, and the total impact. Part of our job is to influence the roadmap towards high impact work, refine the scope so that it actually does have impact, and deliver quickly so that we can fail and learn fast. 

Anything in the realm of execution is also fair game. Common blockers like cross-organization dependencies, alignment with leadership, and even a project being de-prioritized before shipping are things we are ultimately accountable for resolving. 


# Do the best thing for the company 

When in doubt, do the best thing for the company. I expect this from anyone in a leadership position. Often there is temptation to optimize for the local team. As you get more senior, I expect you to optimize for the company more. ---
title: "Organizational Design"
tags: manager director playbook
toc: true
header:
  overlay_image: /images/header-org-design.webp
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "You don’t have to choose just one organizational strategy for your entire company. A mature product org may be organized by goal, while a new product org has no structure, and a shared platform organization is structured by technology."
---

Organizational Design is how you group teams together in order to optimize for various constraints. There is no perfect organization design, only a series of trade-offs. Unless you’re a small company, how you group teams together will optimize for some dimensions and constraints over others. No organization design you choose will remain stable any longer than a few years. But you can mix and match different design strategies for different parts of the organization. 

# What is an Organization?

An organization is a set of teams with one name. Your company is one organization. A large company includes many nested sets of organizations. How you group sets of teams together, and what type of shared mission you give them, is the result of trade-offs you make between competing factors like coordination and dependencies. 

# What is Organizational Design? 

Organization design is how you choose to group the teams together, and what name you give them. Mission scope, coordination cost, and dependency management are the primary factors in any organization design. 

Mission scope simple means the job of the team. What is their primary goal, and what problem are they trying to solve? The mission should be compelling, distinct from the missions of other organizations, and meaty enough to have impact commensurate with the expectations of the larger company.

Coordination cost is incurred in the form of communication overhead for any two teams that have a dependency between them. If possible, you want to remove the dependency altogether. If you cannot remove it, you want two teams involved in the same work to share the shallowest common “branches”. As the distance increases, so does the risk of misalignment and prioritization conflict, which can lead to the dependency blocking the project. 

The reporting structure of the organization, literally who reports to whom, determines the distance and number of people in the decision-making loop. In terms of risk, there is a natural incentive to make sets of teams in an organization as small as possible. But there is a competing factor called the “span of control”—giving teams a sufficient number of people to realize impact—that incentivizes larger sets of teams. For any given manager, most companies ideally want somewhere between five and eight direct reports, for cost efficiency and to give each direct report the right amount of attention and support for their personal development. 

Making trade-offs between these factors are the primary driving force informing organization structure. 

# Small Organizations Don’t Need Structure

My first job was in a start-up with one and a half other software engineers, and no product managers or designers. There were no teams, or rather we were one big team of 12 including sales, finance, and customer support. There was a reporting structure, where sales people reported to the head of sales, etc. But there wasn’t an organization structure. We didn’t need one.

In a small company, communication and coordination overhead is negligible. When you’re coding a feature, there are probably no dependencies. If there are, you can literally turn to the person next to you to resolve it with a quick conversation. Engineers are able to deliver almost all their work by themselves. 

# Medium-Sized Organizations

As a company gets larger, there will be many teams. You’re all working on one product, but no matter how you organize, you will start to see more and more dependencies. 

Teams can be grouped by technology, feature, business goal, or customer. We are going to ignore grouping by customer, because it tends not to scale past a few types of customer. 

## Grouping Teams by Technology 

Codebases are broken up into repositories, services, modules, and files. They are naturally categorized by theme, i.e. what can I include in this set that’s cohesive as a unit, and has minimal dependencies on other things? 

Engineers have a tendency to first think of organizational structures that match these themes. This makes sense! You know this will minimize dependencies, and you know that assigning code ownership will be straightforward. Using an organization structure like this for a set of teams will tend to incentivize quality, which is also a classic engineering value. 

Organizations structured this way will excel at keeping things running, and running well. They will maximize uptime, performance, and correctness. They will tend to be more stable over time, as the basic thematic groupings of code do not change often. They are great for knowledge sharing across engineering teams. This structure is often used in Platform organizations.

On the other hand, these organizations will bias towards investment in existing use cases vs new use cases. Innovation will often look like rewriting an existing component. Engineers may gravitate towards this work, even when the business thinks it’s low value. It may be difficult to get engineers to work on higher impact stuff. Work on new initiatives may be diffused across more teams, increasing overhead and risk, and reducing accountability. 

## Group Teams by Feature

If you need to make significant progress on a relatively well-known new or existing feature, consider grouping all the teams who need to deliver work for that feature together. This may make sense, especially for a handful of top priority initiatives. By having every skillset the teams need to deliver their roadmap, you are minimizing coordination cost and dependency risk. 

This could look like front-end, back-end, mobile, database, and devops engineers all working directly together on cross-functional teams, separate from other engineers of the same horizontal discipline. One manager may have direct reports from each discipline, and together with their peer managers report to one organization manager, who can easily resolve prioritization issues. 

This organization structure maximizes execution, assuming that the solution is relatively well known. It may be ideal for large cannot-fail mega-projects. Of course, projects can always fail, but this structure greatly decreases execution risk. 

With this organization design, you are incurring maximum churn on team missions and superstructure, as missions revolving around features will naturally change more often as the features ship. When that churn happens, more people will change managers, and it will take time to reestablish processes on affected teams. Plus, there is a high likelihood that this churn will be incurred again, as the project is completed and teams align to the next feature. 

In the initial move to feature organization, there may be resistance to decoupling teams from their previous groupings, especially if they have formed a sense of identity around being a “platform”, or “mobile” organization. 

These organizations are sometimes over-staffed. It’s a fine balance on the continuum between creating an organization for a solution that’s fairly well known, versus creating the organization in order to define the solution. Too much of the latter, and you run the risk of having people and teams roped in without a lot of actual work to do. Worst case scenario, this can lead to scope creep and unneeded complexity as teams “invent” work to do. 

# Large Organizations 

As the company gets even larger, you are likely to be working on many more totally different things, even multiple different products. The number of dependencies grows exponentially. You cannot truly minimize dependencies, instead you need to pick which ones to derisk. 

Large companies contain many organizations. Each organization may be responsible for one product, or one sub-goal. Inside each organization, you can mix and match grouping sets of teams by technology, features, or customer. 

## Grouping Teams by Business Goals 

Like the feature grouping, grouping by business goal puts a sufficient number of people under one overall accountable person to achieve the goal self-sufficiently. Unlike feature grouping, the solution is often undefined. Instead of being about estimates to build a specific feature, the number of people allocated to this type of organization is based on how much of a "bet" the company is willing to make on this goal. Sub organizations can be created for sub goals, which should have their own single accountable person. 

This type of structure prioritizes innovation. Doing this well requires clear goals and metrics. The strategic vision must be compelling. With unknown solutions, there is a tendency to give each product leader similar resources. This option is often used for product-led organizations, i.e. organizations where everyone ultimate reports to a head of product. 

Teams and individuals in this model may need to exercise more of their cross-functional skills, versus organizing by technology or feature. You may need to repurpose an engineer for something that does not 100% match their skillset, or you may discover that you need a new skillset as you uncover solutions in the space. You may also need to unblock dependencies by changing the working agreement so that teams inside your organization can do work in codebases that other teams own. These both have real costs. Swim lanes inside the organization may be unclear, especially as you uncover new solutions that don’t match the initial structure. 

## Hybrid Grouping

You don’t have to choose just one organizational strategy for your entire company. A mature product may be organized by goal, while a new product has no organization, and a shared platform organization is structured by technology. Structures will also change over time, as the situation changes. Knowing when to use which structure is important, even if there is no “right” answer. 

A common tool when mixing structures is the “matrixed organization”. This means that the reporting structure does not match the sets of teams structure. For example, an engineering manager may have direct reports in various squads, working on different things, and working with different product managers. This increases coordination cost. It also reduces the context between a manager and their direct reports, which can make performance evaluation more difficult. Watch out for negative impact on cross-functional alignment. 

## Other Factors to Consider  

Inertia is a force that is actively fighting your organizational design. All else being equal, change is hard, and people will be naturally resistant to it. Assuming you overcome that, enacting change is a lot of work. You need to communicate the changes, move people around, and change the names of teams in hundreds of places. Teams with personnel or mission changes will be starting further back on the forming/norming/storming curve. 

Changes in code ownership have their own cost. There should be a very good reason for moving ownership between teams. “Keep the Lights On” (KTLO) type work should be as uniform as possible across teams, which may involve additional ownership changes.

Teams and organization names are a powerful tool for autonomy, i.e. "You’re the Activation team, you’re empowered to own anything that involves user activation across the entire company". But names can also be burdens, as in “every team with something they can squint at and call activation work is going to try to get you to own that”. 

You should also consider whether to group specialized skill sets together (i.e. Mobile, Devops, etc). You may also want to think about giving teams the largest possible “swim lane” to an organization, where they can have autonomy, cohesion, and separation of concerns from others. 

There is no perfect solution that will solve all factors. The best you can do is pick a set of dimensions to optimize for, and be aware of the trade-offs you’re making.

# Conclusion 

As an organizational design changes over time, you will make different trade-offs. This is fine, because there is no perfect organization structure. If you stay at a company long enough, you may see a particular pendulum swing back and forth more than once. Each time is a response to a current problem. 

Organizational design is not something you can solve permanently. It will need to be solved again as strategies change, and projects come and go. You can only design an organization for about 12 months, the typical macrocycle where both company and team roadmaps are most stable. 

Remember to not create dependencies if you don’t have to. Definitely don’t architect in more dependencies than absolutely necessary. Make the best trade-offs you can, for today. You can always make different trade-offs in the next reorg! ---
title: "Dependency Management Playbook"
tags: manager checklist scorecard playbook
toc: true
header:
  overlay_image: /images/header-dependency-management.webp
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "How to manage risk for your project dependencies, i.e. when you need another team to put something on their roadmap."
---

For engineering managers, delivering projects reliably and on time is a critical part of the job. Even if everything is estimated and executed well on your team, dependencies can derail you. If you poll a group of managers on their top risks — and I have — they will come back with a list that’s 80% dependencies on other teams. But many will feel helpless to resolve those dependencies. If your project requires another team to prioritize your dependency, what can you actually do to mitigate that risk?

Not needing dependencies is a luxury afforded to teams in small companies, or those running very high priority projects. I once ran a tiger team that was both. We moved experts from across the company onto the team so that we could own every piece of the execution. In large companies where a single product spans many teams, most projects will require one or more dependencies. You need to have a playbook for managing dependency risk. 

# List the Dependencies 

First, you need to know what the dependencies are. One of your core competencies as an engineering manager is “asking the right questions”. Hold a brainstorm and help the team see dependencies that they might not know exist. Cross-functional dependencies like security, legal and compliance can be particular blind spots for engineers. For actual software engineering, the team can almost certainly identify the big dependencies off the top of their heads. Doing a pre-mortem exercise can be one way to enumerate them and quantify the risk for each. You want to do this early in the project, as part of project estimation. 

Create a list of all the dependencies for the team, for the year. Each one should have a project name, a short description of the dependency, the risk or priority of the dependency, the name of the remote team that you are dependent on, and a specific team member who owns it. Try stack ranking them, with the highest risk/impact dependencies first. Share this list with your manager. 

Having a prioritized list of dependencies is useful for more than just your team. Suggest that your manager ask all their teams for this, and that they create a combined list. How they prioritize the list across teams will tell you a lot about how well aligned you are, how they are going to present these priorities to their own peers and stakeholders, and may also give you early signals on when a dependency is a non-starter. 

Show your list to the engineering managers you are asking for dependencies from. Ask them to share their full roadmap, and share yours. Include not just the engineering managers, but tech leads and product managers on both sides, as well. Make sure you have a solid business case for your project. These stakeholders, artifacts, and visibility are how dependency problems are turned into creative solutions.

For any given dependency, you should first try to remove it, and then you should secondarily try to push it back. Removing a dependency typically looks like changing scope, so that the dependency is not required. Pushing it back looks like structuring milestones -- provided that early milestones deliver user value -- so that it is not required until later. This is basic execution management that should happen for all your projects, and should be old hat for yourself, your tech lead, and your product manager. 

# Get Engineers Engaged 

If the dependency cannot be resolved with a creative solution, the next step is to get the right people to bring the right expertise to bear on the problem, and to get work started right away. A dependency that is not being actively worked on by engineers is an at-risk dependency. As an engineering manager, allocating these resources appropriately for the business is another core competency. That’s not to say that it will be easy! Managers sometimes confuse accountability for allocating resources with having authority to dictate those decisions. Given that we know that we will mostly not be unilaterally allocating people across disparate teams, what staffing levers do we have at our disposal? 

Use your knowledge about the organization and other teams to identify the correct team for the dependency, and also the most likely points of contact on that team — the experts — who can help the most. Start by asking for a consultation with that expert. This could start as one-time, and then you can collectively evaluate if consultation alone can satisfy the dependency. Identify your own experts, and get them involved. If you respect the remote team’s time, create a pre-read, and get aligned ahead of time with their manager, this should be a slam dunk. Even if the work turns out to be too large for a consult, you will have cemented a clear point of contact on the remote team.

Next, create a dialog with the right points of contact on both sides. This could be a simple Slack channel dedicated to the consultation. Eventually, this may be the rally point for a full on roadmap dependency. Your goal is to create and nurture a relationship between the teams, to facilitate a conversation. If you make it transactional, that is how it will feel to the other team. Try to set ground rule expectations. Can the other team agree to provide support for things like technical spec reviews, and code reviews? 

Another staffing mechanism available to all managers is embedding, also known as tours of duty. This means offering to loan one of your engineers to another team for a specific period of time. Alternatively, they could loan you an engineer. It’s easier than a roadmap commitment, because it’s timeboxed. The receiving team agrees to return the engineer on a specific date, based on the initiating team’s estimates of the work. The engineer(s) exclusively work on the dependency. If it’s not resolved in time, that is the responsibility of the initiating team. The embedding should end on time regardless. Managers should write down expectations for things like how specs and code get reviewed, what needs to happen to merge a code change, and whether the person is expected to improve the state of the surrounding code. If folks are willing to cross-train, this can work even if there is a skills or domain knowledge gap in the person being embedded. 

Other staffing solutions may or may not be available to you. Most dependencies that get to this stage require domain knowledge. If the dependency is primarily about raw staffing, you can of course hire. Even if this is not the case, if you are in charge of a headcount budget, reserving some budget for dependencies like this can be effective, to facilitate potential internal transfers, both to be able to receive an internal transfer, or to offer as compensation to another team for a transfer. 

Depending on your scope, other options like moving people between teams, or moving entire teams, may be available. During a regular reorganization of teams, you should be actively managing your dependency risk. 

# Mitigate Risk with Milestones

Once you know what your dependencies are, and the right people are working on them, the next step is to structure the work. How you structure the work greatly affects how much risk you are taking on. Your goals are to shorten the window before work begins, bring forward the first increment that delivers user value, and shorten the cycle time between further iterations. 

Creating a prototype is a critical early milestone. This should be a quick and dirty development environment only implementation of just the very core of the product. This gives you some real code to discuss, some real experience to demo, and will also uncover some of the primary unknowns and decisions that need to be resolved. Concurrently, you can also do “spike” sprints to try to resolve specific unknown and technical risks. 

After that, you want to move to an in-production implementation, behind a feature flag, as soon as possible. This is Milestone 0, a foothold that you can continue to expand, refine, and polish for the rest of the project lifecycle. After that, milestones should be “theoretically shippable”, i.e. a user could get some value out of this, no matter how small. Don’t break things down into milestones like backend, front-end, release testing, etc. Instead, think of thin vertical slices. Each slice should deliver some user value, end to end. The slice itself should accomplish part of the overall job to be done. It should be just polished enough that you could release to wider and wider audiences, such as team-only, internal-only, early access customers, etc. 

When planning the scope of the milestones, think about whether certain dependencies are only required for some of the scope. Pushing that scope back into future milestones will reduce the risk associated with delivering earlier milestones, at the cost of non resolving unknown technical risk inside those dependencies early. If possible, try to de-risk those separately with their own prototypes, potentially on another team’s roadmap. 

For any cross-team dependencies, agree on a code interface for the dependency. This can greatly clarify what each team needs. You may also want to create failing unit tests for those code interfaces. You could go as far as to mock the dependency with basic behavior, which will be useful in your own development and testing. Any performance or scalability requirements can also be part of your interface definition. 

Finally, come up with a Plan B for each dependency. This will not always be possible. Examples include planning to build and ship a heuristic solution to a machine learning model dependency, or hard-coding something that will eventually be configurable. Other examples could be shipping a mock implementation of a back-end system, or further dropping scope so that it’s not necessary. 

# Escalate Unless Development Starts

If dependencies cannot be eliminated or committed to such that they are beginning work soon, you should escalate. This is a last resort, only because you need to attempt other methods first. But don’t let dependency conversations linger too long without making forward progress. If progress has plateaued for two or three weeks, it’s time to start the escalation process. Much of the time, you will not end up having to escalate at all, but that’s part of the magic!

> Often the mere act of sitting down and trying to write up a proposal will lead to Frank and Faythe to come to a compromise. Partially this is because writing down details forces clarity on the situation. It also reveals options that neither party had thought of independently. A big part of last-minute compromises is that everyone is reluctant to involve their manager, asking for a decision. This makes it more likely that each side will accept a compromise that they would not otherwise have accepted, simply to avoid escalating. — [How to Escalate](https://chase-seibert.github.io/blog/2021/04/05/escalations.html#clean-escalations-the-one-pager)

The short version of how to escalate is:
1. Create a doc collaboratively with the other engineering manager, and include cross-functional peers 
2. Pair down to two or three potential outcomes 
3. CC both of your leads at the same time, plus any cross-functional partners, via email 

If the escalation results in a roadmap commitment, you should have a record in the above email. If not, create one. This is critical for clarity and accountability. 

Done correctly, most escalations will be resolved by a compromise before your leads are included. This is what healthy cross-team collaboration looks like; identifying problems and finding solutions. Don’t treat an escalation as a threat, your goal is to share context and exercise empathy, and iterate to a solution that is best for the entire company. The biggest failure mode in escalations is not doing them, or not doing them early enough. 

# Checklists 

## Dependency Risk Level Scorecard 

How high risk is your dependency? If you cannot answer “Yes” to three or more of these, your dependency is at high risk.

- [ ] Has work started? 
- [ ] Is work starting in the next quarter?
- [ ] Is there a working prototype of the solution?
- [ ] Is the team confident that they know what the technical solution is, exactly?
- [ ] Do you have an API interface definition agreed on by both teams? 

## Team Lead Dependency Mitigation Checklist 

- [ ] Have you asked critical questions about the product and the technical plan? 
- [ ] Have you identified the right stakeholders? 
- [ ] Have you written and communicated a compelling business case for this work? 
- [ ] Have you brainstormed potential dependencies and risks with the team? 
- [ ] Have you held a pre-mortem about what could go wrong, worst case? 
- [ ] Have you listed all of your dependencies and risks somewhere? 
- [ ] Have you prioritized your dependencies by negative impact? 
- [ ] Have you documented your full team roadmap, and shared it? 
- [ ] Have you defined milestones that are thin vertical slices of potentially shippable product increment? 
- [ ] Have you cut scope from early milestones to de-risk dependencies? 
- [ ] Have you set up an ongoing consultation with your primary dependencies for questions, design review, and code review? 
- [ ] Have you created a safe space where you can engage in dialog with each dependency? 
- [ ] Have you built a working prototype of the solution? 
- [ ] Have you mocked up a code interface for each dependency, with example inputs and output? 
- [ ] Have you documented the dependency and the commitment in an email? 
- [ ] Have you identified specific people as points of contact for the dependency? 
- [ ] Have you put together an escalation doc for this dependency?
- [ ] Have you sent the escalation doc to leads on both sides? 
- [ ] Have you created a Plan B for if this dependency is not satisfied? 
- [ ] Have you determined whether there is a relationship or trust issue between these teams? (manager of managers)
- [ ] Have you brought in the right experts from outside the teams to consult on this dependency? (manager of managers)
- [ ] Have you considered moving people around for this dependency? (manager of managers)
- [ ] Have you considered embedding engineers across teams? (manager of managers)
- [ ] Have you reserved headcount to facilitate this dependency? (director) 
---
title: "How to Run Calibrations"
tags: process performance
toc: true
header:
  overlay_image: /images/header-calibrations.jpeg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Calibrations are a group-based process for an organization to increase the equity of individual performance reviews across a large set of teams"
---

When there are many managers in a large organization, how do you make sure that any performance ratings, promotions, and subsequent compensation changes are fair, relative to each other? A common mechanism is for all the managers to get together, and compare notes — or “calibrate” — on their team’s ratings and promotions. 

# Why Calibrations?

Calibrations are primarily about reducing the bias of ratings from individual managers, and increasing equity between managers. Secondarily, they are a great way to train managers on how the company thinks about performance. They also give senior managers data on the highest potential folks in a large organization, for strategic organization design. 

In most companies, there is a fixed budget in dollars for things like compensation changes and promotions. Calibrations distribute this budget as fairly as possible. This post will not explicitly cover compensation, but know that most companies will want to link compensation to some combination of company, business unit and individual performance. 

In a “pay for performance” system, you want to have outsized rewards for outsized impact. But the budget is still fixed. If you put twice as many people in the exceeding expectations category, then those people will get half the compensation increase they would have otherwise gotten. In order to maximize the impact of rewards, you do need to make decisions between cases.

## Elephant in the Room: Stack Ranking

Stack ranking means that you create an ordered list of individuals, according to their performance. It’s a mechanism to force the discussion about how to allocate rewards for greatest impact. The version that most people object to is if you are forced to label some people as low performers. Don’t do that. 

If you have a large organization, it’s reasonable for there to be a bell curve of performance. In practice, I have seen that for cohorts of 50 people or more — at both large companies and start-ups — performance will roughly fit a curve. This comports with the central limit theorem, which says that a sample size of at least 30 will approach a normal distribution. But that does not mean that the low end of the curve are necessarily low performers. I do not think it’s acceptable to mandate a fixed percentage here. 

You can keep yourselves honest by pressure testing people in the “meeting expectations” bucket. This is less of a budget issue, and more about holding managers and individuals accountable for real performance problems. While it’s not fair to absolutely require a fixed number of people to be labeled underperforming, there is a definite tendency for managers to avoid these difficult conversations by giving someone an unwarranted “pass”. 

Stack ranking gets a bad name from being forced on too-small cohorts, and for forcing low performance ratings. Companies that had a bad reputation for doing this in the past commonly still do have ratings and curves, but have substantially changed their process for how the ratings are generated and reconciled. Some do away with ratings, but still have differentiated compensation changes that could be reverse engineered into ratings. Some move the reconciliation to the Director level, and give them more discretion on fitting the curve. 

# How to Run Calibrations 

I’m going to outline a process that works well for cohorts of about 50 individuals, and can scale up to any size by layering on multiple calibration rounds. The goal of calibrations is to have fair and consistent performance ratings across organizations, roles, and levels. You want to reward high performance, and make sure that appropriate poor performance conversations are happening. 

This process can also be scaled up and down, in terms of how much time it takes. You can cap write-ups in length. You could potentially do the entire thing asynchronous ,with no meeting. Reducing the size of the cohort will also save time. There are all trade-offs against how much to want ratings to be equitable across teams. 

## Pre-work Before Calibrations 

Before you can calibrate, you need initial ratings and write ups. You need to gather data. This is where self reviews and 360 reviews come in. 

Self reviews should use the same format as the eventual manager review. You should have a defined template for this up front. They should reference your existing framework for expectations per role and level. You should ask people to provide a rating for themselves. Any training you can do here to help folks write good self reviews will pay off. 

Individuals and their manager should determine a set of three to five 360 reviewers per person. The format of a 360 should be short. Ask reviewers to timebox their effort, skip pleasantries and platitudes, and focus on situation, behavior, and impact. 360s can be anonymous or not, as long as it’s clear to the reviewer. 

Managers should draft their reviews in parallel with self reviews and 360s, and then incorporate any new information prior to calibrations. It’s at this point that managers should document initial ratings and promotion candidates. You are going to want to gather these in some central system, even if it’s just a spreadsheet. 

## The Calibration Meeting

For the actual calibrations, you should pick one facilitator. It’s this person's responsibility to set an agenda, set a timebox for each individual, and manage time during the meeting. The facilitator will need to have access to the preliminary ratings and promotions for every person to be discussed. They should create an agenda that has the name of each individual, the order they will be discussed, and a link to the manager write-up. It may help to give the attendees basic guidance about how many people would need to receive each rating in order to hit the budget. 

Calibration meetings have a reputation of taking forever. A six or more hour meeting to decide the ratings for 50 individuals is not uncommon. It’s also common for certain managers to dominate the airtime, which can result in bias. How can these be run more efficiently, and result in equitable outcomes? 

After several iterations, I’ve arrived at a format that I love. It’s pretty simple:

1. Send out a pre-read with the links to each write-up ahead of time. Group by level, and then by proposed rating. For example, all people at a certain level who are up for an exceeding expectations rating should be calibrated on in a block. 
2. Solicit Q&A about each individual right in the agenda doc. This is the most valuable feedback the manager will ever get about how well they are calibrated. 
3. In the calibration meeting, each candidate starts with 5 minutes of silent reading. Managers do not “present”. Attendees can write additional questions during this time.
4. The manager answers the written questions live for the rest of the time, someone else takes notes. 
5. At the end of time, everyone submits a “confidence vote” on the candidate

The confidence voting is the real secret sauce here. The idea is that every person present at calibrations submits a “blind” score for how confident they are in this individual being “at the bar” for the proposed rating or promotion. Blind means that scores are submitted without seeing anyone else’s score, to avoid bias. You can use any scale, but I like a star system with options for:

* Not Ready 
* Stretch Case 
* Solid Case 
* Slam Dunk 

You can also include a free-text field for comments. If not, encourage folks to leave any comments about why this case was not a “Slam Dunk” in the Q&A notes. 

The confidence voting system can also be used to pressure test low performers. Typically, managers are somewhat reluctant to put individuals up for discussion as potential low performers. Assuming you do not have time to calibrate on every single individual in the organization, it helps if you have some proxy data you can use to identify people to talk about here. For example, you could identify individuals who have missed delivery milestones, or who have not done the expected volume of candidate interviews. Group these folks together, and vote on confidence at a “meeting expectations” rating. This is how the group can come up with calibrated ratings for potential low performers. 

## Post Calibrations

The confidence votes are tallied and revealed after calibrations. Scores are averaged together per case, and the results are made available for all calibration attendees. It’s at this point that the overall decision maker for the organization can make final decisions by defining a “cut line” in the graph. It’s up to them to figure out how closely to hew to any distribution guidance. 

Calibrations need one decision maker. Typically this would be the manager that all of the calibration attendees report into, with the same being true for the individuals whose performance is being calibrated on. Alternatively, you could randomize cohorts across an organization to further mitigate bias, and the cost of the calibrators having less context. 

With the voting system, the decision maker does not need to make the final decisions live during the meeting. Instead, they can look at the results after the fact, and decide. Just because an average score is under the “Solid Case” line, does not mean that the rating or promotion does not get approved. It may, but much more important is the scores relative to other individuals up for the same rating or promotion. 

I’ve been surprised in that decision making role just how close the ranges of scores on any given individual are in practice, and also how much differentiation there is between cases. When this happens, it’s a sign that the managers in those calibrations are indeed thinking about performance and expectations in a way that is aligned. In that case, the decision maker knows that they are not unilaterally making decisions that may be biased. 

# Large Organizations & Multiple Calibration Rounds

A single calibration session can only scale up to cover 50 individuals. Assume that you only discuss people who are not solidly “meeting expectations”, and that those folks will be roughly 50% of the total. If you need a minimum of eight minutes per person, this would mean a three hour meeting. The total number of managers, or meeting attendees, would be between 6 and 12.

You will most likely have multiple calibrations, due to visibility issues. It’s typical for managers at level N to only sit in on calibrations discussing individual contributors at N-1 and below, or similar. That means that higher level individuals are likely to be discussed by managers that control more scope, which means managing larger orgs, which means that there are probably hundreds of individuals under those managers, way too many for a single session. 

There should be one overall facilitator to create the timeline and define the cohorts for how these multiple calibrations will be scheduled, so that no cohort is over about 50 individuals, and individuals are not discussed multiple times. 

Sometimes, you may choose to intentionally discuss individuals multiple times. This can be when you want to give managers practice writing their cases, and provide feedback in time to affect the final decisions. It can also be when you want to gather data from managers who work with a high-level individual regularly, before presenting the case in a higher level calibration. 

In all these calibrations, you should strive (or require) cohorts to individually meet their budget guidance. This is how you scale to any organization size without having to directly compare every individual with every other individual. It will not be possible to draw cut lines across cohorts later, because the relative confidence scoring between cohorts is not directly comparable. 

# Last Word on Confidence Voting 

The confidence voting system in particular delivers some key benefits that most calibrations struggle to achieve. The meeting time and conversations about individuals can be timeboxed. You can choose to not discuss up to 50% of people who are solidly in the “meeting expectations” camp, if time does not allow. The final results can be justified with data, and we can reason about exactly how “calibrated” the group was. We can choose how closely to follow any given budget guidance. We can also pressure test low performers. 


---
title: "Burnout Playbook"
tags: management 
toc: true
header:
  overlay_image: /images/header-burnout.jpeg
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "Leaders need to be actively managing their own mental well-being at work"
---

# What is Burnout?

Burnout is a state of poor professional mental health, where the feeling of being overwhelmed or a lack of enthusiasm impacts your effectiveness. It can be caused by a mixture of mental or physical exhaustion, lack of belief in the work, and lack of belief in yourself. Each individual has a personal expectation about an ideal work environment on these dimensions. When reality becomes too far detached from this ideal, the result is burnout. 

If you are currently burned out, you probably already know it. It feels like a crippling inability to get things done or to even care. It's hard to ignore. But it's also common to slowly be trending towards burnout without realizing it until it's too late. If you think you might be at risk of burnout, ask yourself the questions in the [Burnout Risk Checklist](#burnout-risk-checklist).

Burnout can severely impact your attitude, professionalism and performance at work. It's also one of the leading causes of attrition. 

# Well-being and Performance 

Personal well-being is the best leading indicator of professional success. Think about all the people in your career that have left a company, whether it was their individual choice or they were actively managed out. If that person chose to leave, they were likely unhappy in their work situation. If they were managed out, they were likely unhappy in their work situation, and that resulted in poor performance. It's relatively rare for someone happy and engaged to have severe performance issues. 

Now think about high performers you have seen over your career. They were likely universally enthusiastic, and engaged in their work. They were happy in their work situation.

This is especially true for a leader. Your well-being is highly visible. It also directly informs the well-being of others and the performance of the larger organization. 

# Own Your Mental Health 

The more senior you get, the more you will need to manage yourself. It's true for career growth, but also for well-being. You won't always be maximizing growth, impact, and well-being at the same time. You may have periods where you are trading off one for another. There will be well-being fallow periods. Part of your role as a leader is navigating yourself — and others — through those periods.

Everyone is responsible for their personal expectations about an ideal work environment. But leaders have a greater effect on both the expectations and reality of the work environment for others. Part of your role is taking ownership of the well-being of everyone. 

You can do this by providing a narrative that ties short-term goals to long-term strategy and impact. That's critical for your well-being too, as more senior roles have longer cycles for impact. By owning your well-being, you are taking control of your performance, longevity in the role, and ultimately the impact of the organization. Doing what's necessary for your well-being **is** doing the best thing for the company. 

To be alert for personal burnout, you need to learn to identify the warning signs. You need to be able to diagnose burnout signals in yourself and others. 

# Diagnosing Burnout 

Lack of action is the number one leading indicator of burnout. If you notice in yourself a distinct lack of activity relative to your baseline norm, it's worth digging in to see if you may be on the path to burning out. If you manifest any of the overt warning signs of burnout, then move on to trying to figure out which flavors of burnout you are experiencing. 

You may be experiencing poor mental well-being due to a mix of exhaustion, depersonalization, or ineffectiveness. 

## Exhaustion 

Exhaustion means that you feel mentally or physically tired, consistently, over a period of weeks. 

Taking an extended vacation may help. Utilize general stress management techniques. That means sleeping well, exercising, focusing on a healthy diet, seeing family, and engaging in hobbies. 

Making lists can also help. What things in your personal life do you find the most engaging? What things are you grateful for? Try tracking your mood over time with a journal or app. 

If you get energy from spending time with other people, make room for that in your work week. Traveling to a shared working space or social event can be time well spent if it's combating mental fatigue and re-igniting well-being. 

## Depersonalization 

Depersonalization is a lack of belief in the impact of your projects or the lack of alignment with your values and well-being. 

Actively connecting the reality of your work to your ideals is the general mechanism for addressing depersonalization. Make and update lists of your accomplishments. Think of it as preparation for your next self review. This will serve as a forcing function to articulate what you are doing, and what the impact is. 

You might find that work that you previously thought was not valuable did deliver impact. Format your notes to briefly describe the situation you were in, the behavior you demonstrated, and the impact on the business. If you feel stuck, you may need to force yourself to document even the smallest impact. After doing this, you may find that some projects did not have impact. Use that to inform what you choose to work on in the future. 

Make a list of all the places you have worked. For each one, write down four or five projects that stick out as the most memorable. These are likely to be the projects that align most closely with your values. What did you enjoy about them? Translate these into generic themes like "mentorship", or "hiring". Brainstorm some side projects that you could be doing now for each theme. 

It's OK to take on new projects, if it's something you are going to be excited about, or you feel confident it will have an impact. If needed, make space for this by punting — or delegating — items that you are not as enthusiastic about. Just make sure to inform any stakeholders that may be counting on you to complete a project. A good source of project ideas are items that you have heard other folks propose and own, but which are stalled out. Ask them if you can take it off their plate! 

Start small, and pile up quick wins. Making forward progress on items that naturally engage you is the more surefire way to build back up your sense of making a difference. 

## Ineffectiveness 

Ineffectiveness is burnout stemming from a belief that a project cannot be completed, either because they are generally infeasible, or because of your lack of ability.

Remind yourself of projects that are being completed, no matter how small. Try tracking and celebrating wins that are happening. As a manager, it's common to focus your conversations with others on things that are not going well. Try to intentionally do the opposite. Talk to folks in 1:1s about what projects are succeeding. Compile and send an update on recent successes. Intentionally spend extra time giving others public kudos or otherwise show your appreciation. 

You also need to keep in mind that your ability can change over time. Focus on what you are learning. This is known as a growth mindset. Try keeping a list of things that didn't go well, and what you learned from them. 

# Talking about Burnout 

You should talk about burnout with significant others, family, and friends. You should talk about it with your boss and other coworkers, provided that you have the necessary psychological safety. If your boss has already proactively raised the topic with you, talk about your plan to reestablish well-being. 

Like a midlife crisis, burnout at work is ultimately caused by the gap between your ideals and reality. Both are likely to be most accurate after major changes. If that's the case, give yourself time; this isn't something that will be fixed quickly. Self-reflection will be important. If talking to friends and family is not enough, then seek professional therapy. 

Acceptance is the other half of the equation. Increasing self-engagement will be very helpful. Take the opportunity to reexamine your ideals about your work environment. Whether your self-worth is too tied to your engagement in your professional life is a question only you can answer. 

# Don't Give Away (All) Your Toys

Especially when you are digging out of a burnout hole — but anytime — it's OK to spend time on some things just because they boost your well-being. Sometimes senior leaders delegate too many things, including their favorite work. You may have gotten the advice to stop coding altogether. But, if you find it engaging, and the code will have an impact, give yourself permission to continue doing it. Everything is in moderation. Just don't neglect your core responsibilities. 

Never sit around with nothing to do. You can always invent impactful work. If all else fails, read an article or a book for professional development. Take ideas from that and implement them. 

Don't wait for an exciting project to come down the pipe before you re-engage. You need to proactively be fixing your engagement, first. You are ultimately accountable for staying engaged and managing your own burnout risk. 

# Burnout Risk Checklist 

- [ ] Have you been "bored" at work? 
- [ ] Have you caught yourself being overly cynical?
- [ ] Have you been unable to stop thinking about a work event that happened a while ago? 
- [ ] Have you been dreading starting your work day? 
- [ ] Have you experienced a perceived lack of work to engage with? 
- [ ] Have you experienced regret about projects other people are doing, instead of you? 
- [ ] Have you noticed a decrease in enthusiasm in your personal life? 
- [ ] Have you gone through a major change at work? 
- [ ] Have you suffered a major professional setback or failure? 
- [ ] Have you projects you completed not had the expected impact? 
- [ ] Have you said something in a professional setting that you wish you could take back? 
- [ ] Have you let projects languish on your TODO list? 
- [ ] Have you been unable to do something that you used to do regularly? 
- [ ] Have you gotten feedback about your attitude or engagement? 

# Burnout Playbook 

- [ ] Keep a journal of how you're feeling
- [ ] Talk to a significant other 
- [ ] Talk to a family member 
- [ ] Talk to a professional peer 
- [ ] Take some time off 
- [ ] Improve your exercise and diet 
- [ ] List things that you are grateful for 
- [ ] Make an effort to connect with coworkers in person 
- [ ] Celebrate the impact of others with public kudos 
- [ ] List things that you would be excited to learn professionally 
- [ ] List recent successes in situation, behavior, impact format 
- [ ] List things that you are grateful for 
- [ ] Brainstorm past work projects that have been the most engaging 
- [ ] Generate a couple of projects that you will be enthusiastic about 
- [ ] Make time to work on enthusiasm projects 
- [ ] Delegate current projects that you are not enthusiastic about 
- [ ] Identify some small wins you can achieve 
- [ ] List what you have learned from challenging situations 
- [ ] Write down how your professional life relates to your self-worth 
- [ ] Write down what is most important to you in life 
- [ ] Write down what you are giving yourself permission to do in the name of mental health 
---
title: "What are the less obvious downsides of a Health Savings Plan?"
tags: personal-finance
toc: true
header:
  overlay_image: /images/header-hsa.webp
  overlay_color: "#000"
  overlay_filter: "0.5"
excerpt: "As someone with a young family and who is generally on track for retirement savings, I switched to an HSA and HDHP for two years, but then switched back"
---

A Health Savings Accounts (HSA) lets you save money pre-tax to use for medical expenses at any time in the future. You can also invest the money if you plan to use it in the distant future, such as in retirement. It’s one of only a handful of vehicles for pre-tax investing, which makes it very attractive. HSAs exist in large part to get people to enroll in a High Deductible Health Plan (HDHP). Even though HSAs look attractive financially, they are non-financial drawbacks that are not obvious at first. 

# Benefits of HDHP + HSA

The premium — the amount of money coming out of your paycheck automatically — for an HDHP will be lower than an PPO or HMO. How much lower will depend on your employer’s contribution, but it could be a low as a $500 a year. By itself, this is probably not motivating enough to switch. The main benefit is the pre-tax savings. In 2023, the limit on HSA contributions is $7,750 for a family. That comes off of your taxable income for the year. If you are paying close to the marginal tax rate of 37%, that would save you about $2,800 in taxes. Many employers will make their own contribution to the HSA, which could be thousands of dollars. This is extra free money.

The real incentive is investing that money. If you invested the whole $7,750 in an index fund that returns 7% a year for 15 years, you would contribute $116,000 and it would grow to $194,000 in those 15 years. That return gets larger over long time horizons. If you did the same thing for 30 years, you would contribute $232,000 and it would grow to $731,000. This is the primary attraction to an HSA for people already maxing out their other pre-tax investment vehicles.

*Note: leaving contributions to grow requires that you don’t use the HSA money to pay for in-year medical expenses. This means that your post-tax expenses in the current year would increase up to your HDHP deductible, typically something like $6,000 for a family. On paper, the pre-tax benefit of the HSA is still worth it.* 

# How HDHP + HSA Works 

When you’re on an Health Medical Organization (HMO) medical insurance plan, you do not get billed directly for most medical expenses. You may receive a statement in the mail that says “this is not a bill”, as a cost transparency mechanism. But, you only have to pay the premium — deducted from your paycheck — and small per-visit copays. When you’re on an HDHP, you get billed for medical expenses from the service provider, and you need to pay those bills until you hit the deductible limit on your plan. After that, you will mostly not get billed and the insurance covers the additional expenses, with the exception of copays and potentially a small percentage-based co-insurance payment. You should keep receipts for any medical expenses you want to get reimbursed for. 

The HSA is designed for you to tap into your pre-tax HSA contributions to pay for these expenses before you hit your deductible. For small expenses, you can use a debit card they issue you. For larger expenses you do this by logging in to their website and telling them how much of the funds to send back to you. This can be either a check in the mail or a direct deposit. The HSA provider will not ask you for a receipt for your expenses. When you file your taxes for the year, you will declare how much you contributed to your HSA and how much you were reimbursed from your HSA. You will not need to submit receipts, but you should keep them for seven years in case you are audited. 

You invest HSA funds through the HSA provider’s website. Similar to a 401k provider, you select contributions amounts and which funds you want to invest it. With an HSA, you are selecting one contribution amount for how much additional money you want to deduct from your paycheck to put into the HSA, plus a second amount that you want to be moved from your HSA cash balance to your investments. 

# Downsides of HDHP + HSA 

The downside of an HDHP + HSA that everyone knows about is the contribution amount. There is more money coming out of your paycheck. Assuming you contribute pre-tax to the HSA, that will more than offset the savings in premiums. People who are already maxing out their other pre-tax savings vehicles see this as a benefit; it’s reducing their taxable income and increasing pre-tax investments. But, the short-term negative cash impact is nonetheless a downside — and the most well-known one. 

## Billing Errors

Increased paperwork and exposure to medical bureaucracy are less well-known downsides. When you’re on an HDHP you need to keep track of and pay medical bills. When you’re on an HSA, you need to keep receipts for medical expenses. If you’re on an HMO you may never have looked closely at a medical statement. It’s common for various doctors and specialists involved in something like a visit to the emergency room to all send you different bills. There are often mistakes on the bills, such as being double-billed for something. There is not a lot of clarity on exactly what an item on the bill represents. If you’re on an HMO you don’t really care. If you’re on an HDHP, you may very well find yourself calling your insurance company to figure this stuff out. They will often need to bring someone from the service provider (hospital) into the loop to sort it 
out. All this takes time, and is frustrating. 

## Unreasonable Medical Costs 

Exposure to absolutely insane, detached-from-all-reality, inside-baseball itemized medical costs are the next downside. On an HDHP, you are incentivized to spend less on medical expenses, until you hit your deductible. Did you know that something like a 1-hour speech therapy session for a child can cost $700? You will get bills like this from the provider. You may very well not be able to find out how much it costs until *after* the service. It doesn’t matter that the going rate for this service outside of insurance is $150. When they bill you this amount, it’s not negotiable. If you’re on an HMO, you don’t care what inflated price the provider is changing themselves. 

## Tax Paperwork

The next downside is tax complexity and audit risk. When you file your taxes, you will submit a form 8889 for your HSA. You report contributions and distributions. This is not a big deal, it’s just one more thing to keep track of. HSA providers do not send this form to you, like 401k providers do. You need to fill it out manually. You don’t need receipts for medical expenses either when you request a distribution from the HSA provider, or when you file your taxes. You will need your receipts if you even get audited, however. 

## Bad HSA Providers

You are locked in to the HSA provider that your company chooses. Like 401k providers, there is a wide range of quality here. The company has an incentive to pick the lowest cost provider, not the one with the best website or customer support. In short, there is a good chance the only provider you can use will be terrible. Websites may be poorly implemented. There may be no phone number of call for customer support — instead you may be directed to your internal benefits team. There may be limitations on the number of transactions you can make. There may be annual fees. 

Your company may also change providers. Your cash funds may or may not be rolled over to the new provider automatically. Even if cash funds are moved, investment funds may not be. There is the added complexity of special “in-kind” transfers that are necessary for avoiding tax implications of moving to a new HSA provider. You may or may not be able to do an in-plan (i.e. without leaving the company) transfer to your own third-party HSA provider. But, you will not be able to use a third-party provider for automatic contributions from your paycheck and your employer. Over the course of a long career, you may end up with many different HSA accounts at many different providers, unless you spend time to consolidate them. 

## Psychological Cost

Don’t underestimate the added element of decision making on top of all medical decisions, especially when there are family members and spouses involved. Even if the cost of medical treatment up to your deductible is a negligible expense in your overall budget, you may be surprised to find yourself in protracted conversations about whether it’s “worth” getting various medical treatments. Maybe your knee is bothering you, but it’s not a show stopper. Do you get an MRI, even if that might cost you $1500 dollars out of pocket? This is one of the *points* of a HDHP, from the perspective of the insurance company — to make consumers more aware of medical expenses. They know that this will reduce unnecessary medical expenses. But, it will also reduce necessary preventative expenses. 

## Identity Theft

One of the primary use cases for an HSA is paying for medical expenses. You will get a debit card sent to you for this purpose, whether you want one or not. You will be required to create a username and password for the website of the HSA provider. This website is like small bank account, where anyone with your password can make a deduction. Both the debit card and the website are vectors for identity theft and potential financial loss. Neither of these are purely theoretical threats — there are news articles about fraud happening in the wild. 

# Conclusion 

Enrolling in an HSA is not a one-way door. You can typically make a different election once a year, or when you change employers or at certain qualifying life events. If you save a large amount of money in an HSA and then switch back to an HMO, you may have trouble spending that money. It’s not clear what expenses are eligible when you’re no longer on an HDHP. It’s difficult to research, but you are probably fine to expense copays and any services or medications purchased outside your insurance. Plus, you can still save the money for medical expenses in retirement. 

What would it take to “fix” these risks to HSA programs? Medical costs would need to become both understandable and reasonable. You would need to be allowed to choose your own HSA provider. Neither of these are likely to happen. 

In the meantime — as with many aspects of finances — you can’t go wrong with keeping it simple. For medical insurance, it doesn’t get any simpler than an HMO. 

